[
  {
    "path": "posts/11-29-20_Alejandra/",
    "title": "Random Forest Model",
    "description": "This is how fitting a Random Forest model went for me!",
    "author": [
      {
        "name": "Alejandra Garcia Isaza",
        "url": {}
      }
    ],
    "date": "2020-12-07",
    "categories": [],
    "contents": "\n\nContents\nPrep work\nLet‚Äôs dive in on the model fitting process!\nFirst round of tuning\nSecond round of tuning\nConclusionFinal fit\n\nKaggle submission\n\nTo complete this project, I chose to run a Random Forest model. I decided to go with the Out-of-Bag (OOB) samples to conduct model fitting and model assessment due to its relatively lower run time, as compared to using cross-validation resamples.\nInitially, I planned to work locally on my computer with only 5% of the data. Overall, my model tuning process was taking between 15 and 25 minutes each round of tuning, but unfortunately my R session froze several times and I could not finish the model tuning process.\nAt this point, Daniel suggested I use HPC via the Open On Demand interface. As always, I was afraid to try something new, but Daniel patiently showed me how to work with the new interface and explained for the 97th time what was the difference between the train and test files we have been working with all term long. I think I finally got it! I‚Äôm glad it only took me ten weeks üò¨ to understand this.\nPrep work\nI followed the same steps we have been following all term long, reading in the data, sampling a fraction of the data (note that I used 50% of the data because I was working with HPC), and joining with other datasets. Here I joined a dataset we called ‚Äúbonus‚Äù that inludes variables from different datasets. To learn more about this dataset and its variables, please go to <<<< link to page >>>>>.\nA huge shout-out to Chris Ives for taking the time to find and process the bonus datasets we are using here.\n\n\nset.seed(3000)\n\nfull_train <- import(here(\"data\", \"train.csv\"), setclass = \"tbl_df\") %>%\n  select(-classification) %>%\n  mutate_if(is.character, factor) %>%\n  mutate(ncessch = as.double(ncessch)) %>%\n  sample_frac(0.5)\n\nbonus <- import(here(\"data\", \"bonus_data.csv\")) %>%\n  mutate(pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>%\n  mutate(ncessch = as.double(ncessch))\n\n## joining data\ndata <- left_join(full_train, bonus)\n\n\n\nHere I created the initial split and the train and test objects for model fitting and assessment. Notice that I did not create a cross-validation resample object because I was only using the OOB samples.\n\n\nset.seed(3000)\ndata_split <- initial_split(data)\n\nset.seed(3000)\ndata_train <- training(data_split)\ndata_test <- testing(data_split)\n\n\n\nI created a recipe following the recommended preprocessing steps for a Random Forest model. I found this guide that Joe shared with our group very useful.\nThis recipe is pretty straightforward, the only thing that I think is worth highlighting is that we took the liberty to assume that most of the students‚Äô home language was English; that is why we specified this code lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"). I took extra care not to lose the Spanish speaking students in the lang_cd variable to the step_nzv() near-zero variance variable removal due to the role of language in academic achievement.\nIn this model, we had one outcome score, 77 predictors and ten Id variables.\n\n\nrec <- recipe(score ~ ., data_train) %>%\n  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>% \n  update_role(contains(\"id\"), ncessch, ncesag, sch_name, new_role = \"id_vars\") %>%\n  step_zv(all_predictors(), -starts_with(\"lang_cd\")) %>%\n  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(\"id_vars\")) %>%\n  step_novel(all_nominal()) %>%\n  step_unknown(all_nominal()) %>% \n  step_dummy(all_nominal()) %>%\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\n\n\n\nLet‚Äôs dive in on the model fitting process!\nFirst, I wanted to develop a baseline model using the default hypermarameters. I focused only on three hyperparameters: predictors mtry = floor(p/3), number of trees trees = 500, and minimun node size min_n = 5. In this model, mtry was around 26 predictors.\n\n\n# default model\nrf_def <- rand_forest() %>%\n  set_engine(\"ranger\",\n             num.threads = 8,\n             importance = \"permutation\",\n             verbose = TRUE) %>%\n  set_mode(\"regression\")\n\n# workflow for default model\nrf_def_wkflw <- workflow() %>% \n  add_model(rf_def) %>% \n  add_recipe(rec)\n\n# fitting the default model\nrf_def_fit <- fit(\n  rf_def_wkflw,\n  data_train)\n\n\n\nThe rmse for the default model was 88.80.\nFitting the default model with 50% of the data took less than 3 minutes using HPC!!\nFirst round of tuning\nRandom forest models are know to have very good out-of-the-box performance, however, I wanted to tune at least mtry and min_n to evaluate how much lower the rmse could be.\nI decided not to spend time tuning for number of trees as the literature suggests that growing p * 10 trees is pretty safe. The number of trees needs to be large enough to stabilize the error rate; for this model I could have used 770 trees, but I went for 1000 trees just to be extra safe.\nTo tune for mtry and min_n, I followed what Boehmke & Greenwell (2020) suggest in the chapter focused on random forest models in the book Hands-On Machine Learning with R\n‚ÄúStart with five evenly spaced values of mtry across the range 2 ‚Äì p centered at the recommended default‚Äù (Boehmke & Greenwell, 2020).\n‚ÄúWhen adjusting node size start with three values between 1‚Äì10 and adjust depending on impact to accuracy and run time (Boehmke & Greenwell, 2020).‚Äù\nHere I created a regular grid with 20 different combinations of mtry and min_n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[2,14,26,38,50,2,14,26,38,50,2,14,26,38,50,2,14,26,38,50],[1,1,1,1,1,4,4,4,4,4,7,7,7,7,7,10,10,10,10,10]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nBelow is a function that Daniel helped me build. This is an all-in-one function that builds the model, updates the default model workflow with the new hyperparameters, fits the models with the selected hyperparameters, and outputs a table with the tuned hyperparameters and the corresponding rmse value. It‚Äôs pretty neat!\n\n\n# All-in-one function\nhyp_rf_search <- function(mtry_val, min_n_val, wf) {\n  mod <- rand_forest() %>% \n    set_engine(\"ranger\",\n               num.threads = 8,\n               importance = \"permutation\",\n               verbose = TRUE) %>% \n    set_mode(\"regression\") %>% \n    set_args(mtry = {{mtry_val}},\n             min_n = {{min_n_val}},\n             trees = 1000)\n  \n  wf <- wf %>% \n    update_model(mod)\n  \n  rmse <- fit(wf, data_train) %>% \n    extract_rmse()\n  \n  tibble(mtry = mtry_val, min_n = min_n_val, rmse = rmse, workflow = list(wf))\n}\n\n# Applying the function\nmtry_results_1 <- map2_df(grd$Var1, grd$Var2, ~hyp_rf_search(.x, .y, rf_def_wkflw))\n\n\n\nThe lowest rmse for this round of tuning was 88.54 with mtry = 14, min_n = 10, and trees = 1000. Running this first round of tuning took around an hour and forty minutes and the decrease on the rmse value relative to the default model was not substantial, only around three decimal points difference.\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[14,14,26,14,38,14,26,50,38,26,50,26,38,50,38,50,2,2,2,2],[10,7,10,4,10,1,7,10,7,4,7,1,4,4,1,1,7,1,10,4],[88.5422609601283,88.9375105622991,89.2748002583518,89.3467365378118,89.6531075854499,89.7182599041209,89.7944994293632,89.9400737655761,90.3377558734669,90.5156819860458,90.6806244642205,91.1849421483674,91.2063236506577,91.742655109802,92.1480079640799,92.7881197919246,93.312497707138,93.3501092283632,93.3760153020166,93.3817847358467]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n      <th>rmse<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nBy examining the plot, I could see that an mtry = 14 was consistently producing lower rmse values, however, higher values of min_n appeared to produce slighty lower values of rmse.\n\n\n\nSecond round of tuning\nOn the second round of tuning I chose a range of numbers between the two mtry values that produced the lowest rmse values, 14 and 26 (the recommended default). In addition, I chose a range of numbers that included the best two min_n values, 7 and 10, but also a few more numbers higher than ten to check if, as I intuited, higher values of min_n increased the perfomance of the models.\nFor this round of tuning, I created a regular grid with 25 different combinations of mtry and min_n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[14,17,20,23,26,14,17,20,23,26,14,17,20,23,26,14,17,20,23,26,14,17,20,23,26],[7,7,7,7,7,9,9,9,9,9,11,11,11,11,11,13,13,13,13,13,15,15,15,15,15]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nI used the same all-in-one function shown above to run these models. The lowest rmse for this second round of tuning was 88.17 with mtry = 14, min_n = 15, and trees = 1000. The model fitting process took a little less than two hours and the decrease on the rmse value relative to the previous model was again of only three decimal points.\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[14,17,14,20,17,14,23,20,26,17,14,23,20,26,17,14,23,26,20,17,23,20,26,23,26],[15,15,13,15,13,11,15,13,15,11,9,13,11,13,9,7,11,11,9,7,9,7,9,7,7],[88.1702701032027,88.2913312019863,88.2917472699144,88.4320547892268,88.4478151959256,88.4714496091745,88.5588965088155,88.6207445582953,88.6414907304891,88.6730477850128,88.6868663843177,88.7485721937091,88.8327314174613,88.8925891056003,88.9084821719671,88.9127044769608,88.9748487664824,89.1090208580788,89.123262575347,89.1928533213956,89.2882857406444,89.4205005670333,89.4284658762646,89.6031329821944,89.79396337313]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n      <th>rmse<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nBy examining this plot, I confirmed what I intuited in the first round of tuning, that an mtry = 14 consistently produced the lowest rmse values and that higher values of min_n continued to produce lower values of rmse.\nI could have kept tuning for higher values of min_n, however, I thought that the cost in time was not worth the small gain in decrease of rmse values.\n\n\n\nConclusion\nTaking it all together, the default model was without a doubt the one that had the higher cost/benefit return, it only took around three minutes to fit! Whereas each round of tuning took arond two hours, with very little return (imo). I guess that the person who said that random forest models have great out-of-the-box performance was not joking!\nFinal fit\nAt this point I was convinced that continue to tune was not worth the time, however, I had already spent quite a bit of time in model tuning so I decided to use mtry = 14 and a min_n = 15, the hyperparameters that produced the lowest rmse on my model tuning process.\n\n\n# final model with hard coded hyperparameters\nfinal_mod <- rand_forest() %>%\n  set_engine(\"ranger\",\n             num.threads = 8,\n             importance = \"permutation\",\n             verbose = TRUE) %>%\n  set_mode(\"regression\") %>%\n  set_args(mtry = 14,\n           min_n = 15,\n           trees = 1000)\n\n# workflow for final model\nfinal_wkfl <-  workflow() %>%\n  add_model(final_mod) %>%\n  add_recipe(rec)\n\n# final split on the initial split\nfinal_fit <- last_fit(final_wkfl,\n                      split = data_split)\n\n\n\nFitting the final model took around 4 minutes and the resulting rmse was 88.3.\nKaggle submission\nThe next step was to submit predictions to Kaggle to test how the model performed when compared against the true values. To do this I had to fit the model using the unsplit dataset called data, read in the test.csv file, join the bonus data, prep and bake the recipe and create a dataframe with the predicted score values.\n\n\n# fit with the unsplit data\ncheck_fit <- final_wkfl %>%\n                 fit(data)\n\n# read in test.csv file\nfull_test <- import(\"data/test.csv\", setclass = \"tbl_df\") %>%\n  mutate_if(is.character, factor) %>%\n  mutate(ncessch = as.double(ncessch))\n\n## joining data\nall_test <- left_join(full_test, bonus)\n\n# baking the recipe\nprocessed_test <- rec %>%\n  prep() %>% \n  bake(all_test)\n\n# make predictions\npreds <- predict(check_fit, all_test)\n\n# a tibble\npred_frame <- tibble(Id = all_test$id, Predicted = preds$.pred)\n\n\n\nThis table shows only the first six rows of data:\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[4,6,8,9,11,15],[2492.97321250139,2501.58582718069,2668.15853429978,2464.77458894793,2530.78887087146,2487.80269035691]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Id<\\/th>\\n      <th>Predicted<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n\nI was gladly suprised to find that the model performed better than expected! ü•≥\n\n\n\n",
    "preview": "posts/11-29-20_Alejandra/rf_models_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2020-12-07T17:21:37-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_Asha/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\nTable of Contents\n1.Splitting and resampling1a. Splitting\n1b. Resampling\n\n2.Pre-processing\n3. Linear model\n4. Lasso Regression (Type of penalized Regression)\n1.Splitting and resampling\nData splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.\n1a. Splitting\nWe will split our dataset into training and testing set. The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model‚Äôs performance.\nWe will use initial_split() function from the rsample package which creates a split object. The split object d_split, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).\n\n\nset.seed(3000)\n# Create split object specifying (75%) and testing (25%)\n\ndata_split <- initial_split(data, prop = 3/4) \n\ndata_split\n\n<Analysis/Assess/Total>\n<1421/473/1894>\n\nWe will extract the training and testing set from the split object, d_split by using the training() and testing() functions.\n\n\n# Extract training and testing set\nset.seed(3000)\n\ndata_train <- training(data_split)  # Our training dataset has 1421 observations.\n\ndata_test <- testing(data_split)    # Our test dataset has 473 observations.\n\n1b. Resampling\nAt some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we‚Äôll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = number of time we resample.\n\n\n# Resample the data with 10-fold cross validation.\nset.seed(3000)\ncv <- vfold_cv(data_train)\n\n2.Pre-processing\nPreprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as all_numeric(), all_predictors() as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (step_medianimpute), rescaling (step_scale), standardizing (step_normalize), PCA (step_pca) and creating dummy variables (step_dummy). A full list of pre-processing can be found here.\nWe will use recipe() function to indicate our outcome and predictor variables in our recipe. We will use ~. to indicate that we are using all variables to predict the outcome variable score. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, rec, it shows numbers of predictor variables have been specified. It doen‚Äôt actually apply the predictors yet. We will use same receipe throughout this post.\n\n\nset.seed(3000)\n\nrec <- recipe(score ~ ., data_train) %>% \n  step_mutate(tst_dt = as.numeric(lubridate::\n                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\n  step_medianimpute(all_numeric(), -all_outcomes(), \n                    -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\n  step_dummy(all_nominal(), -has_role(\"id vars\")) %>% # dummy codes categorical variables\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\n\nExtract the pre-processed dataset\nTo extract the pre-processed dataset, we can prep() the recipe for our datset then bake() the prepped recipe to extract the pre-processed data.\nHowever, in tidymodels we can use workflows() where we don‚Äôt need to prep() or bake() the recipe.\n\n\nprep(rec) #%>%\n\nData Recipe\n\nInputs:\n\n      role #variables\n   id vars          8\n   outcome          1\n predictor         79\n\nTraining data contained 1421 data points and 1421 incomplete rows. \n\nOperations:\n\nVariable mutation for tst_dt [trained]\nSparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\nMedian Imputation for enrl_grd, tst_dt, lat, lon, ... [trained]\nDummy variables from gndr, ethnic_cd, tst_bnch, ... [trained]\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\n\n # bake(data_train)   # We are using workflow so no need to bake.\n\nThe next step is to specify our ML models, using the parsnip package. To specify the model, there are four primary components: model type, arguments, engine, and mode.\n3. Linear model\n\n\nset.seed(3000)\n\n# Specify the model\n\nmod_linear <- linear_reg() %>%\n  set_engine(\"lm\") %>%  # engine for linear regression\n  set_mode(\"regression\")  # regression for continous outcome varaible.\n\n# Workflow\nlm_wf <- workflow() %>% # set the workflow\n  add_recipe(rec) %>% # add recipe\n  add_model(mod_linear) # add model\n  \n\n# Fit the linear model\nmod_linear_fit<- fit_resamples(\n  mod_linear,\n  preprocessor = rec,\n  resamples = cv,\n  metrics = metric_set(rmse),\n  control = control_resamples(verbose = TRUE,\n                                    save_pred = TRUE))\n\nCollect metrics on linear model\n\n\nset.seed(3000)\n\nmod_linear_fit %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") \n\n# A tibble: 1 x 5\n  .metric .estimator  mean     n std_err\n  <chr>   <chr>      <dbl> <int>   <dbl>\n1 rmse    standard    96.3    10    2.49\n\n#RMSE = 96.28\n\nOur data has multiple variables and some of them are highly correlated. In case like this, linear model usually performs poorly. So let‚Äôs try other alternatives such as penalized regression. We will use the same recipe (i.e. rec) for all models in this post. We will use tune_grid() to perform a grid search for the best combination of tuned hyperparameters such penalty.\n4. Lasso Regression (Type of penalized Regression)\n\n\n# Specify the model\n\nset.seed(3000)\n\n# specity lasso with random penalty value and set 1 for mixture.\nlasso_mod <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\") \n\nwf_lasso <- workflow() %>%\n   add_recipe(rec)\n\nlasso_fit <- wf_lasso %>%\n  add_model(lasso_mod) %>%\n  fit(data = data_train)\n\n\nlasso_fit %>%\n  pull_workflow_fit() %>%\n  tidy()\n\n# A tibble: 80 x 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept) -6.64e+3     0.1\n 2 enrl_grd     2.74e+1     0.1\n 3 tst_dt       4.49e-6     0.1\n 4 lat         -6.14e-1     0.1\n 5 lon         -3.36e+0     0.1\n 6 ncesag       0.          0.1\n 7 zip          1.65e-2     0.1\n 8 total_n     -1.57e-3     0.1\n 9 fr_lnch_n   -8.84e-2     0.1\n10 red_lnch_n   4.70e-1     0.1\n# ... with 70 more rows\n\nTune Lasso parameters We will use resampling and tuning to figure out right regularization parameter ‚Äôpenalty`\n\n\n# Tuning lasso parameters\n\nset.seed(3000)\n\ntune_lasso <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n# Tune the lasso grid\nlambda_grid <- grid_regular(penalty(), levels = 50)\n\nTune the grid using workflow object\n\n\ndoParallel::registerDoParallel()\n\nset.seed(3000)\nlasso_grid <- tune_grid(\n  wf_lasso %>%\n    add_model(tune_lasso), \n  resamples = cv,\n  grid = lambda_grid\n)\n\n\n\n# Results\n\nlasso_grid %>%\n  collect_metrics()\n\n# A tibble: 100 x 7\n    penalty .metric .estimator   mean     n std_err .config\n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <fct>  \n 1 1.00e-10 rmse    standard   95.9      10  2.50   Model01\n 2 1.00e-10 rsq     standard    0.362    10  0.0169 Model01\n 3 1.60e-10 rmse    standard   95.9      10  2.50   Model02\n 4 1.60e-10 rsq     standard    0.362    10  0.0169 Model02\n 5 2.56e-10 rmse    standard   95.9      10  2.50   Model03\n 6 2.56e-10 rsq     standard    0.362    10  0.0169 Model03\n 7 4.09e-10 rmse    standard   95.9      10  2.50   Model04\n 8 4.09e-10 rsq     standard    0.362    10  0.0169 Model04\n 9 6.55e-10 rmse    standard   95.9      10  2.50   Model05\n10 6.55e-10 rsq     standard    0.362    10  0.0169 Model05\n# ... with 90 more rows\n\n# RMSE = 95.94\n\nVisualize the performance Results look fine so let‚Äôs visualize the performance with the regularization parameters.\n\n\noptions(scipen = 999)\n\nlasso_grid %>%\n  collect_metrics() %>%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_errorbar(aes(\n    ymin = mean - std_err,\n    ymax = mean + std_err\n  ),\n  alpha = 0.5\n  ) +\n  geom_line(size = 1.5) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  theme(legend.position = \"none\")\n\n\nFor choosing our final parameters, let‚Äôs get the lowest RMSE. Once we have the lowest RMSE, we can finalize our workflow by updating with lowest RMSE.\n\n\nlowest_rmse <- lasso_grid %>%\n  select_best(\"rmse\")\n\nfinal_lasso <- finalize_workflow(\n  wf_lasso %>% add_model(tune_lasso),\n  lowest_rmse\n)\n\n\n\nlibrary(vip)\n\nfinal_lasso %>%\n  fit(data_train) %>%\n  pull_workflow_fit() %>%\n  vi(lambda = lowest_rmse$penalty) %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\n\n\n\n\nlast_fit(\n  final_lasso,\n  data_split) %>%\n  collect_metrics()\n\n# A tibble: 2 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      90.0  \n2 rsq     standard       0.424\n\n# RMSE for full dataset = 89.95\n\n\n\n",
    "preview": "posts/11-29-20_Asha/Linear_model_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2020-12-07T17:21:37-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "Tuning process and final model summary",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\n\nContents\nData Import\nRecipeRecipe Notes\n\nTuning HyperparametersApproach #1: Tuning Gamma FirstTuning Results\n\nFollow-up Tree-Parameter TuningCreate Grid\nSpecify Model\nTuning Results\n\nApproach #2: Tuning Tree Parameters FirstSpecify Model\nTuning Results\nFollowup Gamma Tuning\nFine Tuning Gamma\n\nEvaluating Approaches\nTune Stochastic ParametersSpecify Model\nTuning Results\n\nRetune Learning RateSpecify Model\n\n\nTune Number of Trees\nFinal Fit StatisticsGenerate Final Fit\nFit to test set\nFinal Fit Results\nFeature Importance Plot of Final Model (Top 35)\n\n\nData Import\n\n\ndata <- read.csv(\"data/train.csv\") %>%\n  select(-classification) %>%\n  mutate_if(is.character, factor) %>%\n  mutate(ncessch = as.numeric(ncessch))\n\nbonus <- read.csv(\"data/bonus_data_v2.csv\") %>%\n  mutate(ncessch = as.numeric(ncessch)) %>%\n  mutate(locale = gsub(\"^.{0,3}\", \"\", locale)) %>%\n  separate(col = locale, into = c(\"locale\", \"sublocale\"), sep = \": \")\n\ndisc <- read_csv(\"data/disc_drop.csv\") %>%\n  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))\n\n## join data\ndata <- data %>% \n  left_join(bonus) %>% \n  left_join(disc)\n\n\n\nData was merged from three files:\nOriginal Competition Training Dataset\n‚ÄúBonus Dataset‚Äù with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels\nSupplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs\nDropout Rate Data Source: https://www.oregon.gov/ode/reports-and-data/students/Pages/Dropout-Rates.aspx\nSuspension Rate Data Source: https://www.oregon.gov/ode/students-and-family/healthsafety/Pages/School-Discipline,-Bullying,-Restraint-and-Seclusion.aspx\n\nImportantly, the bonus data includes the variables described in the original data description page, as well as the following:\n2016-2017 District Finance Data:\nTotal revenue (rev_total)\nTotal local revenue (rev_local_total)\nTotal state revenue (rev_state_total)\nTotal federal revenue (rev_fed_total)\nTotal expenditures (exp_total)\nTotal current expenditures for elementary and secondary education (exp_current_elsec_total)\nTotal current expenditures for instruction (exp_current_instruction_total)\nTotal current expenditures for support services (exp_current_supp_serve_total)\nTotal capital outlay expenditures (outlay_capital_total)\nTotal salary amount (salaries_total)\nTotal employee benefits in dollars (benefits_employee_total)\nNumber of students for which the reporting local education agency is financially responsible (enrollment_fall_responsible)\n\nDistrict financial data was obtained using the educationdata R package.\nLastly, district finance data was included in both its raw form, and transformed by dividing each revenue and expenditure value by the number of students the LEA was financially responsible for (e.g., exp_total/enrollment_fall_responsible)\nRecipe\n\n\n\n\n\nrec <- recipe(score ~ ., train) %>%\n  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,\n                                         pupil_tch_ratio < 25 ~ 2,\n                                         pupil_tch_ratio < 30 ~ 3, \n                                         TRUE ~ 4),\n              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% \n  step_rm(contains(\"id\"), ncessch, ncesag, lea_name, sch_name) %>%\n  step_mutate(hpi = as.numeric(hpi),\n              lat = round(lat, 2),\n              lon = round(lon, 2),\n              median_income = log(median_income),\n              frl_prop = fr_lnch_prop + red_lnch_prop,\n              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,\n                                    TRUE ~ 0),\n              over_100 = under_200 + over_200) %>% \n  step_interact(terms = ~ lat:lon) %>% \n  step_rm(fr_lnch_prop, red_lnch_prop) %>% \n  step_string2factor(all_nominal()) %>% \n  step_zv(all_predictors()) %>%\n  step_unknown(all_nominal()) %>% \n  step_medianimpute(all_numeric()) %>%\n  step_dummy(all_nominal(), one_hot = TRUE) %>% \n  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% \n  step_interact(~ lang_cd_S:p_hispanic_latino) %>% \n  step_nzv(all_predictors(), freq_cut = 995/5)\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\n\n\nRecipe Notes\nPupil/Teacher ratio (pupil_tch_ratio) is binned and treated as a factor to remove noise (pupil_tch_rate). Both ratio and binned version of the variable remain in the data.\nID variables are removed\nLatitude (lat) and longitude (lon) are rounded to two decimal places to reduce noise and limit precision to within 2/3 of a mile.\nMedian income is log transformed, as the variations in income level are expected to have greater effect near the poverty threshold and less effect in higher income brackets.\nFree lunch proportions and reduced lunch proportions are collapsed into a single variable frl_prop given their expected similar effects.\nFree lunch proportions (fr_lnch_prop) and reduced lunch proportions (red_lnch_prop) are removed from the data set in lieu of their combined proportion.\n\nA dummy coded school-level variable (schl_perf) is created that denotes whether the percentage of students reaching proficiency on the EOY state test exceeds the percentage of students that fall below proficiency standards.\nZero variance predictors are removed\nMissing data is median imputed\nExplicit interaction is specified between the student-level special education flag and the per-student district expenditures for special services. Th effect of special education status is expected to vary depending on the funding available and potential quality of the district‚Äôs special services.\nExplicit interaction is specified for the effect of Spanish language code (lang_cd) and percentage of Hispanic/Latino students at the school (p_hispanic_latino). The justification is that Spanish-speaking students are expected to receive less effective supports in schools in which they are a greater minority.\nNear-zero variance predictors are removed\nTuning Hyperparameters\nBecause the loss reduction (gamma) and maximum tree depth are highly dependent on one another, two approaches were taken to tune the initial model. One approach involved first tuning loss reduction, followed by tree depth and hessian weights. The other involved tuning tree complexity and hessian weights without any severe loss reduction regularization, and then pruning it back by tuning gamma. All tuning was done using the full training dataset.\nApproach #1: Tuning Gamma First\n\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\ngrid <- expand.grid(loss_reduction = seq(0, 80, 5))\n\ngamma_mods <- map(grid$loss_reduction, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 10000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50,\n    nfold = 10,\n    verbose = 1,\n    params = list(\n      eta = 0.1,\n      gamma = .x,\n      nthread = 24\n    )\n  )\n})\n\n\n\nTuning Results\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  452         77.0087         0.0983        82.6056        0.6781\n 2:  448         77.0472         0.0837        82.6143        0.4099\n 3:  445         77.0877         0.0555        82.6204        0.5452\n 4:  524         76.3820         0.1016        82.6213        0.5638\n 5:  436         77.1400         0.0551        82.6243        0.3445\n 6:  452         77.0063         0.0802        82.6287        0.5083\n 7:  416         77.3305         0.0714        82.6291        0.7941\n 8:  447         77.0524         0.0613        82.6352        0.5932\n 9:  424         77.2553         0.0888        82.6371        0.8803\n10:  465         76.9038         0.0983        82.6379        0.5911\n    eta gamma\n 1: 0.1    10\n 2: 0.1    50\n 3: 0.1    30\n 4: 0.1    55\n 5: 0.1    70\n 6: 0.1    80\n 7: 0.1    45\n 8: 0.1    15\n 9: 0.1    20\n10: 0.1    40\n\n\nAs indicated in the results, a gamma of 10 produced the best fit the to test folds. A gamma of 50 was considered as a potential alternative given its lower RMSE SD; however, 50 was considered too extreme of a regularization setting.\nA follow-up gamma tuning process was conducted with a narrowed range of 5 to 15 and is reported below.\ngrid <- expand.grid(loss_reduction = seq(5, 15, 1))\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  426         77.1033         0.0673        82.6085        0.5934\n 2:  440         76.9364         0.0804        82.6233        0.6727\n 3:  411         77.2343         0.0918        82.6301        0.5782\n 4:  470         76.7071         0.0412        82.6407        0.4116\n 5:  452         76.8494         0.0946        82.6417        0.5207\n 6:  497         76.4667         0.0793        82.6423        0.3931\n 7:  478         76.6126         0.0703        82.6454        0.3906\n 8:  427         77.0770         0.0583        82.6646        0.5045\n 9:  389         77.4035         0.1140        82.6658        0.6899\n10:  381         77.5321         0.0441        82.6747        0.4971\n    eta gamma\n 1: 0.1    12\n 2: 0.1    13\n 3: 0.1    15\n 4: 0.1    14\n 5: 0.1    11\n 6: 0.1     9\n 7: 0.1     6\n 8: 0.1     8\n 9: 0.1     7\n10: 0.1     5\n\n\nAfter second round of tuning, a gamma value of 12 produced the best fit.\nBest gamma value = 12\nRMSE/SD to beat = 82.6085/0.5934\nFollow-up Tree-Parameter Tuning\nCreate Grid\n\n\n# Set learning rate, tune tree specific parameters\ngrid <- grid_max_entropy(min_n(c(4, 12)), # min_child_weight\n                         tree_depth(), # max_depth\n                         size = 30)\nhead(grid)\n\n\n# A tibble: 6 x 2\n  min_n tree_depth\n  <int>      <int>\n1    12         13\n2    10         12\n3     7         12\n4     8          5\n5     9         14\n6     5          7\n\nSpecify Model\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12, \n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTuning Results\n\n\n{\"x\":{\"visdat\":{\"efc316738e8d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"efc316738e8d\",\"attrs\":{\"efc316738e8d\":{\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":5},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"min_child_weight\"},\"yaxis\":{\"title\":\"max_depth\"},\"zaxis\":{\"title\":\"test_rmse_mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[42,47,33,31,39,18,41,31,49,50,50,8,18,38,2,22,22,49,10,37,13,38,23,29,31,10,1,5,44,3],\"y\":[9,7,9,5,7,5,5,6,4,10,11,5,8,11,7,3,11,13,9,3,11,14,14,15,2,13,11,13,1,1],\"z\":[82.5145417,82.5371338,82.5425514,82.5442901,82.5475464,82.5559967,82.5578004,82.5624109,82.5731157,82.5927047,82.6007805,82.601892,82.6220056,82.6244148,82.6763656,82.6869271,82.6877203,82.6914842,82.7015336,82.7148949,82.79383,82.814676,82.9005066,82.9449509,82.9704437,83.0080811,83.0145393,83.1440653,84.5998801,84.6120261],\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2},\"cmin\":82.5145417,\"cmax\":84.6120261,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666641\",\"rgba(70,19,97,1)\"],[\"0.083333333333335\",\"rgba(72,32,111,1)\"],[\"0.124999999999999\",\"rgba(71,45,122,1)\"],[\"0.16666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333334\",\"rgba(64,70,135,1)\"],[\"0.249999999999998\",\"rgba(60,82,138,1)\"],[\"0.291666666666669\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999997\",\"rgba(46,114,142,1)\"],[\"0.416666666666668\",\"rgba(42,123,142,1)\"],[\"0.458333333333332\",\"rgba(38,133,141,1)\"],[\"0.499999999999997\",\"rgba(37,144,140,1)\"],[\"0.541666666666668\",\"rgba(33,154,138,1)\"],[\"0.583333333333332\",\"rgba(39,164,133,1)\"],[\"0.625000000000003\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333331\",\"rgba(79,191,110,1)\"],[\"0.750000000000002\",\"rgba(98,199,98,1)\"],[\"0.791666666666666\",\"rgba(119,207,85,1)\"],[\"0.833333333333337\",\"rgba(147,214,70,1)\"],[\"0.875000000000001\",\"rgba(172,220,52,1)\"],[\"0.916666666666665\",\"rgba(199,225,42,1)\"],[\"0.958333333333336\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5145417,82.5371338,82.5425514,82.5442901,82.5475464,82.5559967,82.5578004,82.5624109,82.5731157,82.5927047,82.6007805,82.601892,82.6220056,82.6244148,82.6763656,82.6869271,82.6877203,82.6914842,82.7015336,82.7148949,82.79383,82.814676,82.9005066,82.9449509,82.9704437,83.0080811,83.0145393,83.1440653,84.5998801,84.6120261],\"size\":5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":82.5145417,\"cmax\":84.6120261,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666641\",\"rgba(70,19,97,1)\"],[\"0.083333333333335\",\"rgba(72,32,111,1)\"],[\"0.124999999999999\",\"rgba(71,45,122,1)\"],[\"0.16666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333334\",\"rgba(64,70,135,1)\"],[\"0.249999999999998\",\"rgba(60,82,138,1)\"],[\"0.291666666666669\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999997\",\"rgba(46,114,142,1)\"],[\"0.416666666666668\",\"rgba(42,123,142,1)\"],[\"0.458333333333332\",\"rgba(38,133,141,1)\"],[\"0.499999999999997\",\"rgba(37,144,140,1)\"],[\"0.541666666666668\",\"rgba(33,154,138,1)\"],[\"0.583333333333332\",\"rgba(39,164,133,1)\"],[\"0.625000000000003\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333331\",\"rgba(79,191,110,1)\"],[\"0.750000000000002\",\"rgba(98,199,98,1)\"],[\"0.791666666666666\",\"rgba(119,207,85,1)\"],[\"0.833333333333337\",\"rgba(147,214,70,1)\"],[\"0.875000000000001\",\"rgba(172,220,52,1)\"],[\"0.916666666666665\",\"rgba(199,225,42,1)\"],[\"0.958333333333336\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5145417,82.5371338,82.5425514,82.5442901,82.5475464,82.5559967,82.5578004,82.5624109,82.5731157,82.5927047,82.6007805,82.601892,82.6220056,82.6244148,82.6763656,82.6869271,82.6877203,82.6914842,82.7015336,82.7148949,82.79383,82.814676,82.9005066,82.9449509,82.9704437,83.0080811,83.0145393,83.1440653,84.5998801,84.6120261]}},\"type\":\"scatter3d\",\"mode\":\"markers\",\"frame\":null},{\"x\":[1,50],\"y\":[1,15],\"type\":\"scatter3d\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":82.5145417,\"cmax\":84.6120261,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666641\",\"rgba(70,19,97,1)\"],[\"0.083333333333335\",\"rgba(72,32,111,1)\"],[\"0.124999999999999\",\"rgba(71,45,122,1)\"],[\"0.16666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333334\",\"rgba(64,70,135,1)\"],[\"0.249999999999998\",\"rgba(60,82,138,1)\"],[\"0.291666666666669\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999997\",\"rgba(46,114,142,1)\"],[\"0.416666666666668\",\"rgba(42,123,142,1)\"],[\"0.458333333333332\",\"rgba(38,133,141,1)\"],[\"0.499999999999997\",\"rgba(37,144,140,1)\"],[\"0.541666666666668\",\"rgba(33,154,138,1)\"],[\"0.583333333333332\",\"rgba(39,164,133,1)\"],[\"0.625000000000003\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333331\",\"rgba(79,191,110,1)\"],[\"0.750000000000002\",\"rgba(98,199,98,1)\"],[\"0.791666666666666\",\"rgba(119,207,85,1)\"],[\"0.833333333333337\",\"rgba(147,214,70,1)\"],[\"0.875000000000001\",\"rgba(172,220,52,1)\"],[\"0.916666666666665\",\"rgba(199,225,42,1)\"],[\"0.958333333333336\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[82.5145417,84.6120261],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"z\":[82.5145417,84.6120261],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  426         77.1033         0.0673        82.6085        0.5934\n 2:  440         76.9364         0.0804        82.6233        0.6727\n 3:  411         77.2343         0.0918        82.6301        0.5782\n 4:  470         76.7071         0.0412        82.6407        0.4116\n 5:  452         76.8494         0.0946        82.6417        0.5207\n 6:  497         76.4667         0.0793        82.6423        0.3931\n 7:  478         76.6126         0.0703        82.6454        0.3906\n 8:  427         77.0770         0.0583        82.6646        0.5045\n 9:  389         77.4035         0.1140        82.6658        0.6899\n10:  381         77.5321         0.0441        82.6747        0.4971\n    eta gamma\n 1: 0.1    12\n 2: 0.1    13\n 3: 0.1    15\n 4: 0.1    14\n 5: 0.1    11\n 6: 0.1     9\n 7: 0.1     6\n 8: 0.1     8\n 9: 0.1     7\n10: 0.1     5\n\nA min_child_weight of 42 and max_depth of 9 demonstrate the best fit to the test folds and have a low SD relative to other hyperparameter tunings.\nTuning Summary:\ngamma = 12\nmin_child_weight = 42\nmax_depth = 9\nFinal RMSE/SD to beat: 82.5145/0.4865\nApproach #2: Tuning Tree Parameters First\nSpecify Model\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTuning Results\n\n\n{\"x\":{\"visdat\":{\"efc337c73f6\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"efc337c73f6\",\"attrs\":{\"efc337c73f6\":{\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":5},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"min_child_weight\"},\"yaxis\":{\"title\":\"max_depth\"},\"zaxis\":{\"title\":\"test_rmse_mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[31,50,33,39,47,41,31,42,18,18,8,49,50,38,2,49,37,10,22,13,22,38,29,23,31,10,1,5,3,44],\"y\":[6,10,9,7,7,5,5,9,5,8,5,4,11,11,7,13,3,9,3,11,11,14,15,14,2,13,11,13,1,1],\"z\":[82.5124091,82.5301652,82.5313851,82.5396958,82.5406203,82.5474305,82.5509614,82.568315,82.5730545,82.5890701,82.6059165,82.6113472,82.6179329,82.6347573,82.6624367,82.6903612,82.6917137,82.7149749,82.727191,82.7310548,82.7431975,82.7834778,82.8315184,82.9064498,82.9607202,82.9930764,83.0443854,83.1455025,84.606839,84.6138031],\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2},\"cmin\":82.5124091,\"cmax\":84.6138031,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666672\",\"rgba(70,19,97,1)\"],[\"0.0833333333333345\",\"rgba(72,32,111,1)\"],[\"0.125000000000002\",\"rgba(71,45,122,1)\"],[\"0.166666666666669\",\"rgba(68,58,128,1)\"],[\"0.208333333333336\",\"rgba(64,70,135,1)\"],[\"0.249999999999997\",\"rgba(60,82,138,1)\"],[\"0.291666666666664\",\"rgba(56,93,140,1)\"],[\"0.333333333333331\",\"rgba(49,104,142,1)\"],[\"0.374999999999998\",\"rgba(46,114,142,1)\"],[\"0.416666666666666\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333334\",\"rgba(39,164,133,1)\"],[\"0.625000000000002\",\"rgba(47,174,127,1)\"],[\"0.666666666666669\",\"rgba(53,183,121,1)\"],[\"0.708333333333336\",\"rgba(79,191,110,1)\"],[\"0.750000000000003\",\"rgba(98,199,98,1)\"],[\"0.791666666666664\",\"rgba(119,207,85,1)\"],[\"0.833333333333331\",\"rgba(147,214,70,1)\"],[\"0.874999999999998\",\"rgba(172,220,52,1)\"],[\"0.916666666666666\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5124091,82.5301652,82.5313851,82.5396958,82.5406203,82.5474305,82.5509614,82.568315,82.5730545,82.5890701,82.6059165,82.6113472,82.6179329,82.6347573,82.6624367,82.6903612,82.6917137,82.7149749,82.727191,82.7310548,82.7431975,82.7834778,82.8315184,82.9064498,82.9607202,82.9930764,83.0443854,83.1455025,84.606839,84.6138031],\"size\":5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":82.5124091,\"cmax\":84.6138031,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666672\",\"rgba(70,19,97,1)\"],[\"0.0833333333333345\",\"rgba(72,32,111,1)\"],[\"0.125000000000002\",\"rgba(71,45,122,1)\"],[\"0.166666666666669\",\"rgba(68,58,128,1)\"],[\"0.208333333333336\",\"rgba(64,70,135,1)\"],[\"0.249999999999997\",\"rgba(60,82,138,1)\"],[\"0.291666666666664\",\"rgba(56,93,140,1)\"],[\"0.333333333333331\",\"rgba(49,104,142,1)\"],[\"0.374999999999998\",\"rgba(46,114,142,1)\"],[\"0.416666666666666\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333334\",\"rgba(39,164,133,1)\"],[\"0.625000000000002\",\"rgba(47,174,127,1)\"],[\"0.666666666666669\",\"rgba(53,183,121,1)\"],[\"0.708333333333336\",\"rgba(79,191,110,1)\"],[\"0.750000000000003\",\"rgba(98,199,98,1)\"],[\"0.791666666666664\",\"rgba(119,207,85,1)\"],[\"0.833333333333331\",\"rgba(147,214,70,1)\"],[\"0.874999999999998\",\"rgba(172,220,52,1)\"],[\"0.916666666666666\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5124091,82.5301652,82.5313851,82.5396958,82.5406203,82.5474305,82.5509614,82.568315,82.5730545,82.5890701,82.6059165,82.6113472,82.6179329,82.6347573,82.6624367,82.6903612,82.6917137,82.7149749,82.727191,82.7310548,82.7431975,82.7834778,82.8315184,82.9064498,82.9607202,82.9930764,83.0443854,83.1455025,84.606839,84.6138031]}},\"type\":\"scatter3d\",\"mode\":\"markers\",\"frame\":null},{\"x\":[1,50],\"y\":[1,15],\"type\":\"scatter3d\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":82.5124091,\"cmax\":84.6138031,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666672\",\"rgba(70,19,97,1)\"],[\"0.0833333333333345\",\"rgba(72,32,111,1)\"],[\"0.125000000000002\",\"rgba(71,45,122,1)\"],[\"0.166666666666669\",\"rgba(68,58,128,1)\"],[\"0.208333333333336\",\"rgba(64,70,135,1)\"],[\"0.249999999999997\",\"rgba(60,82,138,1)\"],[\"0.291666666666664\",\"rgba(56,93,140,1)\"],[\"0.333333333333331\",\"rgba(49,104,142,1)\"],[\"0.374999999999998\",\"rgba(46,114,142,1)\"],[\"0.416666666666666\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333334\",\"rgba(39,164,133,1)\"],[\"0.625000000000002\",\"rgba(47,174,127,1)\"],[\"0.666666666666669\",\"rgba(53,183,121,1)\"],[\"0.708333333333336\",\"rgba(79,191,110,1)\"],[\"0.750000000000003\",\"rgba(98,199,98,1)\"],[\"0.791666666666664\",\"rgba(119,207,85,1)\"],[\"0.833333333333331\",\"rgba(147,214,70,1)\"],[\"0.874999999999998\",\"rgba(172,220,52,1)\"],[\"0.916666666666666\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[82.5124091,84.6138031],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"z\":[82.5124091,84.6138031],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  476         77.7006         0.0998        82.5124        0.6930\n 2:  164         76.2352         0.1608        82.5302        0.6939\n 3:  176         76.5310         0.1472        82.5314        0.7656\n 4:  386         76.8895         0.0762        82.5397        0.7031\n 5:  353         77.3357         0.1209        82.5406        0.7410\n 6:  841         78.0260         0.0806        82.5474        0.7158\n 7:  772         78.1773         0.0749        82.5510        0.5863\n 8:  180         76.7896         0.0835        82.5683        0.4212\n 9:  765         77.9577         0.0949        82.5731        0.8062\n10:  218         76.6201         0.0898        82.5891        0.6832\n    min_child_weight max_depth\n 1:               31         6\n 2:               50        10\n 3:               33         9\n 4:               39         7\n 5:               47         7\n 6:               41         5\n 7:               31         5\n 8:               42         9\n 9:               18         5\n10:               18         8\n\nWithout any gamma specification, the best min_child_weight value was 31 and the best max_depth was 6. They will be retained for the gamma tuning process.\nRMSE/SD = 82.5124/0.6930\nFollowup Gamma Tuning\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  533         77.3554         0.0682        82.5224        0.5267\n 2:  469         77.7686         0.0927        82.5352        0.8634\n 3:  471         77.7354         0.0776        82.5378        0.4401\n 4:  524         77.4174         0.0992        82.5383        0.6807\n 5:  499         77.5734         0.0750        82.5462        0.8300\n 6:  517         77.4582         0.0884        82.5475        0.7874\n 7:  524         77.3996         0.0529        82.5575        0.5187\n 8:  514         77.4428         0.1058        82.5591        0.4798\n 9:  460         77.8339         0.0813        82.5609        0.5974\n10:  473         77.7356         0.0656        82.5616        0.5277\n    eta gamma min_child_weight max_depth\n 1: 0.1     5               31         6\n 2: 0.1    80               31         6\n 3: 0.1    45               31         6\n 4: 0.1    50               31         6\n 5: 0.1    75               31         6\n 6: 0.1    20               31         6\n 7: 0.1     0               31         6\n 8: 0.1    25               31         6\n 9: 0.1    35               31         6\n10: 0.1    10               31         6\n\n\nThe best RMSE was obtained with a gamma value of 5. Thus a narrower grid search was conducted around this value.\nFine Tuning Gamma\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  483         77.6636         0.0682        82.5199        0.5321\n 2:  580         77.0910         0.0922        82.5308        0.7158\n 3:  526         77.4045         0.0615        82.5482        0.5523\n 4:  530         77.3647         0.0591        82.5531        0.7015\n 5:  561         77.1607         0.0504        82.5531        0.5370\n 6:  505         77.5384         0.1148        82.5568        0.7819\n 7:  524         77.3996         0.0529        82.5575        0.5187\n 8:  475         77.7281         0.0901        82.5679        0.6583\n 9:  511         77.4975         0.1031        82.5682        0.6308\n10:  498         77.6087         0.0835        82.5774        0.5429\n    eta gamma min_child_weight max_depth\n 1: 0.1     1               31         6\n 2: 0.1     8               31         6\n 3: 0.1     4               31         6\n 4: 0.1     9               31         6\n 5: 0.1     3               31         6\n 6: 0.1    10               31         6\n 7: 0.1     0               31         6\n 8: 0.1     6               31         6\n 9: 0.1     2               31         6\n10: 0.1     5               31         6\n\n\nAfter fine tuning gamma, the best RMSE was obtained using a value of 1. However, RMSE values did not appear to converge towards a particular value, as illustrated in the plot. Consequently, some caution is warranted for these hyperparameter values.\nBest RMSE/SD = 82.5199/0.5321\nEvaluating Approaches\nApproach #1: RMSE/SD = 82.5145/0.4865 Approach #2: RMSE/SD = 82.5199/0.5321\nTuning gamma first, followed by the tree hyperparameters yielded better RMSE during the cross validations. Thus, the hyperparameter values generated through the first tuning approach were retained.\nUpdated model summary:\ngamma = 12\nmax_depth = 6\nmin_child_weight = 10\nTune Stochastic Parameters\nAfter tuning loss reduction and the tree hyperparameters, the stochastic parameters were tuned to identify the best subsampling of columns and cases.\n\n\n\nSpecify Model\n\n\nsample_mods <- map2(grid$mtry, grid$sample_size, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12,\n      max_depth = 6,\n      min_child_weight = 10,\n      colsample_bytree = .x,\n      subsample = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTuning Results\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  180         76.8111         0.1001        82.4945        0.4857\n 2:  162         77.0779         0.1225        82.5165        0.7459\n 3:  169         76.9934         0.0828        82.5251        0.5528\n 4:  176         77.1740         0.1421        82.5602        0.6855\n 5:  168         77.0995         0.1517        82.5628        0.7122\n 6:  190         76.7031         0.0754        82.5678        0.2103\n 7:  164         77.2467         0.0799        82.5744        0.4960\n 8:  154         77.5365         0.0981        82.5795        0.5072\n 9:  166         77.4868         0.1162        82.5908        0.5536\n10:  178         77.4601         0.0839        82.5981        0.4447\n    gamma colsample_bytree subsample\n 1:    10           0.7568    0.9742\n 2:    10           0.8865    0.9124\n 3:    10           0.8919    0.8517\n 4:    10           0.4216    0.9423\n 5:    10           0.7514    0.8682\n 6:    10           0.6432    0.9851\n 7:    10           0.5676    0.9561\n 8:    10           0.8054    0.7719\n 9:    10           0.4541    0.8728\n10:    10           0.4216    0.8013\n\nBest values for colsample_bytree and subsample appear to be around values >.70. A narrower grid search was completed and is summarized below. Notably, gamma was mistakenly specified as 10 (instead of 12) during this tuning process. However, this wasn‚Äôt expected to dramatically shift the tenable range of best stochastic parameter values during fine tuning.\nFine Tuning Results\n\n\n{\"x\":{\"visdat\":{\"efc37bbcc0d8\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"efc37bbcc0d8\",\"attrs\":{\"efc37bbcc0d8\":{\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":5},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"colsample_bytree\"},\"yaxis\":{\"title\":\"subsample\"},\"zaxis\":{\"title\":\"test_rmse_mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[0.891891891891892,0.972972972972973,0.983783783783784,0.87027027027027,1,0.724324324324324,0.783783783783784,0.783783783783784,0.897297297297297,0.951351351351351,0.810810810810811,0.702702702702703,0.989189189189189,0.881081081081081,0.718918918918919],\"y\":[0.927007813049905,0.923192792285493,0.983015414936299,0.988202998521855,0.871605546561554,0.995889350320265,0.948194551981418,0.822707116210319,0.853748152319279,0.805553600337862,0.874723727739847,0.842035616245513,0.838072781023439,0.806630534243683,0.912923206869853],\"z\":[82.5002296,82.5245126,82.5343529,82.5375488,82.5563118,82.5563462,82.567076,82.571678,82.5793435,82.5839614,82.5863265,82.597895,82.5998147,82.6007409,82.608178],\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2},\"cmin\":82.5002296,\"cmax\":82.608178,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666008\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125000000000066\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333268\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666732\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999934\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333399\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666601\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625000000000066\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333268\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666732\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.874999999999934\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333399\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5002296,82.5245126,82.5343529,82.5375488,82.5563118,82.5563462,82.567076,82.571678,82.5793435,82.5839614,82.5863265,82.597895,82.5998147,82.6007409,82.608178],\"size\":5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":82.5002296,\"cmax\":82.608178,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666008\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125000000000066\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333268\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666732\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999934\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333399\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666601\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625000000000066\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333268\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666732\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.874999999999934\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333399\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5002296,82.5245126,82.5343529,82.5375488,82.5563118,82.5563462,82.567076,82.571678,82.5793435,82.5839614,82.5863265,82.597895,82.5998147,82.6007409,82.608178]}},\"type\":\"scatter3d\",\"mode\":\"markers\",\"frame\":null},{\"x\":[0.702702702702703,1],\"y\":[0.805553600337862,0.995889350320265],\"type\":\"scatter3d\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":82.5002296,\"cmax\":82.608178,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666008\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125000000000066\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333268\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666732\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999934\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333399\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666601\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625000000000066\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333268\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666732\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.874999999999934\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333399\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[82.5002296,82.608178],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"z\":[82.5002296,82.608178],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  201         76.2962         0.1157        82.5002        0.7741\n 2:  164         76.9645         0.0987        82.5245        0.3507\n 3:  202         76.2556         0.0651        82.5344        0.4445\n 4:  201         76.3411         0.1021        82.5375        0.7820\n 5:  171         76.8405         0.1879        82.5563        1.0331\n 6:  204         76.4118         0.0689        82.5563        0.4480\n 7:  182         76.6834         0.0717        82.5671        0.5603\n 8:  152         77.5125         0.0959        82.5717        0.7425\n 9:  162         77.1778         0.0651        82.5793        0.4789\n10:  143         77.6607         0.1106        82.5840        0.8543\n    gamma colsample_bytree subsample\n 1:    12           0.8919    0.9270\n 2:    12           0.9730    0.9232\n 3:    12           0.9838    0.9830\n 4:    12           0.8703    0.9882\n 5:    12           1.0000    0.8716\n 6:    12           0.7243    0.9959\n 7:    12           0.7838    0.9482\n 8:    12           0.7838    0.8227\n 9:    12           0.8973    0.8537\n10:    12           0.9514    0.8056\n\nBest value of colsample_bytree = .892\nBest value of subsample = .927\nUpdated model summary:\ngamma = 12\nmax_depth = 6\nmin_child_weight = 10\ncolsample_bytree = .892\nsubsample = .927\nRetune Learning Rate\nSpecify Model\n\n\nr <- seq(0.0001, 0.1, length.out = 20)\n\nlr_mods <- map(lr, function(learn_rate) {\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50,\n    nfold = 10,\n    verbose = 0,\n    params = list(\n      eta = learn_rate,\n      gamma = 12,\n      max_depth = 6,\n      min_child_weight = 10,\n      colsample_bytree = 0.8918919,\n      subsample = 0.9270078,\n      nthread = 24\n    )\n  )\n})\n\n\n\nThe learning rate could not be re-tuned under the time constraints, so it was lowered to .04, which was identified to be the best learning rate during tuning of early models.\nTune Number of Trees\n\n\nbst <- xgb.cv(\n  data = train_x,\n  label = train_y,\n  nrounds = 10000,\n  objective = \"reg:squarederror\",\n  early_stopping_rounds = 50,\n  nfold = 10,\n  verbose = 1,\n  params = list(\n    eta = .04,\n  gamma = 12,\n  max_depth = 6,\n  min_child_weight = 10,\n  colsample_bytree = 0.8918919,\n  subsample = 0.9270078,\n  nthread = 24))\n\n\n\n\n   iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n1: 1119        77.19118     0.05822473       82.45213     0.4973091\n\nBest iteration was 1119. Only 1119 trees will be specified in final fit.\nFinal Fit Statistics\nGenerate Final Fit\n\n\nfinal_mod <- xgboost(\n  data = train_x,\n  label = train_y,\n  nrounds = 1119,\n  objective = \"reg:squarederror\",\n  verbose = 1,\n  params = list(\n    eta = .04,\n  gamma = 12,\n  max_depth = 6,\n  min_child_weight = 10,\n  colsample_bytree = 0.8918919,\n  subsample = 0.9270078,\n  nthread = 24))\n\n\n\nFit to test set\n\n\nbaked_test <- prep(rec) %>%\n  bake(test)\n\ntest_x = data.matrix(baked_test[, -73])\n\npred <- predict(final_mod, as.matrix(test_x))\nactual <- baked_test$score\n\nMetrics::rmse(actual, pred)\n\n\n\nFinal Fit Results\n\n[1] 78.86593\n\nAfter tuning hyperparameters, reducing the learning rate, and determining the number of trees, the final RMSE on the test split is 78.86593. This was only slightly higher than the RMSE calculated during the final fit to the training data, which was 77.51970.\nThis model was submitted 20 minutes late to the Kaggle competition, but received a RMSE of 81.65183 on the 30% test set for the public leaderboard, and an RMSE of 82.07486 for the private leaderboard.\nFeature Importance Plot of Final Model (Top 35)\n\n\n\n\n\n\n",
    "preview": "posts/11-29-20_Chris/ML_models_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-07T18:09:46-08:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_data_prep_&_description_AY/",
    "title": "Data preparation and exploration",
    "description": "Here we joined our datasets and explored through visualization.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\nTable of Contents\n1. Packages\n2. Joining the datasets\n3.Data Exploration\n1. Packages\nFor the purpose of data loading and cleaning, we are using following packages in R: {tidyverse}, {here}, {rio}, and {skimr}\n2. Joining the datasets\nFor the purpose of demonstration, we will be using 1% of the data with sample_frac() to keep computing time low. All our datasets have school ids which we used as key to join the datasets.\nAfter loading our three datasets, we joined them together to make one cohesive dataset, to be used for ML modeling. After joining, the dataset contains student-level variables (e.g.¬†gender, ethnicity, enrollement in special education, etc.) as well as district-level variables ( school longitude and latitude, proportion of free and reduced lunch, etc.). All of these variables will be used in our ML models to predict student score in the statewide assessment. Here is the preview of our final dataset, ready to be used for ML modeling.\n\nTable 1: Data summary\nName\ndata\nNumber of rows\n1894\nNumber of columns\n88\n_______________________\n\nColumn type frequency:\n\ncharacter\n5\nfactor\n28\nlogical\n1\nnumeric\n54\n________________________\n\nGroup variables\n\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncounty\n28\n0.99\n11\n17\n0\n34\n0\nlocale\n28\n0.99\n14\n19\n0\n12\n0\ntitle1_status\n28\n0.99\n36\n47\n0\n3\n0\nlea_name\n30\n0.98\n20\n43\n0\n145\n0\nsch_name\n28\n0.99\n8\n60\n0\n695\n0\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\ngndr\n0\n1.00\nFALSE\n2\nM: 967, F: 927\nethnic_cd\n0\n1.00\nFALSE\n7\nW: 1183, H: 441, M: 103, A: 79\ntst_bnch\n0\n1.00\nFALSE\n6\n2B: 347, G4: 319, G6: 316, 3B: 307\ntst_dt\n0\n1.00\nFALSE\n52\n5/2: 160, 5/2: 138, 5/1: 125, 5/2: 114\nmigrant_ed_fg\n0\n1.00\nFALSE\n2\nN: 1848, Y: 46\nind_ed_fg\n1\n1.00\nFALSE\n2\nN: 1872, Y: 21, y: 0\nsp_ed_fg\n1\n1.00\nFALSE\n2\nN: 1652, Y: 241\ntag_ed_fg\n6\n1.00\nFALSE\n2\nN: 1791, Y: 97\necon_dsvntg\n6\n1.00\nFALSE\n2\nY: 1094, N: 794\nayp_lep\n1534\n0.19\nFALSE\n7\nF: 165, Y: 69, E: 64, N: 29\nstay_in_dist\n6\n1.00\nFALSE\n2\nY: 1845, N: 43\nstay_in_schl\n6\n1.00\nFALSE\n2\nY: 1830, N: 58\ndist_sped\n6\n1.00\nFALSE\n2\nN: 1867, Y: 21\ntrgt_assist_fg\n6\n1.00\nFALSE\n3\nN: 1812, Y: 74, y: 2\nayp_dist_partic\n0\n1.00\nFALSE\n2\nY: 1888, N: 6\nayp_schl_partic\n0\n1.00\nFALSE\n2\nY: 1867, N: 27\nayp_dist_prfrm\n0\n1.00\nFALSE\n2\nY: 1842, N: 52\nayp_schl_prfrm\n0\n1.00\nFALSE\n2\nY: 1808, N: 86\nrc_dist_partic\n0\n1.00\nFALSE\n2\nY: 1888, N: 6\nrc_schl_partic\n0\n1.00\nFALSE\n2\nY: 1867, N: 27\nrc_dist_prfrm\n0\n1.00\nFALSE\n2\nY: 1842, N: 52\nrc_schl_prfrm\n0\n1.00\nFALSE\n2\nY: 1808, N: 86\nlang_cd\n1842\n0.03\nFALSE\n1\nS: 52\ntst_atmpt_fg\n0\n1.00\nFALSE\n2\nY: 1886, P: 8\ngrp_rpt_dist_partic\n0\n1.00\nFALSE\n2\nY: 1888, N: 6\ngrp_rpt_schl_partic\n0\n1.00\nFALSE\n2\nY: 1867, N: 27\ngrp_rpt_dist_prfrm\n0\n1.00\nFALSE\n2\nY: 1882, N: 12\ngrp_rpt_schl_prfrm\n0\n1.00\nFALSE\n2\nY: 1861, N: 33\nVariable type: logical\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\ncalc_admn_cd\n1894\n0\nNaN\n:\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\nhist\nid\n0\n1.00\n1.273822e+05\n73162.85\n‚ñá‚ñá‚ñá‚ñá‚ñá\nattnd_dist_inst_id\n0\n1.00\n2.127740e+03\n212.96\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nattnd_schl_inst_id\n0\n1.00\n1.400890e+03\n1393.20\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\nenrl_grd\n0\n1.00\n5.490000e+00\n1.68\n‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ\npartic_dist_inst_id\n6\n1.00\n2.131510e+03\n225.53\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\npartic_schl_inst_id\n6\n1.00\n1.403160e+03\n1394.28\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\nscore\n0\n1.00\n2.500430e+03\n119.82\n‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ\nncessch\n26\n0.99\n4.106893e+11\n395453731.73\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\nlat\n30\n0.98\n4.475000e+01\n1.04\n‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñá\nlon\n30\n0.98\n-1.225200e+02\n1.17\n‚ñÖ‚ñá‚ñÅ‚ñÅ‚ñÅ\nncesag\n28\n0.99\n4.106891e+06\n3956.50\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\nzip\n28\n0.99\n9.731347e+04\n232.97\n‚ñá‚ñá‚ñÜ‚ñÅ‚ñÇ\ntotal_n\n28\n0.99\n5.721300e+02\n357.46\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nfr_lnch_n\n75\n0.96\n2.310900e+02\n148.02\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\nred_lnch_n\n75\n0.96\n4.091000e+01\n25.11\n‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÅ\npupil_tch_ratio\n28\n0.99\n2.026000e+01\n3.60\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\nipr_est\n28\n0.99\n3.220900e+02\n155.65\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\nleaid\n28\n0.99\n4.106891e+06\n3956.50\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\nst_schid\n28\n0.99\n1.388760e+03\n1387.95\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\nlea_cwiftest\n30\n0.98\n9.400000e-01\n0.07\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÜ\np_american_indian_alaska_native\n28\n0.99\n1.000000e-02\n0.04\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_asian\n28\n0.99\n4.000000e-02\n0.07\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_native_hawaiian_pacific_islander\n28\n0.99\n1.000000e-02\n0.01\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_black_african_american\n28\n0.99\n2.000000e-02\n0.04\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_hispanic_latino\n28\n0.99\n2.500000e-01\n0.19\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\np_white\n28\n0.99\n6.100000e-01\n0.20\n‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá\np_multiracial\n28\n0.99\n7.000000e-02\n0.03\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\nfr_lnch_prop\n75\n0.96\n4.400000e-01\n0.21\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\nred_lnch_prop\n75\n0.96\n8.000000e-02\n0.03\n‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÅ\npercent_level_4\n31\n0.98\n2.211000e+01\n13.36\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\npercent_level_3\n31\n0.98\n3.123000e+01\n9.00\n‚ñÇ‚ñÜ‚ñá‚ñÅ‚ñÅ\npercent_level_2\n31\n0.98\n2.268000e+01\n7.09\n‚ñÅ‚ñÜ‚ñá‚ñÅ‚ñÅ\npercent_level_1\n31\n0.98\n2.399000e+01\n13.48\n‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ\nsch_percent_level_4\n28\n0.99\n2.243000e+01\n12.46\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\nsch_percent_level_3\n28\n0.99\n3.118000e+01\n6.83\n‚ñÅ‚ñÇ‚ñá‚ñá‚ñÇ\nsch_percent_level_2\n28\n0.99\n2.259000e+01\n5.31\n‚ñÅ‚ñÉ‚ñá‚ñá‚ñÅ\nsch_percent_level_1\n28\n0.99\n2.380000e+01\n12.05\n‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ\nunder_25\n44\n0.98\n2.100000e-01\n0.06\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\nunder_50\n44\n0.98\n2.300000e-01\n0.06\n‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÇ\nunder_75\n44\n0.98\n1.600000e-01\n0.03\n‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ\nunder_100\n44\n0.98\n1.200000e-01\n0.02\n‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ\nunder_200\n44\n0.98\n2.000000e-01\n0.07\n‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÅ\nover_200\n44\n0.98\n7.000000e-02\n0.08\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\npercent_less_than_9th_grade\n40\n0.98\n4.040000e+00\n3.37\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\npercent_high_school_graduate_or_higher\n40\n0.98\n8.972000e+01\n5.79\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÖ\npercent_bachelors_degree_or_higher\n40\n0.98\n3.122000e+01\n16.15\n‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÅ\npercent_associates_degree\n40\n0.98\n8.780000e+00\n1.99\n‚ñÇ‚ñá‚ñá‚ñÅ‚ñÅ\npercent_graduate_or_professional_degree\n40\n0.98\n1.171000e+01\n7.87\n‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÅ\npercent_bachelors_degree\n40\n0.98\n1.815000e+01\n8.52\n‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÇ\npercent_high_school_graduate\n40\n0.98\n2.511000e+01\n7.67\n‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÅ\npercent_9th_to_12th_grade_no_diploma\n40\n0.98\n6.900000e+00\n3.42\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\npercent_some_college\n40\n0.98\n2.676000e+01\n4.37\n‚ñÅ‚ñÉ‚ñá‚ñá‚ñÅ\nmedian_income\n28\n0.99\n2.812101e+04\n4487.40\n‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÜ\nmedian_rent\n28\n0.99\n9.835000e+02\n153.33\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñá\n\n3.Data Exploration\nCorrelation: Figure 1 displays significant (p < 0.05) correlation between numeric variables. Red dots show significant pearson‚Äôs correlation coefficient between 0 to 1 and white dots show significant coefficient between -1 to 0. Blank spaces are not significant.\n\n\n\nWe looked if pupil-teacher ratio and score differ by county.\n\n\n\nWe explored adult qualification by economic disadvantage and grade.\n\n\n\n\n\n",
    "preview": "posts/11-29-20_data_prep_&_description_AY/data_preparation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-07T17:21:37-08:00",
    "input_file": {},
    "preview_width": 3840,
    "preview_height": 3456
  },
  {
    "path": "posts/11-29-20_ML_models_AY/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\nTable of Contents\n1.Splitting and resampling1a. Splitting\n1b. Resampling\n\n2.Pre-processing\n3. Linear model\n1.Splitting and resampling\nData splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.\n1a. Splitting\nWe will split our dataset into training and testing set. The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model‚Äôs performance.\nWe will use initial_split() function from the rsample package which creates a split object. The split object d_split, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).\n\n\nset.seed(3000)\n# Create split object specifying (75%) and testing (25%)\n\ndata_split <- initial_split(data, prop = 3/4) \n\ndata_split\n\n<Analysis/Assess/Total>\n<1421/473/1894>\n\nWe will extract the training and testing set from the split object, d_split by using the training() and testing() functions.\n\n\n# Extract training and testing set\nset.seed(3000)\n\ndata_train <- training(data_split)  # Our training dataset has 1421 observations.\n\ndata_test <- testing(data_split)    # Our test dataset has 473 observations.\n\n1b. Resampling\nAt some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we‚Äôll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = number of time we resample.\n\n\n# Resample the data with 10-fold cross validation.\nset.seed(3000)\ncv <- vfold_cv(data_train)\n\n2.Pre-processing\nPreprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as all_numeric(), all_predictors() as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (step_medianimpute), rescaling (step_scale), standardizing (step_normalize), PCA (step_pca) and creating dummy variables (step_dummy). A full list of pre-processing can be found here.\nWe will use recipe() function to indicate our outcome and predictor variables in our recipe. We will use ~. to indicate that we are using all variables to predict the outcome variable score. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, rec, it shows numbers of predictor variables have been specified. It doen‚Äôt actually apply the predictors yet. We will use same receipe throughout this post.\n\n\nset.seed(3000)\n\nrec <- recipe(score ~ ., data_train) %>% \n  step_mutate(tst_dt = as.numeric(lubridate::\n                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\n  step_medianimpute(all_numeric(), -all_outcomes(), \n                    -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\n  step_dummy(all_nominal(), -has_role(\"id vars\")) %>% # dummy codes categorical variables\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\n\nExtract the pre-processed dataset\nTo extract the pre-processed dataset, we can prep() the recipe for our datset then bake() the prepped recipe to extract the pre-processed data.\nHowever, in tidymodels we can use workflows() where we don‚Äôt need to prep() or bake() the recipe.\n\n\nprep(rec) #%>%\n\nData Recipe\n\nInputs:\n\n      role #variables\n   id vars          8\n   outcome          1\n predictor         79\n\nTraining data contained 1421 data points and 1421 incomplete rows. \n\nOperations:\n\nVariable mutation for tst_dt [trained]\nSparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\nMedian Imputation for enrl_grd, tst_dt, lat, lon, ... [trained]\nDummy variables from gndr, ethnic_cd, tst_bnch, ... [trained]\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\n\n # bake(data_train)   # We are using workflow so no need to bake.\n\nThe next step is to specify our ML models, using the parsnip package. To specify the model, there are four primary components: model type, arguments, engine, and mode.\n3. Linear model\n\n\nset.seed(3000)\n\n# Specify the model\n\nmod_linear <- linear_reg() %>%\n  set_engine(\"lm\") %>%  # engine for linear regression\n  set_mode(\"regression\")  # regression for continous outcome varaible.\n\n# Workflow\nlm_wf <- workflow() %>% # set the workflow\n  add_recipe(rec) %>% # add recipe\n  add_model(mod_linear) # add model\n  \n\n# Fit the linear model\nmod_linear_fit<- fit_resamples(\n  mod_linear,\n  preprocessor = rec,\n  resamples = cv,\n  metrics = metric_set(rmse),\n  control = control_resamples(verbose = TRUE,\n                                    save_pred = TRUE))\n\nCollect metrics on linear model\n\n\nset.seed(3000)\n\nmod_linear_fit %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") \n\n# A tibble: 1 x 5\n  .metric .estimator  mean     n std_err\n  <chr>   <chr>      <dbl> <int>   <dbl>\n1 rmse    standard    96.3    10    2.49\n\n#RMSE = 96.28\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-07T17:21:37-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_datasets_AY/",
    "title": "Datasets",
    "description": "Here is the preview of datasets used in our final project.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\nTable of Contents\n1. Introduction\n2. Statewide testing data (Original dataset)\n3. Fall membership report data\n4. Bonus Dataset\n5. Supplemental small dataset\n1. Introduction\nFor this project, we collected and joined three structured datasets and one supplemental small dataset only for XG boost model. Datasets are selected on the basis of variables that were correlated with the outcome variable. In our datasets, our outcome variable is children‚Äôs score on the statewide assessment. Please see below for more information on each dataset.\n2. Statewide testing data (Original dataset)\nStudents in every state across the nation are tested annually in reading and math in grades 3-8. Dataset used in the project are simulated from an actual statewide testing administration across the state of Oregon and the overall distribution are highly similar. Our continous outcome variable is score which is also presented in categorical form as classification. For the project, our models are run to predict the continous score variable. Our data contains other variables of interest (predictors) such as gender, ethnicity, economic disadvange, and location. In our simulated dataset, school ids are real which we used as key to join other datasets. Here is the preview of our statewide testing dataset.\n\nTable 1: Data summary\nName\nd\nNumber of rows\n1894\nNumber of columns\n39\n_______________________\n\nColumn type frequency:\n\ncharacter\n28\nlogical\n1\nnumeric\n10\n________________________\n\nGroup variables\n\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ngndr\n0\n1.00\n1\n1\n0\n2\n0\nethnic_cd\n0\n1.00\n1\n1\n0\n7\n0\ntst_bnch\n0\n1.00\n2\n2\n0\n6\n0\ntst_dt\n0\n1.00\n16\n17\n0\n48\n0\nmigrant_ed_fg\n0\n1.00\n1\n1\n0\n2\n0\nind_ed_fg\n0\n1.00\n1\n1\n0\n2\n0\nsp_ed_fg\n0\n1.00\n1\n1\n0\n2\n0\ntag_ed_fg\n3\n1.00\n1\n1\n0\n2\n0\necon_dsvntg\n4\n1.00\n1\n1\n0\n2\n0\nayp_lep\n1540\n0.19\n1\n1\n0\n8\n0\nstay_in_dist\n3\n1.00\n1\n1\n0\n2\n0\nstay_in_schl\n3\n1.00\n1\n1\n0\n2\n0\ndist_sped\n3\n1.00\n1\n1\n0\n2\n0\ntrgt_assist_fg\n3\n1.00\n1\n1\n0\n2\n0\nayp_dist_partic\n0\n1.00\n1\n1\n0\n2\n0\nayp_schl_partic\n0\n1.00\n1\n1\n0\n2\n0\nayp_dist_prfrm\n0\n1.00\n1\n1\n0\n2\n0\nayp_schl_prfrm\n0\n1.00\n1\n1\n0\n2\n0\nrc_dist_partic\n0\n1.00\n1\n1\n0\n2\n0\nrc_schl_partic\n0\n1.00\n1\n1\n0\n2\n0\nrc_dist_prfrm\n0\n1.00\n1\n1\n0\n2\n0\nrc_schl_prfrm\n0\n1.00\n1\n1\n0\n2\n0\nlang_cd\n1851\n0.02\n1\n1\n0\n1\n0\ntst_atmpt_fg\n0\n1.00\n1\n1\n0\n2\n0\ngrp_rpt_dist_partic\n0\n1.00\n1\n1\n0\n2\n0\ngrp_rpt_schl_partic\n0\n1.00\n1\n1\n0\n2\n0\ngrp_rpt_dist_prfrm\n0\n1.00\n1\n1\n0\n2\n0\ngrp_rpt_schl_prfrm\n0\n1.00\n1\n1\n0\n2\n0\nVariable type: logical\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\ncalc_admn_cd\n1894\n0\nNaN\n:\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\nhist\nid\n0\n1.00\n1.267936e+05\n72111.86\n‚ñá‚ñá‚ñá‚ñá‚ñá\nattnd_dist_inst_id\n0\n1.00\n2.116080e+03\n188.53\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nattnd_schl_inst_id\n0\n1.00\n1.345800e+03\n1388.28\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\nenrl_grd\n0\n1.00\n5.440000e+00\n1.67\n‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ\npartic_dist_inst_id\n4\n1.00\n2.116160e+03\n188.71\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\npartic_schl_inst_id\n4\n1.00\n1.345210e+03\n1386.64\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\nscore\n0\n1.00\n2.498910e+03\n113.99\n‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÅ\nncessch\n29\n0.98\n4.106795e+11\n385634091.96\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\nlat\n31\n0.98\n4.476000e+01\n1.00\n‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñá\nlon\n31\n0.98\n-1.224600e+02\n1.28\n‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n3. Fall membership report data\nThe Oregon Department for Education (ODE) publicly releases student enrollment reports detailing the number of K-12 students who are enrolled on the first school day in October of each year.This report is known as Fall membership report which contains data on race/ethinicity percentages for schools in Oregon. Here is the preview of our data.\n\nTable 2: Data summary\nName\nethnicities\nNumber of rows\n1459\nNumber of columns\n9\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n8\n________________________\n\nGroup variables\n\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nsch_name\n0\n1\n8\n60\n0\n1381\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\nhist\nattnd_schl_inst_id\n0\n1\n1661.26\n1558.62\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÇ\np_american_indian_alaska_native\n0\n1\n0.02\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_asian\n0\n1\n0.03\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_native_hawaiian_pacific_islander\n0\n1\n0.01\n0.01\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_black_african_american\n0\n1\n0.02\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_hispanic_latino\n0\n1\n0.21\n0.18\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\np_white\n0\n1\n0.65\n0.22\n‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÖ\np_multiracial\n0\n1\n0.06\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n4. Bonus Dataset\nWe retrieved another dataset that contains k-12 data collected by zip code, NCES school IDs, State school IDs, and county level. It contains variables that are not present in the statewide testing dataset such as teacher-pupil ratio, percentage of people with high school, no diploma, and higher education. These variables may have effect on our outcome variable (score). Please see below preview other variables present in the dataset.\n\nTable 3: Data summary\nName\nbonus_data\nNumber of rows\n3889\nNumber of columns\n51\n_______________________\n\nColumn type frequency:\n\ncharacter\n7\nnumeric\n44\n________________________\n\nGroup variables\n\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nncessch\n0\n1.00\n18\n21\n0\n1257\n0\ncounty\n0\n1.00\n11\n17\n0\n36\n0\nlocale\n0\n1.00\n14\n19\n0\n12\n0\ntitle1_status\n0\n1.00\n3\n47\n0\n4\n0\npupil_tch_ratio\n0\n1.00\n1\n6\n0\n789\n0\nlea_name\n54\n0.99\n20\n43\n0\n195\n0\nsch_name\n12\n1.00\n8\n60\n0\n1203\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\nhist\nncesag\n0\n1.00\n4106916.49\n4236.76\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nzip\n0\n1.00\n97366.87\n258.54\n‚ñá‚ñá‚ñá‚ñÇ‚ñÖ\ntotal_n\n1\n1.00\n406.93\n318.97\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nfr_lnch_n\n428\n0.89\n180.93\n138.74\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nred_lnch_n\n428\n0.89\n31.81\n25.57\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nipr_est\n12\n1.00\n303.52\n135.95\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\nleaid\n12\n1.00\n4106915.58\n4240.21\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nst_schid\n12\n1.00\n1610.13\n1592.21\n‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ\nlea_cwiftest\n54\n0.99\n0.92\n0.07\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÖ\np_american_indian_alaska_native\n12\n1.00\n0.01\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_asian\n12\n1.00\n0.03\n0.06\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_native_hawaiian_pacific_islander\n12\n1.00\n0.01\n0.02\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_black_african_american\n12\n1.00\n0.02\n0.05\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\np_hispanic_latino\n12\n1.00\n0.20\n0.18\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\np_white\n12\n1.00\n0.66\n0.21\n‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÜ\np_multiracial\n12\n1.00\n0.06\n0.04\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nfr_lnch_prop\n428\n0.89\n0.45\n0.21\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\nred_lnch_prop\n428\n0.89\n0.08\n0.04\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\nenrl_grd\n236\n0.94\n5.13\n1.66\n‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÇ\npercent_level_4\n556\n0.86\n22.06\n14.25\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\npercent_level_3\n556\n0.86\n30.18\n10.59\n‚ñÅ‚ñá‚ñá‚ñÅ‚ñÅ\npercent_level_2\n556\n0.86\n22.93\n8.54\n‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ\npercent_level_1\n556\n0.86\n24.83\n14.81\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\nsch_percent_level_4\n152\n0.96\n22.89\n13.04\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\nsch_percent_level_3\n152\n0.96\n30.46\n7.65\n‚ñÅ‚ñÇ‚ñá‚ñÉ‚ñÅ\nsch_percent_level_2\n152\n0.96\n22.74\n6.80\n‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÅ\nsch_percent_level_1\n152\n0.96\n23.91\n13.16\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\nunder_25\n180\n0.95\n0.22\n0.07\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\nunder_50\n180\n0.95\n0.24\n0.06\n‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÅ\nunder_75\n180\n0.95\n0.17\n0.04\n‚ñÅ‚ñÖ‚ñá‚ñÅ‚ñÅ\nunder_100\n180\n0.95\n0.12\n0.03\n‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÅ\nunder_200\n180\n0.95\n0.19\n0.08\n‚ñÇ‚ñá‚ñá‚ñá‚ñÅ\nover_200\n180\n0.95\n0.06\n0.07\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\npercent_less_than_9th_grade\n39\n0.99\n3.76\n3.41\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\npercent_high_school_graduate_or_higher\n39\n0.99\n89.72\n5.83\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñá\npercent_bachelors_degree_or_higher\n39\n0.99\n28.77\n15.64\n‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÅ\npercent_associates_degree\n39\n0.99\n8.88\n2.33\n‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ\npercent_graduate_or_professional_degree\n39\n0.99\n10.71\n7.63\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\npercent_bachelors_degree\n39\n0.99\n17.11\n8.56\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\npercent_high_school_graduate\n39\n0.99\n26.60\n8.26\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\npercent_9th_to_12th_grade_no_diploma\n39\n0.99\n7.04\n3.63\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\npercent_some_college\n39\n0.99\n27.29\n5.14\n‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñÅ\nmedian_income\n0\n1.00\n27158.20\n4434.27\n‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÉ\nmedian_rent\n0\n1.00\n942.33\n166.09\n‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñá\n\n5. Supplemental small dataset\nSupplemental small dataset contains high school dropout rates and out-of-school suspension rates by state districts IDs. This dataset has been used only in XG boost model. For details, please check the post\n\n\n",
    "preview": {},
    "last_modified": "2020-12-07T17:21:37-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_welcome_AY/",
    "title": "Hello!",
    "description": "Welcome to our Machine Learning blog",
    "author": [
      {
        "name": "AY, AG, and CI",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\nThis blog was created as part of the final project for the EDLD 654 Applied Machine Learning for Educational Data Science, taught by Prof.¬†Daniel Anderson and Prof.¬†Joseph Nese at the University of Oregon. The blog contains application of Machine Learning models to educational data for predictive analysis. Following models were presented in the final project:\nLinear regression (add link to my post, if posting separate.)\nRandom Forest (add link Ale‚Äôs post if posting separate)\nGradient Boosting (add link Chris‚Äôs post, if posting separete)\nThe contributors of the blog are:\nAsha Yadav\nAlejandra\nChris Ives\n\n\n",
    "preview": {},
    "last_modified": "2020-12-04T17:50:18-08:00",
    "input_file": {}
  }
]
