[
  {
    "path": "posts/11-29-20_data_prep_&_description_AY/",
    "title": "Data preparation and description",
    "description": "Now we got our datasets, let's see how to prepare (join) them for analysis.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": {},
    "last_modified": "2020-11-30T16:58:32-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_ML_models_AY/",
    "title": "Machine learning models",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": {},
    "last_modified": "2020-11-30T15:20:59-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_Alejandra/",
    "title": "Random Forest Model",
    "description": "This is how fitting a Random Forest model went for me!",
    "author": [
      {
        "name": "Alejandra Garcia Isaza",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nLet‚Äôs dive in on the model fitting process!\r\nFirst round of tuning\r\n\r\nTo complete this project, I chose to run a Random Forest model. I decided to go with the Out-of-bag (OOB) samples to conduct model fitting and model assessment due to its relatively low run time.\r\nInitially I was only working locally on my computer with only 5% of the sample. Overall, my model tunning process was taking between 15 and 25 minutes each round of tunning, but unfortunately my R session froze several times and I could not finish the model tuning process.\r\nAt this point, Daniel suggested I use HPC via the Open On Demand interface. As always, I was too afraid to try something new, but Daniel patiently showed me how to work with the new interface and explained for the 97th time what was the difference between the train and test files we have been working with all term long. I think I finally got it! I‚Äôm glad it only took me ten weeks üò¨ to understand this.\r\nI followed the same steps we have been following all term long, reading in the data, sampling a fraction of the data (note that I used 50% of the data because I was working with HPC), and joining with other datasets. Here I joined a dataset we called ‚Äúbonus‚Äù that inludes variables from different datasets. To learn more about this dataset and its variables, please go to <<<< link to page >>>>>.\r\nA huge shout-out to Chris Ives for taking the time to find and process the bonus datasets we are using here.\r\n\r\n\r\nset.seed(3000)\r\n\r\nfull_train <- import(here(\"data\", \"train.csv\"), setclass = \"tbl_df\") %>%\r\n  select(-classification) %>%\r\n  mutate_if(is.character, factor) %>%\r\n  mutate(ncessch = as.double(ncessch)) %>%\r\n  sample_frac(0.5)\r\n\r\nbonus <- import(here(\"data\", \"bonus_data.csv\")) %>%\r\n  mutate(pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>%\r\n  mutate(ncessch = as.double(ncessch))\r\n\r\n## joining data\r\ndata <- left_join(full_train, bonus)\r\n\r\n\r\n\r\nHere I created the initial split and the train and test objects for model fitting and assessment. Notice that I did not create a cross-validation resample because I will be using only the OOB samples.\r\n\r\n\r\nset.seed(3000)\r\ndata_split <- initial_split(data)\r\n\r\nset.seed(3000)\r\ndata_train <- training(data_split)\r\ndata_test <- testing(data_split)\r\n\r\n\r\n\r\nI created a recipe following the recommended preprocessing steps for a Random Forest model. I found this guide that Joe shared with our group very useful.\r\nThis recipe is pretty straightforward, the only thing that I think is worth highlighting is that we took the liberty to assume that most of the students‚Äô home language was English; that is why we specified this code lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"). I took extra care not to lose the Spanish speaking students in the lang_cd variable to the step_nzv() near-zero variance variable removal due to the role of language in academic achievement.\r\nIn this model, we had one outcome score, 77 predictors and XX Id variables.\r\n\r\n\r\nrec <- recipe(score ~ ., data_train) %>%\r\n  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),\r\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\r\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>% \r\n  update_role(contains(\"id\"), ncessch, ncesag, sch_name, new_role = \"id_vars\") %>%\r\n  step_zv(all_predictors(), -starts_with(\"lang_cd\")) %>%\r\n  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(\"id_vars\")) %>%\r\n  step_novel(all_nominal()) %>%\r\n  step_unknown(all_nominal()) %>% \r\n  step_dummy(all_nominal()) %>%\r\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\r\n\r\n\r\n\r\nLet‚Äôs dive in on the model fitting process!\r\nFirst, I wanted to develop a baseline model using the default hypermarameters. I focused only on three hyperparameters: predictors mtry = floor(p/3), in this model, around 26 predictors; number of treestrees = 500, and minimun node size min_n = 5.\r\nThe rmse for the default model was 88.80 Fitting the default model took less than 3 minutes!!\r\n\r\n\r\n# default model\r\nrf_def <- rand_forest() %>%\r\n  set_engine(\"ranger\",\r\n             num.threads = 8,\r\n             importance = \"permutation\",\r\n             verbose = TRUE) %>%\r\n  set_mode(\"regression\")\r\n\r\n# workflow for default model\r\nrf_def_wkflw <- workflow() %>% \r\n  add_model(rf_def) %>% \r\n  add_recipe(rec)\r\n\r\n# fitting the default model\r\nrf_def_fit <- fit(\r\n  rf_def_wkflw,\r\n  data_train)\r\n\r\n\r\n\r\nFirst round of tuning\r\nRandom forest models are know to have good out-of-the-box performance, however, I wanted to tune at least 2 hyperparameters to evaluate how much lower the rmse could be.\r\nI decided not to spend time tuning for number number of trees as the literature suggests that growing p * 10 trees is pretty safe. The number of trees needs to be large enough to stabilize the error rate, for this model I could have used 770 trees, but I went for 1000 trees just to be extra safe.\r\nTo tune for mtry and min_n, I followed what Boehmke & Greenwell (2020) suggest in the chapter focused on random forest models in the book Hands-On Machine Learning with R\r\n‚ÄúStart with five evenly spaced values of mtry across the range 2 ‚Äì p centered at the recommended default‚Äù (Boehmke & Greenwell, 2020)\r\n‚ÄúWhen adjusting node size start with three values between 1‚Äì10 and adjust depending on impact to accuracy and run time.‚Äù\r\n\r\n   Var1 Var2\r\n1     2    1\r\n2    14    1\r\n3    26    1\r\n4    38    1\r\n5    50    1\r\n6     2    4\r\n7    14    4\r\n8    26    4\r\n9    38    4\r\n10   50    4\r\n11    2    7\r\n12   14    7\r\n13   26    7\r\n14   38    7\r\n15   50    7\r\n16    2   10\r\n17   14   10\r\n18   26   10\r\n19   38   10\r\n20   50   10\r\n\r\nBelow is a function that Daniel helped me build. This is an all-in-one function that builds the model, updates the default model workflow with the new hyperparameters, fits the models with the selected hyperparameters, and outputs a table with the tuned hyperparaemters and the corresponding rmse value. It‚Äôs pretty neat!\r\n\r\n\r\nhyp_rf_search <- function(mtry_val, min_n_val, wf) {\r\n  mod <- rand_forest() %>% \r\n    set_engine(\"ranger\",\r\n               num.threads = 8,\r\n               importance = \"permutation\",\r\n               verbose = TRUE) %>% \r\n    set_mode(\"regression\") %>% \r\n    set_args(mtry = {{mtry_val}},\r\n             min_n = {{min_n_val}},\r\n             trees = 1000)\r\n  \r\n  wf <- wf %>% \r\n    update_model(mod)\r\n  \r\n  rmse <- fit(wf, data_train) %>% \r\n    extract_rmse()\r\n  \r\n  tibble(mtry = mtry_val, min_n = min_n_val, rmse = rmse, workflow = list(wf))\r\n}\r\n\r\n\r\nmtry_results_1 <- map2_df(grd$Var1, grd$Var2, ~hyp_rf_search(.x, .y, rf_def_wkflw))\r\n\r\n\r\n\r\nNeed to create a folder with Rds files.\r\nThese are my results rmse = 88.5 (50% of data) with mtry = 14, min_n = 10, trees = 1000\r\n\r\n\r\n\r\nRunning this first round of tuning took an hour and forty minutes and the increase relative to the default model was not substantial, only 3 decimal points. Nonetheless, I went for another round of tuning, just to see what could happen.\r\nLooking at the plot, I could see that‚Ä¶\r\n\r\n\r\n\r\nSo I went for the following values to tune‚Ä¶\r\nI used the same all-in-one function shown above.\r\n\r\n   Var1 Var2\r\n1    14    7\r\n2    17    7\r\n3    20    7\r\n4    23    7\r\n5    26    7\r\n6    14    9\r\n7    17    9\r\n8    20    9\r\n9    23    9\r\n10   26    9\r\n11   14   11\r\n12   17   11\r\n13   20   11\r\n14   23   11\r\n15   26   11\r\n16   14   13\r\n17   17   13\r\n18   20   13\r\n19   23   13\r\n20   26   13\r\n21   14   15\r\n22   17   15\r\n23   20   15\r\n24   23   15\r\n25   26   15\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAfter 2 additional hours, I was convinced that continue to tune was not worth it so I decided to stop and finalize the model with mtry = 14 and min_n = 15\r\n\r\n\r\nfinal_mod <- rand_forest() %>%\r\n  set_engine(\"ranger\",\r\n             num.threads = 8,\r\n             importance = \"permutation\",\r\n             verbose = TRUE) %>%\r\n  set_mode(\"regression\") %>%\r\n  set_args(mtry = 14,\r\n           min_n = 15,\r\n           trees = 1000)\r\n\r\nfinal_wkfl <-  workflow() %>%\r\n  add_model(final_mod) %>%\r\n  add_recipe(rec)\r\n\r\n\r\n\r\nNot sure which one of the final fit models to include: the one that uses data_train or the one that uses data_split??\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-05T21:19:10-08:00",
    "input_file": "rf_models.utf8.md"
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\n\nContents\nData Import\nRecipeRecipe Notes\n\nTuning HyperparametersTuning Gamma FirstTuning Results\n\nFollow-up Tree-Parameter TuningCreate Grid\nUpdate Model Specification\nTuning Results\n\nTuning Tree Parameters FirstUpdate Model Specification\n\nTune Stochastic Parameters\nUpdate Model Specification\nStochastic Parameter Results\nRetune Learning Rate\nFinal Fit Statistics\n\n\nData Import\n\n\ndata <- read.csv(\"data/train.csv\") %>%\n  select(-classification) %>%\n  mutate_if(is.character, factor) %>%\n  mutate(ncessch = as.numeric(ncessch))\n\nbonus <- read.csv(\"data/bonus_data_v2.csv\") %>%\n  mutate(ncessch = as.numeric(ncessch)) %>%\n  mutate(locale = gsub(\"^.{0,3}\", \"\", locale)) %>%\n  separate(col = locale, into = c(\"locale\", \"sublocale\"), sep = \": \")\n\ndisc <- read_csv(\"data/disc_drop.csv\") %>%\n  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))\n\n## join data\ndata <- data %>% \n  left_join(bonus) %>% \n  left_join(disc)\n\n\n\nData was merged from three files:\nOriginal Competition Training Dataset\n‚ÄúBonus Dataset‚Äù with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels\nSupplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs\nDropout Rate Data Source: https://www.oregon.gov/ode/reports-and-data/students/Pages/Dropout-Rates.aspx\nSuspension Rate Data Source: https://www.oregon.gov/ode/students-and-family/healthsafety/Pages/School-Discipline,-Bullying,-Restraint-and-Seclusion.aspx\n\nImportantly, the bonus data includes the variables described in the original data description page, as well as the following:\n2016-2017 District Finance Data:\nTotal revenue (rev_total)\nTotal local revenue (rev_local_total)\nTotal state revenue (rev_state_total)\nTotal federal revenue (rev_fed_total)\nTotal expenditures (exp_total)\nTotal current expenditures for elementary and secondary education (exp_current_elsec_total)\nTotal current expenditures for instruction (exp_current_instruction_total)\nTotal current expenditures for support services (exp_current_supp_serve_total)\nTotal capital outlay expenditures (outlay_capital_total)\nTotal salary amount (salaries_total)\nTotal employee benefits in dollars (benefits_employee_total)\nNumber of students for which the reporting local education agency is financially responsible (enrollment_fall_responsible)\n\nDistrict financial data was obtained using the educationdata R package.\nLastly, district finance data was included in both its raw form, and transformed by dividing each revenue and expenditure value by the number of students the LEA was financially responsible for (e.g., exp_total/enrollment_fall_responsible)\nRecipe\n\n\n\n\n\nrec <- recipe(score ~ ., train) %>%\n  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,\n                                         pupil_tch_ratio < 25 ~ 2,\n                                         pupil_tch_ratio < 30 ~ 3, \n                                         TRUE ~ 4),\n              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% \n  step_rm(contains(\"id\"), ncessch, ncesag, lea_name, sch_name) %>%\n  step_mutate(hpi = as.numeric(hpi),\n              lat = round(lat, 2),\n              lon = round(lon, 2),\n              median_income = log(median_income),\n              frl_prop = fr_lnch_prop + red_lnch_prop,\n              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,\n                                    TRUE ~ 0),\n              over_100 = under_200 + over_200) %>% \n  step_interact(terms = ~ lat:lon) %>% \n  step_rm(fr_lnch_prop, red_lnch_prop) %>% \n  step_string2factor(all_nominal()) %>% \n  step_zv(all_predictors()) %>%\n  step_unknown(all_nominal()) %>% \n  step_medianimpute(all_numeric()) %>%\n  step_dummy(all_nominal(), one_hot = TRUE) %>% \n  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% \n  step_interact(~ lang_cd_S:p_hispanic_latino) %>% \n  step_nzv(all_predictors(), freq_cut = 995/5)\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\n\n\nRecipe Notes\nPupil/Teacher ratio (pupil_tch_ratio) is binned and treated as a factor to remove noise (pupil_tch_rate). Both ratio and binned version of the variable remain in the data.\nID variables are removed\nLatitude (lat) and longitude (lon) are rounded to two decimal places to reduce noise and limit precision to within 2/3 of a mile.\nMedian income is log transformed, as the variations in income level are expected to have greater effect near the poverty threshold and less effect in higher income brackets.\nFree lunch proportions and reduced lunch proportions are collapsed into a single variable frl_prop given their expected similar effects.\nFree lunch proportions (fr_lnch_prop) and reduced lunch proportions (red_lnch_prop) are removed from the data set in lieu of their combined proportion.\n\nA dummy coded school-level variable (schl_perf) is created that denotes whether the percentage of students reaching proficiency on the EOY state test exceeds the percentage of students that fall below proficiency standards.\nZero variance predictors are removed\nMissing data is median imputed\nExplicit interaction is specified between the student-level special education flag and the per-student district expenditures for special services. Th effect of special education status is expected to vary depending on the funding available and potential quality of the district‚Äôs special services.\nExplicit interaction is specified for the effect of Spanish language code (lang_cd) and percentage of Hispanic/Latino students at the school (p_hispanic_latino). The justification is that Spanish-speaking students are expected to receive less effective supports in schools in which they are a greater minority.\nNear-zero variance predictors are removed\nTuning Hyperparameters\nBecause the loss reduction (gamma) and maximum tree depth are highly dependent on one another, two approaches were taken to tune the initial model. One approach involved first tuning loss reduction, followed by tree depth and hessian weights. The other involved tuning tree complexity and hessian weights without any severe loss reduction regularization, and then pruning it back by tuning gamma.\nTuning Gamma First\n\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\ngrid <- expand.grid(loss_reduction = seq(0, 80, 5))\n\ngamma_mods <- map(grid$loss_reduction, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 10000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50,\n    nfold = 10,\n    verbose = 1,\n    params = list(\n      eta = 0.1,\n      gamma = .x,\n      nthread = 24\n    )\n  )\n})\n\n\n\nTuning Results\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  452         77.0087         0.0983        82.6056        0.6781\n 2:  448         77.0472         0.0837        82.6143        0.4099\n 3:  445         77.0877         0.0555        82.6204        0.5452\n 4:  524         76.3820         0.1016        82.6213        0.5638\n 5:  436         77.1400         0.0551        82.6243        0.3445\n 6:  452         77.0063         0.0802        82.6287        0.5083\n 7:  416         77.3305         0.0714        82.6291        0.7941\n 8:  447         77.0524         0.0613        82.6352        0.5932\n 9:  424         77.2553         0.0888        82.6371        0.8803\n10:  465         76.9038         0.0983        82.6379        0.5911\n11:  451         77.0375         0.0940        82.6454        0.4721\n12:  451         76.9870         0.0605        82.6509        0.7167\n13:  422         77.2655         0.0899        82.6538        0.6109\n14:  467         76.8912         0.0458        82.6568        0.2768\n15:  453         76.9979         0.0672        82.6589        0.5673\n16:  415         77.3378         0.1053        82.6646        0.7068\n17:  426         77.2260         0.0745        82.6721        0.5258\n    eta gamma\n 1: 0.1    10\n 2: 0.1    50\n 3: 0.1    30\n 4: 0.1    55\n 5: 0.1    70\n 6: 0.1    80\n 7: 0.1    45\n 8: 0.1    15\n 9: 0.1    20\n10: 0.1    40\n11: 0.1    25\n12: 0.1    35\n13: 0.1    60\n14: 0.1     5\n15: 0.1    65\n16: 0.1     0\n17: 0.1    75\n\n\nAs indicated in the results, a gamma of 10 produced the best fit the to test folds. A gamma of 50 was considered as a potential alternative given its lower RMSE SD; however, 50 was considered too extreme of a regularization setting.\nA follow-up gamma tuning process was conducted with a narrowed range of 5 to 15 and is reported below.\ngrid <- expand.grid(loss_reduction = seq(5, 15, 1))\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  426         77.1033         0.0673        82.6085        0.5934\n 2:  440         76.9364         0.0804        82.6233        0.6727\n 3:  411         77.2343         0.0918        82.6301        0.5782\n 4:  470         76.7071         0.0412        82.6407        0.4116\n 5:  452         76.8494         0.0946        82.6417        0.5207\n 6:  497         76.4667         0.0793        82.6423        0.3931\n 7:  478         76.6126         0.0703        82.6454        0.3906\n 8:  427         77.0770         0.0583        82.6646        0.5045\n 9:  389         77.4035         0.1140        82.6658        0.6899\n10:  381         77.5321         0.0441        82.6747        0.4971\n11:  394         77.3753         0.0697        82.6947        0.5472\n    eta gamma\n 1: 0.1    12\n 2: 0.1    13\n 3: 0.1    15\n 4: 0.1    14\n 5: 0.1    11\n 6: 0.1     9\n 7: 0.1     6\n 8: 0.1     8\n 9: 0.1     7\n10: 0.1     5\n11: 0.1    10\n\n\nAfter second round of tuning, a gamma value of 12 produced the best fit.\nBest gamma value = 12\nRMSE/SD to beat = 82.6085/0.5934\nFollow-up Tree-Parameter Tuning\nCreate Grid\n\n\n# Set learning rate, tune tree specific parameters\ngrid <- grid_max_entropy(min_n(c(4, 12)), # min_child_weight\n                         tree_depth(), # max_depth\n                         size = 30)\nhead(grid)\n\n\n# A tibble: 6 x 2\n  min_n tree_depth\n  <int>      <int>\n1     8         10\n2     7         10\n3     7         15\n4    10          5\n5    12          9\n6    10         11\n\nUpdate Model Specification\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12, \n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTuning Results\n\npreserve5a41452f39ca7ebc\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  308         77.0365         0.1108        82.5139        0.7964\n 2:  515         76.9397         0.0765        82.5260        0.5813\n 3:  495         77.1831         0.0865        82.5867        0.6627\n 4:  726         77.7906         0.0600        82.6039        0.5953\n 5: 1093         79.1339         0.0857        82.6386        0.6009\n 6: 2829         79.4049         0.0475        82.6867        0.5113\n 7:   79         73.8231         0.2118        82.9198        0.7814\n 8:   78         73.2449         0.1283        83.0705        0.5636\n 9:   66         71.2054         0.0958        83.2935        0.5143\n10:   62         70.7752         0.1706        83.3910        0.8272\n11:   59         66.7113         0.2891        84.0744        0.4707\n12:   59         67.0320         0.3218        84.1087        0.4410\n13:   59         66.1274         0.2683        84.2384        0.7169\n14:   58         66.5946         0.2077        84.2738        0.5163\n15:   60         65.3824         0.3601        84.5543        0.8896\n16:   57         64.7440         0.2680        84.6912        0.7897\n17:   57         64.5327         0.2807        84.7328        0.7071\n18:   56         63.5798         0.1836        84.9872        0.5934\n19:   56         63.7438         0.1620        85.0052        0.7152\n20:   55         64.1426         0.1506        85.0242        0.9364\n21:   54         63.6017         0.1720        85.3649        0.8097\n22:   55         61.3615         0.2887        85.6355        0.6014\n23:   54         61.0720         0.0906        85.8088        0.4201\n24:   54         60.1531         0.0897        86.0550        0.6940\n25:   53         59.2248         0.1364        86.6076        1.0046\n26:   53         59.2394         0.1181        86.7726        0.6098\n27:   52         56.6917         0.3274        87.6313        0.4278\n28:   51         54.1861         0.1396        88.7169        0.5957\n29:   50         53.9775         0.0640        89.2941        0.5904\n30:   50         53.8405         0.0552        89.3835        0.3106\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n    max_depth min_child_weight\n 1:         7               15\n 2:         6               10\n 3:         6               12\n 4:         5                2\n 5:         4                7\n 6:         3                4\n 7:        12                7\n 8:        12                4\n 9:        15                9\n10:        17               14\n11:        21               10\n12:        18                5\n13:        23               12\n14:        25               15\n15:        17                1\n16:        19                3\n17:        22                7\n18:        26                9\n19:        34               13\n20:        38               14\n21:        46               13\n22:        28                7\n23:        40                9\n24:        27                5\n25:        35                6\n26:        49                7\n27:        27                2\n28:        33                2\n29:        43                2\n30:        49                2\n    max_depth min_child_weight\n\nTuning Tree Parameters First\nUpdate Model Specification\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12,\n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTune Stochastic Parameters\n\n\n\nUpdate Model Specification\n\n\n\nStochastic Parameter Results\n`` #### (Top 10 results shown)\n\npreserve25e7cf703fc3d53b\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  477        77.34670     0.09624726       82.55346     0.5357044\n 2:  409        77.59206     0.09019289       82.55610     0.5839695\n 3:  511        77.50587     0.07439461       82.56070     0.5872140\n 4:  435        77.50257     0.06786992       82.56779     0.6541752\n 5:  432        77.93361     0.07199536       82.57272     0.5910682\n 6:  451        77.41958     0.07225770       82.58018     0.5463407\n 7:  438        77.86621     0.09569565       82.58743     0.4851545\n 8:  409        77.62566     0.06778519       82.59830     0.5790606\n 9:  395        77.90076     0.09472367       82.60376     0.6282980\n10:  354        78.12057     0.07055603       82.60837     0.5912351\n    colsample_bytree subsample\n 1:        0.6432432 0.9850567\n 2:        0.8864865 0.9124305\n 3:        0.3567568 0.9175125\n 4:        0.7513514 0.8682269\n 5:        0.4216216 0.9422890\n 6:        0.7567568 0.9741536\n 7:        0.4540541 0.8728303\n 8:        0.8918919 0.8516717\n 9:        0.8054054 0.7719082\n10:        0.8918919 0.8044063\n\nRetune Learning Rate\n\n\n\nFinal Fit Statistics\n\n\n\n",
    "preview": "posts/11-29-20_Chris/ML_models_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-06T18:51:18-08:00",
    "input_file": "ML_models.utf8.md",
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/11-28-20_datasets_AY/",
    "title": "Datasets",
    "description": "Let's talk about the datasets used in the project",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "preview": {},
    "last_modified": "2020-11-29T01:02:37-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_welcome_AY/",
    "title": "Hello!",
    "description": "Welcome to our Machine Learning blog",
    "author": [
      {
        "name": "AY, AG, and CI",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "preview": {},
    "last_modified": "2020-11-29T00:47:02-08:00",
    "input_file": {}
  }
]
