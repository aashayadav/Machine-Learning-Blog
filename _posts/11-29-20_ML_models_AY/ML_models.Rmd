---
title: "Linear Regression and Penalized Regression Model (Lasso)"
description: |
    Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.
author:
  - name: Asha
    url:
    affiliation: University of Oregon
    affiliation_url:
date: 11-28-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(here)
library(rio)
library(skimr)
library(rsample)
#install.packages("caret")
library(caret)

```


```{r, echo=FALSE, include=FALSE}

# Reading 'train.csv' dataset
set.seed(3000)


git <- "~/Documents/GitHub/EDLD-654-Final"

data <- import(here("data", "train.csv")) %>%
  select(-classification) %>%
  mutate_if(is.character, factor) %>%
  mutate(ncessch = as.double(ncessch)) 

bonus <- import(here("data", "bonus_data.csv")) %>%
  mutate(pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>%
  mutate(ncessch = as.double(ncessch))

## join data
data <- data %>% 
  left_join(bonus) %>%
  sample_frac(0.01)



```
## 1.Splitting and resampling

Data splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.

### 1a. Splitting
We will split our dataset into training and testing set. 
The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model's performance.

We will use `initial_split()` function from the `rsample` package which creates a `split` object. The split object `d_split`, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).

```{r, echo=TRUE}

set.seed(3000)
# Create split object specifying (75%) and testing (25%)

data_split <- initial_split(data, prop = 3/4) 

data_split

```
We will extract the training and testing set from the split object, `d_split` by using the `training()` and `testing()` functions.

```{r, echo=TRUE}
# Extract training and testing set
set.seed(3000)

data_train <- training(data_split)  # Our training dataset has 1421 observations.

data_test <- testing(data_split)    # Our test dataset has 473 observations.
```

### 1b. Resampling
At some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we'll resample our data using `vfold_cv()`. This function outputs k-fold cross-validated versions of our training data, where `k = number of time we resample`. 

```{r, echo=TRUE}
# Resample the data with 10-fold cross validation.
set.seed(3000)
cv <- vfold_cv(data_train)

```

## 2.Pre-processing

Preprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as `all_numeric()`, `all_predictors()` as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (`step_medianimpute`), rescaling (`step_scale`), standardizing (`step_normalize`), PCA (`step_pca`) and creating dummy variables (`step_dummy`). A full list of pre-processing can be found [here](https://recipes.tidymodels.org/reference/index.html).

We will use `recipe()` function to indicate our outcome and predictor variables in our recipe. We will use `~.` to indicate that we are using all variables to predict the outcome variable `score`. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, `rec`, it shows numbers of predictor variables have been specified. It doen't actually apply the predictors yet. We will use same receipe throughout this post. 

```{r, echo=TRUE}
set.seed(3000)

rec <- recipe(score ~ ., data_train) %>% 
  step_mutate(tst_dt = as.numeric(lubridate::
                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% # declare ID variables
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances
  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels 
  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`
  step_medianimpute(all_numeric(), -all_outcomes(), 
                    -has_role("id vars"))  %>% # replaces missing numeric observations with the median
  step_dummy(all_nominal(), -has_role("id vars")) %>% # dummy codes categorical variables
  step_nzv(all_predictors(), -starts_with("lang_cd"))
  
  
 
```


Extract the pre-processed dataset

To extract the pre-processed dataset, we can `prep()` the recipe for our datset then `bake()` the prepped recipe to extract the pre-processed data.

However, in [tidymodels](https://workflows.tidymodels.org/) we can use `workflows()` where we don't need to `prep()` or `bake()` the recipe. 


```{r, echo=TRUE}
prep(rec) #%>%
 # bake(data_train)   # We are using workflow so no need to bake.
```

The next step is to specify our ML models, using the `parsnip` package. To specify the model, there are four primary components: `model type`, `arguments`, `engine`, and `mode`. 

## 3. Linear model


```{r, echo=TRUE}

set.seed(3000)

# Specify the model

mod_linear <- linear_reg() %>%
  set_engine("lm") %>%  # engine for linear regression
  set_mode("regression")  # regression for continous outcome varaible.

# Workflow
lm_wf <- workflow() %>% # set the workflow
  add_recipe(rec) %>% # add recipe
  add_model(mod_linear) # add model
  

# Fit the linear model
mod_linear_fit<- fit_resamples(
  mod_linear,
  preprocessor = rec,
  resamples = cv,
  metrics = metric_set(rmse),
  control = control_resamples(verbose = TRUE,
                                    save_pred = TRUE))



```

Collect metrics on linear model

```{r, echo=TRUE}

set.seed(3000)

mod_linear_fit %>%
  collect_metrics() %>%
  filter(.metric == "rmse") 

#RMSE = 96.28

```