[
  {
    "path": "posts/11-29-20_Alejandra/",
    "title": "Random Forest",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Alejandra",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": {},
    "last_modified": "2020-12-04T18:07:18-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_Asha/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": "posts/11-29-20_Asha/Linear_model_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2020-12-07T13:48:40-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "Here is our XG boost model!",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": {},
    "last_modified": "2020-12-06T15:42:44-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_data_prep_&_description_AY/",
    "title": "Data preparation and exploration",
    "description": "Here we joined our datasets and explored through visualization.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": {},
    "last_modified": "2020-12-07T03:15:41-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\n\nContents\nData Import\nRecipeRecipe Notes\n\nTuning HyperparametersTuning Gamma FirstTuning Results\n\nFollow-up Tree-Parameter TuningCreate Grid\nUpdate Model Specification\nTuning Results\n\nTuning Tree Parameters FirstUpdate Model Specification\n\nTune Stochastic Parameters\nUpdate Model Specification\nStochastic Parameter Results\nRetune Learning Rate\nFinal Fit Statistics\n\n\nData Import\n\n\ndata <- read.csv(\"data/train.csv\") %>%\n  select(-classification) %>%\n  mutate_if(is.character, factor) %>%\n  mutate(ncessch = as.numeric(ncessch))\n\nbonus <- read.csv(\"data/bonus_data_v2.csv\") %>%\n  mutate(ncessch = as.numeric(ncessch)) %>%\n  mutate(locale = gsub(\"^.{0,3}\", \"\", locale)) %>%\n  separate(col = locale, into = c(\"locale\", \"sublocale\"), sep = \": \")\n\ndisc <- read_csv(\"data/disc_drop.csv\") %>%\n  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))\n\n## join data\ndata <- data %>% \n  left_join(bonus) %>% \n  left_join(disc)\n\n\n\nData was merged from three files:\nOriginal Competition Training Dataset\n“Bonus Dataset” with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels\nSupplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs\nDropout Rate Data Source: https://www.oregon.gov/ode/reports-and-data/students/Pages/Dropout-Rates.aspx\nSuspension Rate Data Source: https://www.oregon.gov/ode/students-and-family/healthsafety/Pages/School-Discipline,-Bullying,-Restraint-and-Seclusion.aspx\n\nImportantly, the bonus data includes the variables described in the original data description page, as well as the following:\n2016-2017 District Finance Data:\nTotal revenue (rev_total)\nTotal local revenue (rev_local_total)\nTotal state revenue (rev_state_total)\nTotal federal revenue (rev_fed_total)\nTotal expenditures (exp_total)\nTotal current expenditures for elementary and secondary education (exp_current_elsec_total)\nTotal current expenditures for instruction (exp_current_instruction_total)\nTotal current expenditures for support services (exp_current_supp_serve_total)\nTotal capital outlay expenditures (outlay_capital_total)\nTotal salary amount (salaries_total)\nTotal employee benefits in dollars (benefits_employee_total)\nNumber of students for which the reporting local education agency is financially responsible (enrollment_fall_responsible)\n\nDistrict financial data was obtained using the educationdata R package.\nLastly, district finance data was included in both its raw form, and transformed by dividing each revenue and expenditure value by the number of students the LEA was financially responsible for (e.g., exp_total/enrollment_fall_responsible)\nRecipe\n\n\n\n\n\nrec <- recipe(score ~ ., train) %>%\n  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\n              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,\n                                         pupil_tch_ratio < 25 ~ 2,\n                                         pupil_tch_ratio < 30 ~ 3, \n                                         TRUE ~ 4),\n              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% \n  step_rm(contains(\"id\"), ncessch, ncesag, lea_name, sch_name) %>%\n  step_mutate(hpi = as.numeric(hpi),\n              lat = round(lat, 2),\n              lon = round(lon, 2),\n              median_income = log(median_income),\n              frl_prop = fr_lnch_prop + red_lnch_prop,\n              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,\n                                    TRUE ~ 0),\n              over_100 = under_200 + over_200) %>% \n  step_interact(terms = ~ lat:lon) %>% \n  step_rm(fr_lnch_prop, red_lnch_prop) %>% \n  step_string2factor(all_nominal()) %>% \n  step_zv(all_predictors()) %>%\n  step_unknown(all_nominal()) %>% \n  step_medianimpute(all_numeric()) %>%\n  step_dummy(all_nominal(), one_hot = TRUE) %>% \n  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% \n  step_interact(~ lang_cd_S:p_hispanic_latino) %>% \n  step_nzv(all_predictors(), freq_cut = 995/5)\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\n\n\nRecipe Notes\nPupil/Teacher ratio (pupil_tch_ratio) is binned and treated as a factor to remove noise (pupil_tch_rate). Both ratio and binned version of the variable remain in the data.\nID variables are removed\nLatitude (lat) and longitude (lon) are rounded to two decimal places to reduce noise and limit precision to within 2/3 of a mile.\nMedian income is log transformed, as the variations in income level are expected to have greater effect near the poverty threshold and less effect in higher income brackets.\nFree lunch proportions and reduced lunch proportions are collapsed into a single variable frl_prop given their expected similar effects.\nFree lunch proportions (fr_lnch_prop) and reduced lunch proportions (red_lnch_prop) are removed from the data set in lieu of their combined proportion.\n\nA dummy coded school-level variable (schl_perf) is created that denotes whether the percentage of students reaching proficiency on the EOY state test exceeds the percentage of students that fall below proficiency standards.\nZero variance predictors are removed\nMissing data is median imputed\nExplicit interaction is specified between the student-level special education flag and the per-student district expenditures for special services. Th effect of special education status is expected to vary depending on the funding available and potential quality of the district’s special services.\nExplicit interaction is specified for the effect of Spanish language code (lang_cd) and percentage of Hispanic/Latino students at the school (p_hispanic_latino). The justification is that Spanish-speaking students are expected to receive less effective supports in schools in which they are a greater minority.\nNear-zero variance predictors are removed\nTuning Hyperparameters\nBecause the loss reduction (gamma) and maximum tree depth are highly dependent on one another, two approaches were taken to tune the initial model. One approach involved first tuning loss reduction, followed by tree depth and hessian weights. The other involved tuning tree complexity and hessian weights without any severe loss reduction regularization, and then pruning it back by tuning gamma.\nTuning Gamma First\n\n\nbaked_train <- prep(rec) %>% \n  bake(train)\n\ntrain_x = data.matrix(baked_train[, -73])\ntrain_y = data.matrix(baked_train[, 73])\n\ngrid <- expand.grid(loss_reduction = seq(0, 80, 5))\n\ngamma_mods <- map(grid$loss_reduction, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 10000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50,\n    nfold = 10,\n    verbose = 1,\n    params = list(\n      eta = 0.1,\n      gamma = .x,\n      nthread = 24\n    )\n  )\n})\n\n\n\nTuning Results\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  452         77.0087         0.0983        82.6056        0.6781\n 2:  448         77.0472         0.0837        82.6143        0.4099\n 3:  445         77.0877         0.0555        82.6204        0.5452\n 4:  524         76.3820         0.1016        82.6213        0.5638\n 5:  436         77.1400         0.0551        82.6243        0.3445\n 6:  452         77.0063         0.0802        82.6287        0.5083\n 7:  416         77.3305         0.0714        82.6291        0.7941\n 8:  447         77.0524         0.0613        82.6352        0.5932\n 9:  424         77.2553         0.0888        82.6371        0.8803\n10:  465         76.9038         0.0983        82.6379        0.5911\n11:  451         77.0375         0.0940        82.6454        0.4721\n12:  451         76.9870         0.0605        82.6509        0.7167\n13:  422         77.2655         0.0899        82.6538        0.6109\n14:  467         76.8912         0.0458        82.6568        0.2768\n15:  453         76.9979         0.0672        82.6589        0.5673\n16:  415         77.3378         0.1053        82.6646        0.7068\n17:  426         77.2260         0.0745        82.6721        0.5258\n    eta gamma\n 1: 0.1    10\n 2: 0.1    50\n 3: 0.1    30\n 4: 0.1    55\n 5: 0.1    70\n 6: 0.1    80\n 7: 0.1    45\n 8: 0.1    15\n 9: 0.1    20\n10: 0.1    40\n11: 0.1    25\n12: 0.1    35\n13: 0.1    60\n14: 0.1     5\n15: 0.1    65\n16: 0.1     0\n17: 0.1    75\n\n\nAs indicated in the results, a gamma of 10 produced the best fit the to test folds. A gamma of 50 was considered as a potential alternative given its lower RMSE SD; however, 50 was considered too extreme of a regularization setting.\nA follow-up gamma tuning process was conducted with a narrowed range of 5 to 15 and is reported below.\ngrid <- expand.grid(loss_reduction = seq(5, 15, 1))\n\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  426         77.1033         0.0673        82.6085        0.5934\n 2:  440         76.9364         0.0804        82.6233        0.6727\n 3:  411         77.2343         0.0918        82.6301        0.5782\n 4:  470         76.7071         0.0412        82.6407        0.4116\n 5:  452         76.8494         0.0946        82.6417        0.5207\n 6:  497         76.4667         0.0793        82.6423        0.3931\n 7:  478         76.6126         0.0703        82.6454        0.3906\n 8:  427         77.0770         0.0583        82.6646        0.5045\n 9:  389         77.4035         0.1140        82.6658        0.6899\n10:  381         77.5321         0.0441        82.6747        0.4971\n11:  394         77.3753         0.0697        82.6947        0.5472\n    eta gamma\n 1: 0.1    12\n 2: 0.1    13\n 3: 0.1    15\n 4: 0.1    14\n 5: 0.1    11\n 6: 0.1     9\n 7: 0.1     6\n 8: 0.1     8\n 9: 0.1     7\n10: 0.1     5\n11: 0.1    10\n\n\nAfter second round of tuning, a gamma value of 12 produced the best fit.\nBest gamma value = 12\nRMSE/SD to beat = 82.6085/0.5934\nFollow-up Tree-Parameter Tuning\nCreate Grid\n\n\n# Set learning rate, tune tree specific parameters\ngrid <- grid_max_entropy(min_n(c(4, 12)), # min_child_weight\n                         tree_depth(), # max_depth\n                         size = 30)\nhead(grid)\n\n\n# A tibble: 6 x 2\n  min_n tree_depth\n  <int>      <int>\n1     8         10\n2     7         10\n3     7         15\n4    10          5\n5    12          9\n6    10         11\n\nUpdate Model Specification\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12, \n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTuning Results\n\npreserve5a41452f39ca7ebc\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  308         77.0365         0.1108        82.5139        0.7964\n 2:  515         76.9397         0.0765        82.5260        0.5813\n 3:  495         77.1831         0.0865        82.5867        0.6627\n 4:  726         77.7906         0.0600        82.6039        0.5953\n 5: 1093         79.1339         0.0857        82.6386        0.6009\n 6: 2829         79.4049         0.0475        82.6867        0.5113\n 7:   79         73.8231         0.2118        82.9198        0.7814\n 8:   78         73.2449         0.1283        83.0705        0.5636\n 9:   66         71.2054         0.0958        83.2935        0.5143\n10:   62         70.7752         0.1706        83.3910        0.8272\n11:   59         66.7113         0.2891        84.0744        0.4707\n12:   59         67.0320         0.3218        84.1087        0.4410\n13:   59         66.1274         0.2683        84.2384        0.7169\n14:   58         66.5946         0.2077        84.2738        0.5163\n15:   60         65.3824         0.3601        84.5543        0.8896\n16:   57         64.7440         0.2680        84.6912        0.7897\n17:   57         64.5327         0.2807        84.7328        0.7071\n18:   56         63.5798         0.1836        84.9872        0.5934\n19:   56         63.7438         0.1620        85.0052        0.7152\n20:   55         64.1426         0.1506        85.0242        0.9364\n21:   54         63.6017         0.1720        85.3649        0.8097\n22:   55         61.3615         0.2887        85.6355        0.6014\n23:   54         61.0720         0.0906        85.8088        0.4201\n24:   54         60.1531         0.0897        86.0550        0.6940\n25:   53         59.2248         0.1364        86.6076        1.0046\n26:   53         59.2394         0.1181        86.7726        0.6098\n27:   52         56.6917         0.3274        87.6313        0.4278\n28:   51         54.1861         0.1396        88.7169        0.5957\n29:   50         53.9775         0.0640        89.2941        0.5904\n30:   50         53.8405         0.0552        89.3835        0.3106\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n    max_depth min_child_weight\n 1:         7               15\n 2:         6               10\n 3:         6               12\n 4:         5                2\n 5:         4                7\n 6:         3                4\n 7:        12                7\n 8:        12                4\n 9:        15                9\n10:        17               14\n11:        21               10\n12:        18                5\n13:        23               12\n14:        25               15\n15:        17                1\n16:        19                3\n17:        22                7\n18:        26                9\n19:        34               13\n20:        38               14\n21:        46               13\n22:        28                7\n23:        40                9\n24:        27                5\n25:        35                6\n26:        49                7\n27:        27                2\n28:        33                2\n29:        43                2\n30:        49                2\n    max_depth min_child_weight\n\nTuning Tree Parameters First\nUpdate Model Specification\n\n\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\n  xgb.cv(\n    data = train_x,\n    label = train_y,\n    nrounds = 5000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 1,\n    params = list( \n      eta = 0.1,\n      gamma = 12,\n      min_child_weight = .x,\n      max_depth = .y,\n      nthread = 24\n    ) \n  )  \n}) \n\n\n\nTune Stochastic Parameters\n\n\n\nUpdate Model Specification\n\n\n\nStochastic Parameter Results\n`` #### (Top 10 results shown)\n\npreserve25e7cf703fc3d53b\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n 1:  477        77.34670     0.09624726       82.55346     0.5357044\n 2:  409        77.59206     0.09019289       82.55610     0.5839695\n 3:  511        77.50587     0.07439461       82.56070     0.5872140\n 4:  435        77.50257     0.06786992       82.56779     0.6541752\n 5:  432        77.93361     0.07199536       82.57272     0.5910682\n 6:  451        77.41958     0.07225770       82.58018     0.5463407\n 7:  438        77.86621     0.09569565       82.58743     0.4851545\n 8:  409        77.62566     0.06778519       82.59830     0.5790606\n 9:  395        77.90076     0.09472367       82.60376     0.6282980\n10:  354        78.12057     0.07055603       82.60837     0.5912351\n    colsample_bytree subsample\n 1:        0.6432432 0.9850567\n 2:        0.8864865 0.9124305\n 3:        0.3567568 0.9175125\n 4:        0.7513514 0.8682269\n 5:        0.4216216 0.9422890\n 6:        0.7567568 0.9741536\n 7:        0.4540541 0.8728303\n 8:        0.8918919 0.8516717\n 9:        0.8054054 0.7719082\n10:        0.8918919 0.8044063\n\nRetune Learning Rate\n\n\n\nFinal Fit Statistics\n\n\n\n",
    "preview": "posts/11-29-20_Chris/ML_models_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-06T18:51:18-08:00",
    "input_file": "ML_models.utf8.md",
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/11-28-20_datasets_AY/",
    "title": "Datasets",
    "description": "Here is the preview of datasets used in our final project.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "preview": {},
    "last_modified": "2020-12-06T15:41:48-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_welcome_AY/",
    "title": "Hello!",
    "description": "Welcome to our Machine Learning blog",
    "author": [
      {
        "name": "AY, AG, and CI",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "preview": {},
    "last_modified": "2020-11-29T00:47:02-08:00",
    "input_file": {}
  }
]
