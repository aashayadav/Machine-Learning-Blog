[
  {
    "path": "posts/11-29-20_Alejandra/",
    "title": "Random Forest Model",
    "description": "This is how fitting a Random Forest model went for me!",
    "author": [
      {
        "name": "Alejandra Garcia Isaza",
        "url": {}
      }
    ],
    "date": "2020-12-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPrep work\r\nLet‚Äôs dive in on the model fitting process!\r\nFirst round of tuning\r\nSecond round of tuning\r\nConclusionFinal fit\r\n\r\nKaggle submission\r\n\r\nTo complete this project, I chose to run a Random Forest model. I decided to go with the Out-of-Bag (OOB) samples to conduct model fitting and model assessment due to its relatively lower run time, as compared to using cross-validation resamples.\r\nInitially, I planned to work locally on my computer with only 5% of the data. Overall, my model tuning process was taking between 15 and 25 minutes each round of tuning, but unfortunately my R session froze several times and I could not finish the model tuning process.\r\nAt this point, Daniel suggested I use HPC via the Open On Demand interface. As always, I was afraid to try something new, but Daniel patiently showed me how to work with the new interface and explained for the 97th time what was the difference between the train and test files we have been working with all term long. I think I finally got it! I‚Äôm glad it only took me ten weeks üò¨ to understand this.\r\nPrep work\r\nI followed the same steps we have been following all term long, reading in the data, sampling a fraction of the data (note that I used 50% of the data because I was working with HPC), and joining with other datasets. Here I joined a dataset we called ‚Äúbonus‚Äù that inlcudes variables from different datasets. To learn more about this dataset and its variables, please go here.\r\nA huge shout-out to Chris Ives for taking the time to find and process the bonus datasets we are using here.\r\n\r\n\r\nset.seed(3000)\r\n\r\nfull_train <- import(here(\"data\", \"train.csv\"), setclass = \"tbl_df\") %>%\r\n  select(-classification) %>%\r\n  mutate_if(is.character, factor) %>%\r\n  mutate(ncessch = as.double(ncessch)) %>%\r\n  sample_frac(0.5)\r\n\r\nbonus <- import(here(\"data\", \"bonus_data.csv\")) %>%\r\n  mutate(pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>%\r\n  mutate(ncessch = as.double(ncessch))\r\n\r\n## joining data\r\ndata <- left_join(full_train, bonus)\r\n\r\n\r\n\r\nHere I created the initial split and the train and test objects for model fitting and assessment. Notice that I did not create a cross-validation resample object because I was only using the OOB samples.\r\n\r\n\r\nset.seed(3000)\r\ndata_split <- initial_split(data)\r\n\r\nset.seed(3000)\r\ndata_train <- training(data_split)\r\ndata_test <- testing(data_split)\r\n\r\n\r\n\r\nI created a recipe following the recommended preprocessing steps for a Random Forest model. I found this guide that Joe shared with our group very useful.\r\nThis recipe is pretty straightforward, the only thing that I think is worth highlighting is that we took the liberty to assume that most of the students‚Äô home language was English; that is why we specified this code lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"). I took extra care not to lose the Spanish speaking students in the lang_cd variable to the step_nzv() near-zero variance variable removal due to the role of language in academic achievement.\r\nIn this model, we had one outcome score, 77 predictors and ten Id variables.\r\n\r\n\r\nrec <- recipe(score ~ ., data_train) %>%\r\n  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),\r\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\r\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>% \r\n  update_role(contains(\"id\"), ncessch, ncesag, sch_name, new_role = \"id_vars\") %>%\r\n  step_zv(all_predictors(), -starts_with(\"lang_cd\")) %>%\r\n  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(\"id_vars\")) %>%\r\n  step_novel(all_nominal()) %>%\r\n  step_unknown(all_nominal()) %>% \r\n  step_dummy(all_nominal()) %>%\r\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\r\n\r\n\r\n\r\nLet‚Äôs dive in on the model fitting process!\r\nFirst, I wanted to develop a baseline model using the default hypermarameters. I focused only on three hyperparameters: predictors mtry = floor(p/3), number of trees trees = 500, and minimun node size min_n = 5. In this model, mtry was around 26 predictors.\r\n\r\n\r\n# default model\r\nrf_def <- rand_forest() %>%\r\n  set_engine(\"ranger\",\r\n             num.threads = 8,\r\n             importance = \"permutation\",\r\n             verbose = TRUE) %>%\r\n  set_mode(\"regression\")\r\n\r\n# workflow for default model\r\nrf_def_wkflw <- workflow() %>% \r\n  add_model(rf_def) %>% \r\n  add_recipe(rec)\r\n\r\n# fitting the default model\r\nrf_def_fit <- fit(\r\n  rf_def_wkflw,\r\n  data_train)\r\n\r\n\r\n\r\nThe rmse for the default model was 88.80.\r\nFitting the default model with 50% of the data took less than 3 minutes using HPC!!\r\nFirst round of tuning\r\nRandom forest models are know to have very good out-of-the-box performance, however, I wanted to tune at least mtry and min_n to evaluate how much lower the rmse could be.\r\nI decided not to spend time tuning for number of trees as the literature suggests that growing p * 10 trees is pretty safe. The number of trees needs to be large enough to stabilize the error rate; for this model I could have used 770 trees, but I went for 1000 trees just to be extra safe.\r\nTo tune for mtry and min_n, I followed what Boehmke & Greenwell (2020) suggest in the chapter focused on random forest models in the book Hands-On Machine Learning with R\r\n‚ÄúStart with five evenly spaced values of mtry across the range 2 ‚Äì p centered at the recommended default‚Äù (Boehmke & Greenwell, 2020).\r\n‚ÄúWhen adjusting node size start with three values between 1‚Äì10 and adjust depending on impact to accuracy and run time (Boehmke & Greenwell, 2020).‚Äù\r\nHere I created a regular grid with 20 different combinations of mtry and min_n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[2,14,26,38,50,2,14,26,38,50,2,14,26,38,50,2,14,26,38,50],[1,1,1,1,1,4,4,4,4,4,7,7,7,7,7,10,10,10,10,10]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nBelow is a function that Daniel helped me build. This is an all-in-one function that builds the model, updates the default model workflow with the new hyperparameters, fits the models with the selected hyperparameters, and outputs a table with the tuned hyperparameters and the corresponding rmse value. It‚Äôs pretty neat!\r\n\r\n\r\n# All-in-one function\r\nhyp_rf_search <- function(mtry_val, min_n_val, wf) {\r\n  mod <- rand_forest() %>% \r\n    set_engine(\"ranger\",\r\n               num.threads = 8,\r\n               importance = \"permutation\",\r\n               verbose = TRUE) %>% \r\n    set_mode(\"regression\") %>% \r\n    set_args(mtry = {{mtry_val}},\r\n             min_n = {{min_n_val}},\r\n             trees = 1000)\r\n  \r\n  wf <- wf %>% \r\n    update_model(mod)\r\n  \r\n  rmse <- fit(wf, data_train) %>% \r\n    extract_rmse()\r\n  \r\n  tibble(mtry = mtry_val, min_n = min_n_val, rmse = rmse, workflow = list(wf))\r\n}\r\n\r\n# Applying the function\r\nmtry_results_1 <- map2_df(grd$Var1, grd$Var2, ~hyp_rf_search(.x, .y, rf_def_wkflw))\r\n\r\n\r\n\r\nThe lowest rmse for this round of tuning was 88.54 with mtry = 14, min_n = 10, and trees = 1000. Running this first round of tuning took around an hour and forty minutes and the decrease on the rmse value relative to the default model was not substantial, only around three decimal points difference.\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[14,14,26,14,38,14,26,50,38,26,50,26,38,50,38,50,2,2,2,2],[10,7,10,4,10,1,7,10,7,4,7,1,4,4,1,1,7,1,10,4],[88.5422609601283,88.9375105622991,89.2748002583518,89.3467365378118,89.6531075854499,89.7182599041209,89.7944994293632,89.9400737655761,90.3377558734669,90.5156819860458,90.6806244642205,91.1849421483674,91.2063236506577,91.742655109802,92.1480079640799,92.7881197919246,93.312497707138,93.3501092283632,93.3760153020166,93.3817847358467]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n      <th>rmse<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nBy examining the plot, I could see that an mtry = 14 was consistently producing lower rmse values, however, higher values of min_n appeared to produce slighty lower values of rmse.\r\n\r\n\r\n\r\nSecond round of tuning\r\nOn the second round of tuning I chose a range of numbers between the two mtry values that produced the lowest rmse values, 14 and 26 (the recommended default). In addition, I chose a range of numbers that included the best two min_n values, 7 and 10, but also a few more numbers higher than ten to check if, as I intuited, higher values of min_n increased the performance of the models.\r\nFor this round of tuning, I created a regular grid with 25 different combinations of mtry and min_n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[14,17,20,23,26,14,17,20,23,26,14,17,20,23,26,14,17,20,23,26,14,17,20,23,26],[7,7,7,7,7,9,9,9,9,9,11,11,11,11,11,13,13,13,13,13,15,15,15,15,15]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nI used the same all-in-one function shown above to run these models. The lowest rmse for this second round of tuning was 88.17 with mtry = 14, min_n = 15, and trees = 1000. The model fitting process took a little less than two hours and the decrease on the rmse value relative to the previous model was again of only three decimal points.\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"],[14,17,14,20,17,14,23,20,26,17,14,23,20,26,17,14,23,26,20,17,23,20,26,23,26],[15,15,13,15,13,11,15,13,15,11,9,13,11,13,9,7,11,11,9,7,9,7,9,7,7],[88.1702701032027,88.2913312019863,88.2917472699144,88.4320547892268,88.4478151959256,88.4714496091745,88.5588965088155,88.6207445582953,88.6414907304891,88.6730477850128,88.6868663843177,88.7485721937091,88.8327314174613,88.8925891056003,88.9084821719671,88.9127044769608,88.9748487664824,89.1090208580788,89.123262575347,89.1928533213956,89.2882857406444,89.4205005670333,89.4284658762646,89.6031329821944,89.79396337313]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mtry<\\/th>\\n      <th>min_n<\\/th>\\n      <th>rmse<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nBy examining this plot, I confirmed what I intuited in the first round of tuning, that an mtry = 14 consistently produced the lowest rmse values and that higher values of min_n continued to produce lower values of rmse.\r\nI could have kept tuning for higher values of min_n, however, I thought that the cost in time was not worth the small gain in decrease of rmse values.\r\n\r\n\r\n\r\nConclusion\r\nTaking it all together, the default model was without a doubt the one that had the higher cost/benefit return, it only took around three minutes to fit! Whereas each round of tuning took around two hours, with very little return (imo). I guess that the person who said that random forest models have great out-of-the-box performance was not joking!\r\nFinal fit\r\nAt this point I was convinced that continue to tune was not worth the time, however, I had already spent quite a bit of time in model tuning so I decided to use mtry = 14 and a min_n = 15, the hyperparameters that produced the lowest rmse on my model tuning process.\r\n\r\n\r\n# final model with hard coded hyperparameters\r\nfinal_mod <- rand_forest() %>%\r\n  set_engine(\"ranger\",\r\n             num.threads = 8,\r\n             importance = \"permutation\",\r\n             verbose = TRUE) %>%\r\n  set_mode(\"regression\") %>%\r\n  set_args(mtry = 14,\r\n           min_n = 15,\r\n           trees = 1000)\r\n\r\n# workflow for final model\r\nfinal_wkfl <-  workflow() %>%\r\n  add_model(final_mod) %>%\r\n  add_recipe(rec)\r\n\r\n# final split on the initial split\r\nfinal_fit <- last_fit(final_wkfl,\r\n                      split = data_split)\r\n\r\n\r\n\r\nFitting the final model took around 4 minutes and the resulting rmse was 88.3.\r\nKaggle submission\r\nThe next step was to submit predictions to Kaggle to test how the model performed when compared against the true values. To do this I had to fit the model using the unsplit dataset called data, read in the test.csv file, join the bonus data, prep and bake the recipe and create a dataframe with the predicted score values.\r\n\r\n\r\n# fit with the unsplit data\r\ncheck_fit <- final_wkfl %>%\r\n                 fit(data)\r\n\r\n# read in test.csv file\r\nfull_test <- import(\"data/test.csv\", setclass = \"tbl_df\") %>%\r\n  mutate_if(is.character, factor) %>%\r\n  mutate(ncessch = as.double(ncessch))\r\n\r\n## joining data\r\nall_test <- left_join(full_test, bonus)\r\n\r\n# baking the recipe\r\nprocessed_test <- rec %>%\r\n  prep() %>% \r\n  bake(all_test)\r\n\r\n# make predictions\r\npreds <- predict(check_fit, all_test)\r\n\r\n# a tibble\r\npred_frame <- tibble(Id = all_test$id, Predicted = preds$.pred)\r\n\r\n\r\n\r\nThis table shows only the first six rows of data:\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[4,6,8,9,11,15],[2492.97321250139,2501.58582718069,2668.15853429978,2464.77458894793,2530.78887087146,2487.80269035691]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Id<\\/th>\\n      <th>Predicted<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\n\r\nI was gladly surprised to find that the model performed better than expected! ü•≥\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T18:30:56-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_Asha/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1.Splitting and resampling1a. Splitting\r\n1b. Resampling\r\n\r\n2.Pre-processing\r\n3. Linear model\r\n4. Lasso Regression (Type of penalized Regression)\r\n1.Splitting and resampling\r\nData splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.\r\n1a. Splitting\r\nWe will split our dataset into training and testing set. The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model‚Äôs performance.\r\nWe will use initial_split() function from the rsample package which creates a split object. The split object d_split, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).\r\n\r\n\r\nset.seed(3000)\r\n# Create split object specifying (75%) and testing (25%)\r\n\r\ndata_split <- initial_split(data, prop = 3/4) \r\n\r\ndata_split\r\n\r\n<Analysis/Assess/Total>\r\n<1421/473/1894>\r\n\r\nWe will extract the training and testing set from the split object, d_split by using the training() and testing() functions.\r\n\r\n\r\n# Extract training and testing set\r\nset.seed(3000)\r\n\r\ndata_train <- training(data_split)  # Our training dataset has 1421 observations.\r\n\r\ndata_test <- testing(data_split)    # Our test dataset has 473 observations.\r\n\r\n1b. Resampling\r\nAt some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we‚Äôll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = number of time we resample.\r\n\r\n\r\n# Resample the data with 10-fold cross validation.\r\nset.seed(3000)\r\ncv <- vfold_cv(data_train)\r\n\r\n2.Pre-processing\r\nPreprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as all_numeric(), all_predictors() as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (step_medianimpute), rescaling (step_scale), standardizing (step_normalize), PCA (step_pca) and creating dummy variables (step_dummy). A full list of pre-processing can be found here.\r\nWe will use recipe() function to indicate our outcome and predictor variables in our recipe. We will use ~. to indicate that we are using all variables to predict the outcome variable score. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, rec, it shows numbers of predictor variables have been specified. It doen‚Äôt actually apply the predictors yet. We will use same receipe throughout this post.\r\n\r\n\r\nset.seed(3000)\r\n\r\nrec <- recipe(score ~ ., data_train) %>% \r\n  step_mutate(tst_dt = as.numeric(lubridate::\r\n                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \r\n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\r\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\r\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \r\n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\r\n  step_medianimpute(all_numeric(), -all_outcomes(), \r\n                    -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\r\n  step_dummy(all_nominal(), -has_role(\"id vars\")) %>% # dummy codes categorical variables\r\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\r\n\r\nExtract the pre-processed dataset\r\nTo extract the pre-processed dataset, we can prep() the recipe for our datset then bake() the prepped recipe to extract the pre-processed data.\r\nHowever, in tidymodels we can use workflows() where we don‚Äôt need to prep() or bake() the recipe.\r\n\r\n\r\nprep(rec) #%>%\r\n\r\nData Recipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   id vars          8\r\n   outcome          1\r\n predictor         79\r\n\r\nTraining data contained 1421 data points and 1421 incomplete rows. \r\n\r\nOperations:\r\n\r\nVariable mutation for tst_dt [trained]\r\nSparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]\r\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\r\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\r\nMedian Imputation for enrl_grd, tst_dt, lat, lon, ... [trained]\r\nDummy variables from gndr, ethnic_cd, tst_bnch, ... [trained]\r\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\r\n\r\n # bake(data_train)   # We are using workflow so no need to bake.\r\n\r\nThe next step is to specify our ML models, using the parsnip package. To specify the model, there are four primary components: model type, arguments, engine, and mode.\r\n3. Linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\n# Specify the model\r\n\r\nmod_linear <- linear_reg() %>%\r\n  set_engine(\"lm\") %>%  # engine for linear regression\r\n  set_mode(\"regression\")  # regression for continous outcome varaible.\r\n\r\n# Workflow\r\nlm_wf <- workflow() %>% # set the workflow\r\n  add_recipe(rec) %>% # add recipe\r\n  add_model(mod_linear) # add model\r\n  \r\n\r\n# Fit the linear model\r\nmod_linear_fit<- fit_resamples(\r\n  mod_linear,\r\n  preprocessor = rec,\r\n  resamples = cv,\r\n  metrics = metric_set(rmse),\r\n  control = control_resamples(verbose = TRUE,\r\n                                    save_pred = TRUE))\r\n\r\nCollect metrics on linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\nmod_linear_fit %>%\r\n  collect_metrics() %>%\r\n  filter(.metric == \"rmse\") \r\n\r\n# A tibble: 1 x 5\r\n  .metric .estimator  mean     n std_err\r\n  <chr>   <chr>      <dbl> <int>   <dbl>\r\n1 rmse    standard    96.3    10    2.49\r\n\r\n#RMSE = 96.28\r\n\r\nOur data has multiple variables and some of them are highly correlated. In case like this, linear model usually performs poorly. So let‚Äôs try other alternatives such as penalized regression. We will use the same recipe (i.e. rec) for all models in this post. We will use tune_grid() to perform a grid search for the best combination of tuned hyperparameters such penalty.\r\n4. Lasso Regression (Type of penalized Regression)\r\n\r\n\r\n# Specify the model\r\n\r\nset.seed(3000)\r\n\r\n# specity lasso with random penalty value and set 1 for mixture.\r\nlasso_mod <- linear_reg(penalty = 0.1, mixture = 1) %>%\r\n  set_engine(\"glmnet\") \r\n\r\nwf_lasso <- workflow() %>%\r\n   add_recipe(rec)\r\n\r\nlasso_fit <- wf_lasso %>%\r\n  add_model(lasso_mod) %>%\r\n  fit(data = data_train)\r\n\r\n\r\nlasso_fit %>%\r\n  pull_workflow_fit() %>%\r\n  tidy()\r\n\r\n# A tibble: 80 x 3\r\n   term        estimate penalty\r\n   <chr>          <dbl>   <dbl>\r\n 1 (Intercept) -6.64e+3     0.1\r\n 2 enrl_grd     2.74e+1     0.1\r\n 3 tst_dt       4.49e-6     0.1\r\n 4 lat         -6.14e-1     0.1\r\n 5 lon         -3.36e+0     0.1\r\n 6 ncesag       0.          0.1\r\n 7 zip          1.65e-2     0.1\r\n 8 total_n     -1.57e-3     0.1\r\n 9 fr_lnch_n   -8.84e-2     0.1\r\n10 red_lnch_n   4.70e-1     0.1\r\n# ... with 70 more rows\r\n\r\nTune Lasso parameters We will use resampling and tuning to figure out right regularization parameter ‚Äôpenalty`\r\n\r\n\r\n# Tuning lasso parameters\r\n\r\nset.seed(3000)\r\n\r\ntune_lasso <- linear_reg(penalty = tune(), mixture = 1) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n# Tune the lasso grid\r\nlambda_grid <- grid_regular(penalty(), levels = 50)\r\n\r\nTune the grid using workflow object\r\n\r\n\r\ndoParallel::registerDoParallel()\r\n\r\nset.seed(3000)\r\nlasso_grid <- tune_grid(\r\n  wf_lasso %>%\r\n    add_model(tune_lasso), \r\n  resamples = cv,\r\n  grid = lambda_grid\r\n)\r\n\r\n\r\n\r\n# Results\r\n\r\nlasso_grid %>%\r\n  collect_metrics()\r\n\r\n# A tibble: 100 x 7\r\n    penalty .metric .estimator   mean     n std_err .config\r\n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <fct>  \r\n 1 1.00e-10 rmse    standard   95.9      10  2.50   Model01\r\n 2 1.00e-10 rsq     standard    0.362    10  0.0169 Model01\r\n 3 1.60e-10 rmse    standard   95.9      10  2.50   Model02\r\n 4 1.60e-10 rsq     standard    0.362    10  0.0169 Model02\r\n 5 2.56e-10 rmse    standard   95.9      10  2.50   Model03\r\n 6 2.56e-10 rsq     standard    0.362    10  0.0169 Model03\r\n 7 4.09e-10 rmse    standard   95.9      10  2.50   Model04\r\n 8 4.09e-10 rsq     standard    0.362    10  0.0169 Model04\r\n 9 6.55e-10 rmse    standard   95.9      10  2.50   Model05\r\n10 6.55e-10 rsq     standard    0.362    10  0.0169 Model05\r\n# ... with 90 more rows\r\n\r\n# RMSE = 95.94\r\n\r\nVisualize the performance Results look fine so let‚Äôs visualize the performance with the regularization parameters.\r\n\r\n\r\noptions(scipen = 999)\r\n\r\nlasso_grid %>%\r\n  collect_metrics() %>%\r\n  ggplot(aes(penalty, mean, color = .metric)) +\r\n  geom_errorbar(aes(\r\n    ymin = mean - std_err,\r\n    ymax = mean + std_err\r\n  ),\r\n  alpha = 0.5\r\n  ) +\r\n  geom_line(size = 1.5) +\r\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\r\n  scale_x_log10() +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\nFor choosing our final parameters, let‚Äôs get the lowest RMSE. Once we have the lowest RMSE, we can finalize our workflow by updating with lowest RMSE.\r\n\r\n\r\nlowest_rmse <- lasso_grid %>%\r\n  select_best(\"rmse\")\r\n\r\nfinal_lasso <- finalize_workflow(\r\n  wf_lasso %>% add_model(tune_lasso),\r\n  lowest_rmse\r\n)\r\n\r\n\r\n\r\nlibrary(vip)\r\n\r\nfinal_lasso %>%\r\n  fit(data_train) %>%\r\n  pull_workflow_fit() %>%\r\n  vi(lambda = lowest_rmse$penalty) %>%\r\n  mutate(\r\n    Importance = abs(Importance),\r\n    Variable = fct_reorder(Variable, Importance)\r\n  ) %>%\r\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\r\n  geom_col() +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  labs(y = NULL)\r\n\r\n\r\n\r\n\r\nlast_fit(\r\n  final_lasso,\r\n  data_split) %>%\r\n  collect_metrics()\r\n\r\n# A tibble: 2 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard      90.0  \r\n2 rsq     standard       0.424\r\n\r\n# RMSE for full dataset = 89.95\r\n\r\n\r\n\r\n",
    "preview": "posts/11-29-20_Asha/Linear_model_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2020-12-07T15:51:49-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "Tuning process and final model summary",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "preview": "posts/11-29-20_Chris/ML_models_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-07T18:30:56-08:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_data_prep_&_description_AY/",
    "title": "Data preparation and exploration",
    "description": "Here we joined our datasets and explored through visualization.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1. Packages\r\n2. Joining the datasets\r\n3.Data Exploration\r\n1. Packages\r\nFor the purpose of data loading and cleaning, we are using following packages in R: {tidyverse}, {here}, {rio}, and {skimr}\r\n2. Joining the datasets\r\nFor the purpose of demonstration, we will be using 1% of the data with sample_frac() to keep computing time low. All our datasets have school ids which we used as key to join the datasets.\r\nAfter loading our three datasets, we joined them together to make one cohesive dataset, to be used for ML modeling. After joining, the dataset contains student-level variables (e.g.¬†gender, ethnicity, enrollement in special education, etc.) as well as district-level variables ( school longitude and latitude, proportion of free and reduced lunch, etc.). All of these variables will be used in our ML models to predict student score in the statewide assessment. Here is the preview of our final dataset, ready to be used for ML modeling.\r\n\r\nTable 1: Data summary\r\nName\r\ndata\r\nNumber of rows\r\n1894\r\nNumber of columns\r\n88\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n5\r\nfactor\r\n28\r\nlogical\r\n1\r\nnumeric\r\n54\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ncounty\r\n28\r\n0.99\r\n11\r\n17\r\n0\r\n34\r\n0\r\nlocale\r\n28\r\n0.99\r\n14\r\n19\r\n0\r\n12\r\n0\r\ntitle1_status\r\n28\r\n0.99\r\n36\r\n47\r\n0\r\n3\r\n0\r\nlea_name\r\n30\r\n0.98\r\n20\r\n43\r\n0\r\n145\r\n0\r\nsch_name\r\n28\r\n0.99\r\n8\r\n60\r\n0\r\n695\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\ngndr\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 967, F: 927\r\nethnic_cd\r\n0\r\n1.00\r\nFALSE\r\n7\r\nW: 1183, H: 441, M: 103, A: 79\r\ntst_bnch\r\n0\r\n1.00\r\nFALSE\r\n6\r\n2B: 347, G4: 319, G6: 316, 3B: 307\r\ntst_dt\r\n0\r\n1.00\r\nFALSE\r\n52\r\n5/2: 160, 5/2: 138, 5/1: 125, 5/2: 114\r\nmigrant_ed_fg\r\n0\r\n1.00\r\nFALSE\r\n2\r\nN: 1848, Y: 46\r\nind_ed_fg\r\n1\r\n1.00\r\nFALSE\r\n2\r\nN: 1872, Y: 21, y: 0\r\nsp_ed_fg\r\n1\r\n1.00\r\nFALSE\r\n2\r\nN: 1652, Y: 241\r\ntag_ed_fg\r\n6\r\n1.00\r\nFALSE\r\n2\r\nN: 1791, Y: 97\r\necon_dsvntg\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1094, N: 794\r\nayp_lep\r\n1534\r\n0.19\r\nFALSE\r\n7\r\nF: 165, Y: 69, E: 64, N: 29\r\nstay_in_dist\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1845, N: 43\r\nstay_in_schl\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1830, N: 58\r\ndist_sped\r\n6\r\n1.00\r\nFALSE\r\n2\r\nN: 1867, Y: 21\r\ntrgt_assist_fg\r\n6\r\n1.00\r\nFALSE\r\n3\r\nN: 1812, Y: 74, y: 2\r\nayp_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\nayp_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\nayp_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1842, N: 52\r\nayp_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1808, N: 86\r\nrc_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\nrc_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\nrc_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1842, N: 52\r\nrc_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1808, N: 86\r\nlang_cd\r\n1842\r\n0.03\r\nFALSE\r\n1\r\nS: 52\r\ntst_atmpt_fg\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1886, P: 8\r\ngrp_rpt_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\ngrp_rpt_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\ngrp_rpt_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1882, N: 12\r\ngrp_rpt_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1861, N: 33\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\ncalc_admn_cd\r\n1894\r\n0\r\nNaN\r\n:\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nid\r\n0\r\n1.00\r\n1.273822e+05\r\n73162.85\r\n‚ñá‚ñá‚ñá‚ñá‚ñá\r\nattnd_dist_inst_id\r\n0\r\n1.00\r\n2.127740e+03\r\n212.96\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nattnd_schl_inst_id\r\n0\r\n1.00\r\n1.400890e+03\r\n1393.20\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nenrl_grd\r\n0\r\n1.00\r\n5.490000e+00\r\n1.68\r\n‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ\r\npartic_dist_inst_id\r\n6\r\n1.00\r\n2.131510e+03\r\n225.53\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\npartic_schl_inst_id\r\n6\r\n1.00\r\n1.403160e+03\r\n1394.28\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nscore\r\n0\r\n1.00\r\n2.500430e+03\r\n119.82\r\n‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ\r\nncessch\r\n26\r\n0.99\r\n4.106893e+11\r\n395453731.73\r\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\r\nlat\r\n30\r\n0.98\r\n4.475000e+01\r\n1.04\r\n‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñá\r\nlon\r\n30\r\n0.98\r\n-1.225200e+02\r\n1.17\r\n‚ñÖ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nncesag\r\n28\r\n0.99\r\n4.106891e+06\r\n3956.50\r\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\r\nzip\r\n28\r\n0.99\r\n9.731347e+04\r\n232.97\r\n‚ñá‚ñá‚ñÜ‚ñÅ‚ñÇ\r\ntotal_n\r\n28\r\n0.99\r\n5.721300e+02\r\n357.46\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nfr_lnch_n\r\n75\r\n0.96\r\n2.310900e+02\r\n148.02\r\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nred_lnch_n\r\n75\r\n0.96\r\n4.091000e+01\r\n25.11\r\n‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÅ\r\npupil_tch_ratio\r\n28\r\n0.99\r\n2.026000e+01\r\n3.60\r\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nipr_est\r\n28\r\n0.99\r\n3.220900e+02\r\n155.65\r\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\r\nleaid\r\n28\r\n0.99\r\n4.106891e+06\r\n3956.50\r\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\r\nst_schid\r\n28\r\n0.99\r\n1.388760e+03\r\n1387.95\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nlea_cwiftest\r\n30\r\n0.98\r\n9.400000e-01\r\n0.07\r\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÜ\r\np_american_indian_alaska_native\r\n28\r\n0.99\r\n1.000000e-02\r\n0.04\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_asian\r\n28\r\n0.99\r\n4.000000e-02\r\n0.07\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_native_hawaiian_pacific_islander\r\n28\r\n0.99\r\n1.000000e-02\r\n0.01\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_black_african_american\r\n28\r\n0.99\r\n2.000000e-02\r\n0.04\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_hispanic_latino\r\n28\r\n0.99\r\n2.500000e-01\r\n0.19\r\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\r\np_white\r\n28\r\n0.99\r\n6.100000e-01\r\n0.20\r\n‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá\r\np_multiracial\r\n28\r\n0.99\r\n7.000000e-02\r\n0.03\r\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\r\nfr_lnch_prop\r\n75\r\n0.96\r\n4.400000e-01\r\n0.21\r\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\r\nred_lnch_prop\r\n75\r\n0.96\r\n8.000000e-02\r\n0.03\r\n‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\npercent_level_4\r\n31\r\n0.98\r\n2.211000e+01\r\n13.36\r\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\r\npercent_level_3\r\n31\r\n0.98\r\n3.123000e+01\r\n9.00\r\n‚ñÇ‚ñÜ‚ñá‚ñÅ‚ñÅ\r\npercent_level_2\r\n31\r\n0.98\r\n2.268000e+01\r\n7.09\r\n‚ñÅ‚ñÜ‚ñá‚ñÅ‚ñÅ\r\npercent_level_1\r\n31\r\n0.98\r\n2.399000e+01\r\n13.48\r\n‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nsch_percent_level_4\r\n28\r\n0.99\r\n2.243000e+01\r\n12.46\r\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nsch_percent_level_3\r\n28\r\n0.99\r\n3.118000e+01\r\n6.83\r\n‚ñÅ‚ñÇ‚ñá‚ñá‚ñÇ\r\nsch_percent_level_2\r\n28\r\n0.99\r\n2.259000e+01\r\n5.31\r\n‚ñÅ‚ñÉ‚ñá‚ñá‚ñÅ\r\nsch_percent_level_1\r\n28\r\n0.99\r\n2.380000e+01\r\n12.05\r\n‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nunder_25\r\n44\r\n0.98\r\n2.100000e-01\r\n0.06\r\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\r\nunder_50\r\n44\r\n0.98\r\n2.300000e-01\r\n0.06\r\n‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÇ\r\nunder_75\r\n44\r\n0.98\r\n1.600000e-01\r\n0.03\r\n‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nunder_100\r\n44\r\n0.98\r\n1.200000e-01\r\n0.02\r\n‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ\r\nunder_200\r\n44\r\n0.98\r\n2.000000e-01\r\n0.07\r\n‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÅ\r\nover_200\r\n44\r\n0.98\r\n7.000000e-02\r\n0.08\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\npercent_less_than_9th_grade\r\n40\r\n0.98\r\n4.040000e+00\r\n3.37\r\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\r\npercent_high_school_graduate_or_higher\r\n40\r\n0.98\r\n8.972000e+01\r\n5.79\r\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÖ\r\npercent_bachelors_degree_or_higher\r\n40\r\n0.98\r\n3.122000e+01\r\n16.15\r\n‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÅ\r\npercent_associates_degree\r\n40\r\n0.98\r\n8.780000e+00\r\n1.99\r\n‚ñÇ‚ñá‚ñá‚ñÅ‚ñÅ\r\npercent_graduate_or_professional_degree\r\n40\r\n0.98\r\n1.171000e+01\r\n7.87\r\n‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÅ\r\npercent_bachelors_degree\r\n40\r\n0.98\r\n1.815000e+01\r\n8.52\r\n‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÇ\r\npercent_high_school_graduate\r\n40\r\n0.98\r\n2.511000e+01\r\n7.67\r\n‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÅ\r\npercent_9th_to_12th_grade_no_diploma\r\n40\r\n0.98\r\n6.900000e+00\r\n3.42\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\npercent_some_college\r\n40\r\n0.98\r\n2.676000e+01\r\n4.37\r\n‚ñÅ‚ñÉ‚ñá‚ñá‚ñÅ\r\nmedian_income\r\n28\r\n0.99\r\n2.812101e+04\r\n4487.40\r\n‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÜ\r\nmedian_rent\r\n28\r\n0.99\r\n9.835000e+02\r\n153.33\r\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñá\r\n\r\n3.Data Exploration\r\nCorrelation: Figure 1 displays significant (p < 0.05) correlation between numeric variables. Red dots show significant pearson‚Äôs correlation coefficient between 0 to 1 and white dots show significant coefficient between -1 to 0. Blank spaces are not significant.\r\n\r\n\r\n\r\nWe looked if pupil-teacher ratio and score differ by county.\r\n\r\n\r\n\r\nWe explored adult qualification by economic disadvantage and grade.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T15:51:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_datasets_AY/",
    "title": "Datasets",
    "description": "Here is the preview of datasets used in our final project.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1. Introduction\r\n2. Statewide testing data (Original dataset)\r\n3. Fall membership report data\r\n4. Bonus Dataset\r\n5. Supplemental small dataset\r\n1. Introduction\r\nFor this project, we collected and joined three structured datasets and one supplemental small dataset only for XG boost model. Datasets are selected on the basis of variables that were correlated with the outcome variable. In our datasets, our outcome variable is children‚Äôs score on the statewide assessment. Please see below for more information on each dataset.\r\n2. Statewide testing data (Original dataset)\r\nStudents in every state across the nation are tested annually in reading and math in grades 3-8. Dataset used in the project are simulated from an actual statewide testing administration across the state of Oregon and the overall distribution are highly similar. Our continous outcome variable is score which is also presented in categorical form as classification. For the project, our models are run to predict the continous score variable. Our data contains other variables of interest (predictors) such as gender, ethnicity, economic disadvange, and location. In our simulated dataset, school ids are real which we used as key to join other datasets. Here is the preview of our statewide testing dataset.\r\n\r\nTable 1: Data summary\r\nName\r\nd\r\nNumber of rows\r\n1894\r\nNumber of columns\r\n39\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n28\r\nlogical\r\n1\r\nnumeric\r\n10\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ngndr\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nethnic_cd\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n7\r\n0\r\ntst_bnch\r\n0\r\n1.00\r\n2\r\n2\r\n0\r\n6\r\n0\r\ntst_dt\r\n0\r\n1.00\r\n16\r\n17\r\n0\r\n48\r\n0\r\nmigrant_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nind_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nsp_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ntag_ed_fg\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\necon_dsvntg\r\n4\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_lep\r\n1540\r\n0.19\r\n1\r\n1\r\n0\r\n8\r\n0\r\nstay_in_dist\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nstay_in_schl\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ndist_sped\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ntrgt_assist_fg\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nlang_cd\r\n1851\r\n0.02\r\n1\r\n1\r\n0\r\n1\r\n0\r\ntst_atmpt_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\ncalc_admn_cd\r\n1894\r\n0\r\nNaN\r\n:\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nid\r\n0\r\n1.00\r\n1.267936e+05\r\n72111.86\r\n‚ñá‚ñá‚ñá‚ñá‚ñá\r\nattnd_dist_inst_id\r\n0\r\n1.00\r\n2.116080e+03\r\n188.53\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nattnd_schl_inst_id\r\n0\r\n1.00\r\n1.345800e+03\r\n1388.28\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nenrl_grd\r\n0\r\n1.00\r\n5.440000e+00\r\n1.67\r\n‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ\r\npartic_dist_inst_id\r\n4\r\n1.00\r\n2.116160e+03\r\n188.71\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\npartic_schl_inst_id\r\n4\r\n1.00\r\n1.345210e+03\r\n1386.64\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nscore\r\n0\r\n1.00\r\n2.498910e+03\r\n113.99\r\n‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÅ\r\nncessch\r\n29\r\n0.98\r\n4.106795e+11\r\n385634091.96\r\n‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ\r\nlat\r\n31\r\n0.98\r\n4.476000e+01\r\n1.00\r\n‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñá\r\nlon\r\n31\r\n0.98\r\n-1.224600e+02\r\n1.28\r\n‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\n\r\n3. Fall membership report data\r\nThe Oregon Department for Education (ODE) publicly releases student enrollment reports detailing the number of K-12 students who are enrolled on the first school day in October of each year.This report is known as Fall membership report which contains data on race/ethinicity percentages for schools in Oregon. Here is the preview of our data.\r\n\r\nTable 2: Data summary\r\nName\r\nethnicities\r\nNumber of rows\r\n1459\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nnumeric\r\n8\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nsch_name\r\n0\r\n1\r\n8\r\n60\r\n0\r\n1381\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nattnd_schl_inst_id\r\n0\r\n1\r\n1661.26\r\n1558.62\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÇ\r\np_american_indian_alaska_native\r\n0\r\n1\r\n0.02\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_asian\r\n0\r\n1\r\n0.03\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_native_hawaiian_pacific_islander\r\n0\r\n1\r\n0.01\r\n0.01\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_black_african_american\r\n0\r\n1\r\n0.02\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_hispanic_latino\r\n0\r\n1\r\n0.21\r\n0.18\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\np_white\r\n0\r\n1\r\n0.65\r\n0.22\r\n‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÖ\r\np_multiracial\r\n0\r\n1\r\n0.06\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n\r\n4. Bonus Dataset\r\nWe retrieved another dataset that contains k-12 data collected by zip code, NCES school IDs, State school IDs, and county level. It contains variables that are not present in the statewide testing dataset such as teacher-pupil ratio, percentage of people with high school, no diploma, and higher education. These variables may have effect on our outcome variable (score). Please see below preview other variables present in the dataset.\r\n\r\nTable 3: Data summary\r\nName\r\nbonus_data\r\nNumber of rows\r\n3889\r\nNumber of columns\r\n51\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n7\r\nnumeric\r\n44\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nncessch\r\n0\r\n1.00\r\n18\r\n21\r\n0\r\n1257\r\n0\r\ncounty\r\n0\r\n1.00\r\n11\r\n17\r\n0\r\n36\r\n0\r\nlocale\r\n0\r\n1.00\r\n14\r\n19\r\n0\r\n12\r\n0\r\ntitle1_status\r\n0\r\n1.00\r\n3\r\n47\r\n0\r\n4\r\n0\r\npupil_tch_ratio\r\n0\r\n1.00\r\n1\r\n6\r\n0\r\n789\r\n0\r\nlea_name\r\n54\r\n0.99\r\n20\r\n43\r\n0\r\n195\r\n0\r\nsch_name\r\n12\r\n1.00\r\n8\r\n60\r\n0\r\n1203\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nncesag\r\n0\r\n1.00\r\n4106916.49\r\n4236.76\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nzip\r\n0\r\n1.00\r\n97366.87\r\n258.54\r\n‚ñá‚ñá‚ñá‚ñÇ‚ñÖ\r\ntotal_n\r\n1\r\n1.00\r\n406.93\r\n318.97\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nfr_lnch_n\r\n428\r\n0.89\r\n180.93\r\n138.74\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nred_lnch_n\r\n428\r\n0.89\r\n31.81\r\n25.57\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nipr_est\r\n12\r\n1.00\r\n303.52\r\n135.95\r\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\r\nleaid\r\n12\r\n1.00\r\n4106915.58\r\n4240.21\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nst_schid\r\n12\r\n1.00\r\n1610.13\r\n1592.21\r\n‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ\r\nlea_cwiftest\r\n54\r\n0.99\r\n0.92\r\n0.07\r\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÖ\r\np_american_indian_alaska_native\r\n12\r\n1.00\r\n0.01\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_asian\r\n12\r\n1.00\r\n0.03\r\n0.06\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_native_hawaiian_pacific_islander\r\n12\r\n1.00\r\n0.01\r\n0.02\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_black_african_american\r\n12\r\n1.00\r\n0.02\r\n0.05\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\np_hispanic_latino\r\n12\r\n1.00\r\n0.20\r\n0.18\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\np_white\r\n12\r\n1.00\r\n0.66\r\n0.21\r\n‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÜ\r\np_multiracial\r\n12\r\n1.00\r\n0.06\r\n0.04\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nfr_lnch_prop\r\n428\r\n0.89\r\n0.45\r\n0.21\r\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\r\nred_lnch_prop\r\n428\r\n0.89\r\n0.08\r\n0.04\r\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nenrl_grd\r\n236\r\n0.94\r\n5.13\r\n1.66\r\n‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÇ\r\npercent_level_4\r\n556\r\n0.86\r\n22.06\r\n14.25\r\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\r\npercent_level_3\r\n556\r\n0.86\r\n30.18\r\n10.59\r\n‚ñÅ‚ñá‚ñá‚ñÅ‚ñÅ\r\npercent_level_2\r\n556\r\n0.86\r\n22.93\r\n8.54\r\n‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ\r\npercent_level_1\r\n556\r\n0.86\r\n24.83\r\n14.81\r\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nsch_percent_level_4\r\n152\r\n0.96\r\n22.89\r\n13.04\r\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nsch_percent_level_3\r\n152\r\n0.96\r\n30.46\r\n7.65\r\n‚ñÅ‚ñÇ‚ñá‚ñÉ‚ñÅ\r\nsch_percent_level_2\r\n152\r\n0.96\r\n22.74\r\n6.80\r\n‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÅ\r\nsch_percent_level_1\r\n152\r\n0.96\r\n23.91\r\n13.16\r\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nunder_25\r\n180\r\n0.95\r\n0.22\r\n0.07\r\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\r\nunder_50\r\n180\r\n0.95\r\n0.24\r\n0.06\r\n‚ñÇ‚ñá‚ñÜ‚ñÅ‚ñÅ\r\nunder_75\r\n180\r\n0.95\r\n0.17\r\n0.04\r\n‚ñÅ‚ñÖ‚ñá‚ñÅ‚ñÅ\r\nunder_100\r\n180\r\n0.95\r\n0.12\r\n0.03\r\n‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÅ\r\nunder_200\r\n180\r\n0.95\r\n0.19\r\n0.08\r\n‚ñÇ‚ñá‚ñá‚ñá‚ñÅ\r\nover_200\r\n180\r\n0.95\r\n0.06\r\n0.07\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\npercent_less_than_9th_grade\r\n39\r\n0.99\r\n3.76\r\n3.41\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\npercent_high_school_graduate_or_higher\r\n39\r\n0.99\r\n89.72\r\n5.83\r\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñá\r\npercent_bachelors_degree_or_higher\r\n39\r\n0.99\r\n28.77\r\n15.64\r\n‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÅ\r\npercent_associates_degree\r\n39\r\n0.99\r\n8.88\r\n2.33\r\n‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ\r\npercent_graduate_or_professional_degree\r\n39\r\n0.99\r\n10.71\r\n7.63\r\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\r\npercent_bachelors_degree\r\n39\r\n0.99\r\n17.11\r\n8.56\r\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\r\npercent_high_school_graduate\r\n39\r\n0.99\r\n26.60\r\n8.26\r\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\r\npercent_9th_to_12th_grade_no_diploma\r\n39\r\n0.99\r\n7.04\r\n3.63\r\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\r\npercent_some_college\r\n39\r\n0.99\r\n27.29\r\n5.14\r\n‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñÅ\r\nmedian_income\r\n0\r\n1.00\r\n27158.20\r\n4434.27\r\n‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÉ\r\nmedian_rent\r\n0\r\n1.00\r\n942.33\r\n166.09\r\n‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñá\r\n\r\n5. Supplemental small dataset\r\nSupplemental small dataset contains high school dropout rates and out-of-school suspension rates by state districts IDs. This dataset has been used only in XG boost model. For details, please check the post\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T15:51:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_welcome_AY/",
    "title": "Hello!",
    "description": "Welcome to our Machine Learning blog",
    "author": [
      {
        "name": "AY, AG, and CI",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\r\nThis blog was created as part of the final project for the EDLD 654 Applied Machine Learning for Educational Data Science, taught by Prof.¬†Daniel Anderson and Prof.¬†Joseph Nese at the University of Oregon. The blog contains application of Machine Learning models to educational data for predictive analysis. Following models were presented in the final project:\r\nLinear regression (add link to my post, if posting separate.)\r\nRandom Forest (add link Ale‚Äôs post if posting separate)\r\nGradient Boosting (add link Chris‚Äôs post, if posting separete)\r\nThe contributors of the blog are:\r\nAsha Yadav\r\nAlejandra\r\nChris Ives\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-04T17:55:44-08:00",
    "input_file": {}
  }
]
