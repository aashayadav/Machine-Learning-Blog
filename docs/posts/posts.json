[
  {
    "path": "posts/11-29-20_Alejandra/",
    "title": "Random Forest",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Alejandra",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-04T18:08:21-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_Asha/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1.Splitting and resampling1a. Splitting\r\n1b. Resampling\r\n\r\n2.Pre-processing\r\n3. Linear model\r\n4. Lasso Regression (Type of penalized Regression)\r\n1.Splitting and resampling\r\nData splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.\r\n1a. Splitting\r\nWe will split our dataset into training and testing set. The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model’s performance.\r\nWe will use initial_split() function from the rsample package which creates a split object. The split object d_split, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).\r\n\r\n\r\nset.seed(3000)\r\n# Create split object specifying (75%) and testing (25%)\r\n\r\ndata_split <- initial_split(data, prop = 3/4) \r\n\r\ndata_split\r\n\r\n<Analysis/Assess/Total>\r\n<1421/473/1894>\r\n\r\nWe will extract the training and testing set from the split object, d_split by using the training() and testing() functions.\r\n\r\n\r\n# Extract training and testing set\r\nset.seed(3000)\r\n\r\ndata_train <- training(data_split)  # Our training dataset has 1421 observations.\r\n\r\ndata_test <- testing(data_split)    # Our test dataset has 473 observations.\r\n\r\n1b. Resampling\r\nAt some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we’ll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = number of time we resample.\r\n\r\n\r\n# Resample the data with 10-fold cross validation.\r\nset.seed(3000)\r\ncv <- vfold_cv(data_train)\r\n\r\n2.Pre-processing\r\nPreprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as all_numeric(), all_predictors() as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (step_medianimpute), rescaling (step_scale), standardizing (step_normalize), PCA (step_pca) and creating dummy variables (step_dummy). A full list of pre-processing can be found here.\r\nWe will use recipe() function to indicate our outcome and predictor variables in our recipe. We will use ~. to indicate that we are using all variables to predict the outcome variable score. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, rec, it shows numbers of predictor variables have been specified. It doen’t actually apply the predictors yet. We will use same receipe throughout this post.\r\n\r\n\r\nset.seed(3000)\r\n\r\nrec <- recipe(score ~ ., data_train) %>% \r\n  step_mutate(tst_dt = as.numeric(lubridate::\r\n                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \r\n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\r\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\r\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \r\n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\r\n  step_medianimpute(all_numeric(), -all_outcomes(), \r\n                    -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\r\n  step_dummy(all_nominal(), -has_role(\"id vars\")) %>% # dummy codes categorical variables\r\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\r\n\r\nExtract the pre-processed dataset\r\nTo extract the pre-processed dataset, we can prep() the recipe for our datset then bake() the prepped recipe to extract the pre-processed data.\r\nHowever, in tidymodels we can use workflows() where we don’t need to prep() or bake() the recipe.\r\n\r\n\r\nprep(rec) #%>%\r\n\r\nData Recipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   id vars          8\r\n   outcome          1\r\n predictor         79\r\n\r\nTraining data contained 1421 data points and 1421 incomplete rows. \r\n\r\nOperations:\r\n\r\nVariable mutation for tst_dt [trained]\r\nSparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]\r\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\r\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\r\nMedian Imputation for enrl_grd, tst_dt, lat, lon, ... [trained]\r\nDummy variables from gndr, ethnic_cd, tst_bnch, ... [trained]\r\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\r\n\r\n # bake(data_train)   # We are using workflow so no need to bake.\r\n\r\nThe next step is to specify our ML models, using the parsnip package. To specify the model, there are four primary components: model type, arguments, engine, and mode.\r\n3. Linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\n# Specify the model\r\n\r\nmod_linear <- linear_reg() %>%\r\n  set_engine(\"lm\") %>%  # engine for linear regression\r\n  set_mode(\"regression\")  # regression for continous outcome varaible.\r\n\r\n# Workflow\r\nlm_wf <- workflow() %>% # set the workflow\r\n  add_recipe(rec) %>% # add recipe\r\n  add_model(mod_linear) # add model\r\n  \r\n\r\n# Fit the linear model\r\nmod_linear_fit<- fit_resamples(\r\n  mod_linear,\r\n  preprocessor = rec,\r\n  resamples = cv,\r\n  metrics = metric_set(rmse),\r\n  control = control_resamples(verbose = TRUE,\r\n                                    save_pred = TRUE))\r\n\r\nCollect metrics on linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\nmod_linear_fit %>%\r\n  collect_metrics() %>%\r\n  filter(.metric == \"rmse\") \r\n\r\n# A tibble: 1 x 5\r\n  .metric .estimator  mean     n std_err\r\n  <chr>   <chr>      <dbl> <int>   <dbl>\r\n1 rmse    standard    96.3    10    2.49\r\n\r\n#RMSE = 96.28\r\n\r\nOur data has multiple variables and some of them are highly correlated. In case like this, linear model usually performs poorly. So let’s try other alternatives such as penalized regression. We will use the same recipe (i.e. rec) for all models in this post. We will use tune_grid() to perform a grid search for the best combination of tuned hyperparameters such penalty.\r\n4. Lasso Regression (Type of penalized Regression)\r\n\r\n\r\n# Specify the model\r\n\r\nset.seed(3000)\r\n\r\n# specity lasso with random penalty value and set 1 for mixture.\r\nlasso_mod <- linear_reg(penalty = 0.1, mixture = 1) %>%\r\n  set_engine(\"glmnet\") \r\n\r\nwf_lasso <- workflow() %>%\r\n   add_recipe(rec)\r\n\r\nlasso_fit <- wf_lasso %>%\r\n  add_model(lasso_mod) %>%\r\n  fit(data = data_train)\r\n\r\n\r\nlasso_fit %>%\r\n  pull_workflow_fit() %>%\r\n  tidy()\r\n\r\n# A tibble: 80 x 3\r\n   term        estimate penalty\r\n   <chr>          <dbl>   <dbl>\r\n 1 (Intercept) -6.64e+3     0.1\r\n 2 enrl_grd     2.74e+1     0.1\r\n 3 tst_dt       4.49e-6     0.1\r\n 4 lat         -6.14e-1     0.1\r\n 5 lon         -3.36e+0     0.1\r\n 6 ncesag       0.          0.1\r\n 7 zip          1.65e-2     0.1\r\n 8 total_n     -1.57e-3     0.1\r\n 9 fr_lnch_n   -8.84e-2     0.1\r\n10 red_lnch_n   4.70e-1     0.1\r\n# ... with 70 more rows\r\n\r\nTune Lasso parameters We will use resampling and tuning to figure out right regularization parameter ’penalty`\r\n\r\n\r\n# Tuning lasso parameters\r\n\r\nset.seed(3000)\r\n\r\ntune_lasso <- linear_reg(penalty = tune(), mixture = 1) %>%\r\n  set_engine(\"glmnet\")\r\n\r\n# Tune the lasso grid\r\nlambda_grid <- grid_regular(penalty(), levels = 50)\r\n\r\nTune the grid using workflow object\r\n\r\n\r\ndoParallel::registerDoParallel()\r\n\r\nset.seed(3000)\r\nlasso_grid <- tune_grid(\r\n  wf_lasso %>%\r\n    add_model(tune_lasso), \r\n  resamples = cv,\r\n  grid = lambda_grid\r\n)\r\n\r\n\r\n\r\n# Results\r\n\r\nlasso_grid %>%\r\n  collect_metrics()\r\n\r\n# A tibble: 100 x 7\r\n    penalty .metric .estimator   mean     n std_err .config\r\n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <fct>  \r\n 1 1.00e-10 rmse    standard   95.9      10  2.50   Model01\r\n 2 1.00e-10 rsq     standard    0.362    10  0.0169 Model01\r\n 3 1.60e-10 rmse    standard   95.9      10  2.50   Model02\r\n 4 1.60e-10 rsq     standard    0.362    10  0.0169 Model02\r\n 5 2.56e-10 rmse    standard   95.9      10  2.50   Model03\r\n 6 2.56e-10 rsq     standard    0.362    10  0.0169 Model03\r\n 7 4.09e-10 rmse    standard   95.9      10  2.50   Model04\r\n 8 4.09e-10 rsq     standard    0.362    10  0.0169 Model04\r\n 9 6.55e-10 rmse    standard   95.9      10  2.50   Model05\r\n10 6.55e-10 rsq     standard    0.362    10  0.0169 Model05\r\n# ... with 90 more rows\r\n\r\n# RMSE = 95.94\r\n\r\nVisualize the performance Results look fine so let’s visualize the performance with the regularization parameters.\r\n\r\n\r\noptions(scipen = 999)\r\n\r\nlasso_grid %>%\r\n  collect_metrics() %>%\r\n  ggplot(aes(penalty, mean, color = .metric)) +\r\n  geom_errorbar(aes(\r\n    ymin = mean - std_err,\r\n    ymax = mean + std_err\r\n  ),\r\n  alpha = 0.5\r\n  ) +\r\n  geom_line(size = 1.5) +\r\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\r\n  scale_x_log10() +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\nFor choosing our final parameters, let’s get the lowest RMSE. Once we have the lowest RMSE, we can finalize our workflow by updating with lowest RMSE.\r\n\r\n\r\nlowest_rmse <- lasso_grid %>%\r\n  select_best(\"rmse\")\r\n\r\nfinal_lasso <- finalize_workflow(\r\n  wf_lasso %>% add_model(tune_lasso),\r\n  lowest_rmse\r\n)\r\n\r\n\r\n\r\nlibrary(vip)\r\n\r\nfinal_lasso %>%\r\n  fit(data_train) %>%\r\n  pull_workflow_fit() %>%\r\n  vi(lambda = lowest_rmse$penalty) %>%\r\n  mutate(\r\n    Importance = abs(Importance),\r\n    Variable = fct_reorder(Variable, Importance)\r\n  ) %>%\r\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\r\n  geom_col() +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  labs(y = NULL)\r\n\r\n\r\n\r\n\r\nlast_fit(\r\n  final_lasso,\r\n  data_split) %>%\r\n  collect_metrics()\r\n\r\n# A tibble: 2 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard      90.0  \r\n2 rsq     standard       0.424\r\n\r\n# RMSE for full dataset = 89.95\r\n\r\n\r\n\r\n",
    "preview": "posts/11-29-20_Asha/Linear_model_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2020-12-07T16:53:51-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_Chris/",
    "title": "XG Boost",
    "description": "We will be fitting linear model, random forest, and gradient boosting.",
    "author": [
      {
        "name": "Chris Ives",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nData Import\r\nRecipeRecipe Notes\r\n\r\nTuning HyperparametersTuning Gamma FirstTuning Results\r\n\r\nFollow-up Tree-Parameter TuningCreate Grid\r\nUpdate Model Specification\r\nTuning Results\r\n\r\nTuning Tree Parameters FirstUpdate Model Specification\r\n\r\nTune Stochastic Parameters\r\nUpdate Model Specification\r\nStochastic Parameter Results\r\nRetune Learning Rate\r\nFinal Fit Statistics\r\n\r\n\r\nData Import\r\n\r\n\r\ndata <- read.csv(\"data/train.csv\") %>%\r\n  select(-classification) %>%\r\n  mutate_if(is.character, factor) %>%\r\n  mutate(ncessch = as.numeric(ncessch))\r\n\r\nbonus <- read.csv(\"data/bonus_data_v2.csv\") %>%\r\n  mutate(ncessch = as.numeric(ncessch)) %>%\r\n  mutate(locale = gsub(\"^.{0,3}\", \"\", locale)) %>%\r\n  separate(col = locale, into = c(\"locale\", \"sublocale\"), sep = \": \")\r\n\r\ndisc <- read_csv(\"data/disc_drop.csv\") %>%\r\n  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))\r\n\r\n## join data\r\ndata <- data %>% \r\n  left_join(bonus) %>% \r\n  left_join(disc)\r\n\r\n\r\n\r\nData was merged from three files:\r\nOriginal Competition Training Dataset\r\n“Bonus Dataset” with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels\r\nSupplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs\r\nDropout Rate Data Source: https://www.oregon.gov/ode/reports-and-data/students/Pages/Dropout-Rates.aspx\r\nSuspension Rate Data Source: https://www.oregon.gov/ode/students-and-family/healthsafety/Pages/School-Discipline,-Bullying,-Restraint-and-Seclusion.aspx\r\n\r\nImportantly, the bonus data includes the variables described in the original data description page, as well as the following:\r\n2016-2017 District Finance Data:\r\nTotal revenue (rev_total)\r\nTotal local revenue (rev_local_total)\r\nTotal state revenue (rev_state_total)\r\nTotal federal revenue (rev_fed_total)\r\nTotal expenditures (exp_total)\r\nTotal current expenditures for elementary and secondary education (exp_current_elsec_total)\r\nTotal current expenditures for instruction (exp_current_instruction_total)\r\nTotal current expenditures for support services (exp_current_supp_serve_total)\r\nTotal capital outlay expenditures (outlay_capital_total)\r\nTotal salary amount (salaries_total)\r\nTotal employee benefits in dollars (benefits_employee_total)\r\nNumber of students for which the reporting local education agency is financially responsible (enrollment_fall_responsible)\r\n\r\nDistrict financial data was obtained using the educationdata R package.\r\nLastly, district finance data was included in both its raw form, and transformed by dividing each revenue and expenditure value by the number of students the LEA was financially responsible for (e.g., exp_total/enrollment_fall_responsible)\r\nRecipe\r\n\r\n\r\n\r\n\r\n\r\nrec <- recipe(score ~ ., train) %>%\r\n  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),\r\n              lang_cd = case_when(lang_cd == \"S\" ~ \"S\", TRUE ~ \"E\"),\r\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\r\n              pupil_tch_ratio = as.numeric(pupil_tch_ratio),\r\n              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,\r\n                                         pupil_tch_ratio < 25 ~ 2,\r\n                                         pupil_tch_ratio < 30 ~ 3, \r\n                                         TRUE ~ 4),\r\n              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% \r\n  step_rm(contains(\"id\"), ncessch, ncesag, lea_name, sch_name) %>%\r\n  step_mutate(hpi = as.numeric(hpi),\r\n              lat = round(lat, 2),\r\n              lon = round(lon, 2),\r\n              median_income = log(median_income),\r\n              frl_prop = fr_lnch_prop + red_lnch_prop,\r\n              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,\r\n                                    TRUE ~ 0),\r\n              over_100 = under_200 + over_200) %>% \r\n  step_interact(terms = ~ lat:lon) %>% \r\n  step_rm(fr_lnch_prop, red_lnch_prop) %>% \r\n  step_string2factor(all_nominal()) %>% \r\n  step_zv(all_predictors()) %>%\r\n  step_unknown(all_nominal()) %>% \r\n  step_medianimpute(all_numeric()) %>%\r\n  step_dummy(all_nominal(), one_hot = TRUE) %>% \r\n  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% \r\n  step_interact(~ lang_cd_S:p_hispanic_latino) %>% \r\n  step_nzv(all_predictors(), freq_cut = 995/5)\r\n\r\nbaked_train <- prep(rec) %>% \r\n  bake(train)\r\n\r\ntrain_x = data.matrix(baked_train[, -73])\r\ntrain_y = data.matrix(baked_train[, 73])\r\n\r\n\r\n\r\nRecipe Notes\r\nPupil/Teacher ratio (pupil_tch_ratio) is binned and treated as a factor to remove noise (pupil_tch_rate). Both ratio and binned version of the variable remain in the data.\r\nID variables are removed\r\nLatitude (lat) and longitude (lon) are rounded to two decimal places to reduce noise and limit precision to within 2/3 of a mile.\r\nMedian income is log transformed, as the variations in income level are expected to have greater effect near the poverty threshold and less effect in higher income brackets.\r\nFree lunch proportions and reduced lunch proportions are collapsed into a single variable frl_prop given their expected similar effects.\r\nFree lunch proportions (fr_lnch_prop) and reduced lunch proportions (red_lnch_prop) are removed from the data set in lieu of their combined proportion.\r\n\r\nA dummy coded school-level variable (schl_perf) is created that denotes whether the percentage of students reaching proficiency on the EOY state test exceeds the percentage of students that fall below proficiency standards.\r\nZero variance predictors are removed\r\nMissing data is median imputed\r\nExplicit interaction is specified between the student-level special education flag and the per-student district expenditures for special services. Th effect of special education status is expected to vary depending on the funding available and potential quality of the district’s special services.\r\nExplicit interaction is specified for the effect of Spanish language code (lang_cd) and percentage of Hispanic/Latino students at the school (p_hispanic_latino). The justification is that Spanish-speaking students are expected to receive less effective supports in schools in which they are a greater minority.\r\nNear-zero variance predictors are removed\r\nTuning Hyperparameters\r\nBecause the loss reduction (gamma) and maximum tree depth are highly dependent on one another, two approaches were taken to tune the initial model. One approach involved first tuning loss reduction, followed by tree depth and hessian weights. The other involved tuning tree complexity and hessian weights without any severe loss reduction regularization, and then pruning it back by tuning gamma.\r\nTuning Gamma First\r\n\r\n\r\nbaked_train <- prep(rec) %>% \r\n  bake(train)\r\n\r\ntrain_x = data.matrix(baked_train[, -73])\r\ntrain_y = data.matrix(baked_train[, 73])\r\n\r\ngrid <- expand.grid(loss_reduction = seq(0, 80, 5))\r\n\r\ngamma_mods <- map(grid$loss_reduction, ~{\r\n  xgb.cv(\r\n    data = train_x,\r\n    label = train_y,\r\n    nrounds = 10000,\r\n    objective = \"reg:squarederror\",\r\n    early_stopping_rounds = 50,\r\n    nfold = 10,\r\n    verbose = 1,\r\n    params = list(\r\n      eta = 0.1,\r\n      gamma = .x,\r\n      nthread = 24\r\n    )\r\n  )\r\n})\r\n\r\n\r\n\r\nTuning Results\r\n\r\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\r\n 1:  452         77.0087         0.0983        82.6056        0.6781\r\n 2:  448         77.0472         0.0837        82.6143        0.4099\r\n 3:  445         77.0877         0.0555        82.6204        0.5452\r\n 4:  524         76.3820         0.1016        82.6213        0.5638\r\n 5:  436         77.1400         0.0551        82.6243        0.3445\r\n 6:  452         77.0063         0.0802        82.6287        0.5083\r\n 7:  416         77.3305         0.0714        82.6291        0.7941\r\n 8:  447         77.0524         0.0613        82.6352        0.5932\r\n 9:  424         77.2553         0.0888        82.6371        0.8803\r\n10:  465         76.9038         0.0983        82.6379        0.5911\r\n11:  451         77.0375         0.0940        82.6454        0.4721\r\n12:  451         76.9870         0.0605        82.6509        0.7167\r\n13:  422         77.2655         0.0899        82.6538        0.6109\r\n14:  467         76.8912         0.0458        82.6568        0.2768\r\n15:  453         76.9979         0.0672        82.6589        0.5673\r\n16:  415         77.3378         0.1053        82.6646        0.7068\r\n17:  426         77.2260         0.0745        82.6721        0.5258\r\n    eta gamma\r\n 1: 0.1    10\r\n 2: 0.1    50\r\n 3: 0.1    30\r\n 4: 0.1    55\r\n 5: 0.1    70\r\n 6: 0.1    80\r\n 7: 0.1    45\r\n 8: 0.1    15\r\n 9: 0.1    20\r\n10: 0.1    40\r\n11: 0.1    25\r\n12: 0.1    35\r\n13: 0.1    60\r\n14: 0.1     5\r\n15: 0.1    65\r\n16: 0.1     0\r\n17: 0.1    75\r\n\r\n\r\nAs indicated in the results, a gamma of 10 produced the best fit the to test folds. A gamma of 50 was considered as a potential alternative given its lower RMSE SD; however, 50 was considered too extreme of a regularization setting.\r\nA follow-up gamma tuning process was conducted with a narrowed range of 5 to 15 and is reported below.\r\ngrid <- expand.grid(loss_reduction = seq(5, 15, 1))\r\n\r\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\r\n 1:  426         77.1033         0.0673        82.6085        0.5934\r\n 2:  440         76.9364         0.0804        82.6233        0.6727\r\n 3:  411         77.2343         0.0918        82.6301        0.5782\r\n 4:  470         76.7071         0.0412        82.6407        0.4116\r\n 5:  452         76.8494         0.0946        82.6417        0.5207\r\n 6:  497         76.4667         0.0793        82.6423        0.3931\r\n 7:  478         76.6126         0.0703        82.6454        0.3906\r\n 8:  427         77.0770         0.0583        82.6646        0.5045\r\n 9:  389         77.4035         0.1140        82.6658        0.6899\r\n10:  381         77.5321         0.0441        82.6747        0.4971\r\n11:  394         77.3753         0.0697        82.6947        0.5472\r\n    eta gamma\r\n 1: 0.1    12\r\n 2: 0.1    13\r\n 3: 0.1    15\r\n 4: 0.1    14\r\n 5: 0.1    11\r\n 6: 0.1     9\r\n 7: 0.1     6\r\n 8: 0.1     8\r\n 9: 0.1     7\r\n10: 0.1     5\r\n11: 0.1    10\r\n\r\n\r\nAfter second round of tuning, a gamma value of 12 produced the best fit.\r\nBest gamma value = 12\r\nRMSE/SD to beat = 82.6085/0.5934\r\nFollow-up Tree-Parameter Tuning\r\nCreate Grid\r\n\r\n\r\n# Set learning rate, tune tree specific parameters\r\ngrid <- grid_max_entropy(min_n(c(4, 12)), # min_child_weight\r\n                         tree_depth(), # max_depth\r\n                         size = 30)\r\nhead(grid)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  min_n tree_depth\r\n  <int>      <int>\r\n1     8         10\r\n2     7         10\r\n3     7         15\r\n4    10          5\r\n5    12          9\r\n6    10         11\r\n\r\nUpdate Model Specification\r\n\r\n\r\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\r\n  xgb.cv(\r\n    data = train_x,\r\n    label = train_y,\r\n    nrounds = 5000,\r\n    objective = \"reg:squarederror\",\r\n    early_stopping_rounds = 50, \r\n    nfold = 10,\r\n    verbose = 1,\r\n    params = list( \r\n      eta = 0.1,\r\n      gamma = 12, \r\n      min_child_weight = .x,\r\n      max_depth = .y,\r\n      nthread = 24\r\n    ) \r\n  )  \r\n}) \r\n\r\n\r\n\r\nTuning Results\r\n\r\n\r\n{\"x\":{\"visdat\":{\"5dc46d906bfc\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"5dc46d906bfc\",\"attrs\":{\"5dc46d906bfc\":{\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":5},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"min_child_weight\"},\"yaxis\":{\"title\":\"max_depth\"},\"zaxis\":{\"title\":\"test_rmse_mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[15,10,12,2,7,4,7,4,9,14,10,5,12,15,1,3,7,9,13,14,13,7,9,5,6,7,2,2,2,2],\"y\":[7,6,6,5,4,3,12,12,15,17,21,18,23,25,17,19,22,26,34,38,46,28,40,27,35,49,27,33,43,49],\"z\":[82.5138854,82.5260468,82.5867472,82.6038711,82.6386328,82.6866509,82.9198134,83.0705093,83.2934946,83.3909638,84.0744431,84.1086517,84.2383599,84.273793,84.5543496,84.6911759,84.732769,84.98723,85.0051866,85.0241614,85.364856,85.6355408,85.8088257,86.0549699,86.6076196,86.7726464,87.6313399,88.7169029,89.294117,89.3834976],\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2},\"cmin\":82.5138854,\"cmax\":89.3834976,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666674\",\"rgba(70,19,97,1)\"],[\"0.0833333333333326\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5138854,82.5260468,82.5867472,82.6038711,82.6386328,82.6866509,82.9198134,83.0705093,83.2934946,83.3909638,84.0744431,84.1086517,84.2383599,84.273793,84.5543496,84.6911759,84.732769,84.98723,85.0051866,85.0241614,85.364856,85.6355408,85.8088257,86.0549699,86.6076196,86.7726464,87.6313399,88.7169029,89.294117,89.3834976],\"size\":5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":82.5138854,\"cmax\":89.3834976,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666674\",\"rgba(70,19,97,1)\"],[\"0.0833333333333326\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5138854,82.5260468,82.5867472,82.6038711,82.6386328,82.6866509,82.9198134,83.0705093,83.2934946,83.3909638,84.0744431,84.1086517,84.2383599,84.273793,84.5543496,84.6911759,84.732769,84.98723,85.0051866,85.0241614,85.364856,85.6355408,85.8088257,86.0549699,86.6076196,86.7726464,87.6313399,88.7169029,89.294117,89.3834976]}},\"type\":\"scatter3d\",\"mode\":\"markers\",\"frame\":null},{\"x\":[1,15],\"y\":[3,49],\"type\":\"scatter3d\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":82.5138854,\"cmax\":89.3834976,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666674\",\"rgba(70,19,97,1)\"],[\"0.0833333333333326\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[82.5138854,89.3834976],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"z\":[82.5138854,89.3834976],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\r\n 1:  308         77.0365         0.1108        82.5139        0.7964\r\n 2:  515         76.9397         0.0765        82.5260        0.5813\r\n 3:  495         77.1831         0.0865        82.5867        0.6627\r\n 4:  726         77.7906         0.0600        82.6039        0.5953\r\n 5: 1093         79.1339         0.0857        82.6386        0.6009\r\n 6: 2829         79.4049         0.0475        82.6867        0.5113\r\n 7:   79         73.8231         0.2118        82.9198        0.7814\r\n 8:   78         73.2449         0.1283        83.0705        0.5636\r\n 9:   66         71.2054         0.0958        83.2935        0.5143\r\n10:   62         70.7752         0.1706        83.3910        0.8272\r\n11:   59         66.7113         0.2891        84.0744        0.4707\r\n12:   59         67.0320         0.3218        84.1087        0.4410\r\n13:   59         66.1274         0.2683        84.2384        0.7169\r\n14:   58         66.5946         0.2077        84.2738        0.5163\r\n15:   60         65.3824         0.3601        84.5543        0.8896\r\n16:   57         64.7440         0.2680        84.6912        0.7897\r\n17:   57         64.5327         0.2807        84.7328        0.7071\r\n18:   56         63.5798         0.1836        84.9872        0.5934\r\n19:   56         63.7438         0.1620        85.0052        0.7152\r\n20:   55         64.1426         0.1506        85.0242        0.9364\r\n21:   54         63.6017         0.1720        85.3649        0.8097\r\n22:   55         61.3615         0.2887        85.6355        0.6014\r\n23:   54         61.0720         0.0906        85.8088        0.4201\r\n24:   54         60.1531         0.0897        86.0550        0.6940\r\n25:   53         59.2248         0.1364        86.6076        1.0046\r\n26:   53         59.2394         0.1181        86.7726        0.6098\r\n27:   52         56.6917         0.3274        87.6313        0.4278\r\n28:   51         54.1861         0.1396        88.7169        0.5957\r\n29:   50         53.9775         0.0640        89.2941        0.5904\r\n30:   50         53.8405         0.0552        89.3835        0.3106\r\n    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\r\n    max_depth min_child_weight\r\n 1:         7               15\r\n 2:         6               10\r\n 3:         6               12\r\n 4:         5                2\r\n 5:         4                7\r\n 6:         3                4\r\n 7:        12                7\r\n 8:        12                4\r\n 9:        15                9\r\n10:        17               14\r\n11:        21               10\r\n12:        18                5\r\n13:        23               12\r\n14:        25               15\r\n15:        17                1\r\n16:        19                3\r\n17:        22                7\r\n18:        26                9\r\n19:        34               13\r\n20:        38               14\r\n21:        46               13\r\n22:        28                7\r\n23:        40                9\r\n24:        27                5\r\n25:        35                6\r\n26:        49                7\r\n27:        27                2\r\n28:        33                2\r\n29:        43                2\r\n30:        49                2\r\n    max_depth min_child_weight\r\n\r\nTuning Tree Parameters First\r\nUpdate Model Specification\r\n\r\n\r\ntree_mods <- map2(grid$min_n, grid$tree_depth, ~{\r\n  xgb.cv(\r\n    data = train_x,\r\n    label = train_y,\r\n    nrounds = 5000,\r\n    objective = \"reg:squarederror\",\r\n    early_stopping_rounds = 50, \r\n    nfold = 10,\r\n    verbose = 1,\r\n    params = list( \r\n      eta = 0.1,\r\n      gamma = 12,\r\n      min_child_weight = .x,\r\n      max_depth = .y,\r\n      nthread = 24\r\n    ) \r\n  )  \r\n}) \r\n\r\n\r\n\r\nTune Stochastic Parameters\r\n\r\n\r\n\r\nUpdate Model Specification\r\n\r\n\r\n\r\nStochastic Parameter Results\r\n`` #### (Top 10 results shown)\r\n\r\n\r\n{\"x\":{\"visdat\":{\"5dc4362341b3\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"5dc4362341b3\",\"attrs\":{\"5dc4362341b3\":{\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":5},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"colsample_bytree\"},\"yaxis\":{\"title\":\"subsample\"},\"zaxis\":{\"title\":\"test_rmse_mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[0.643243243243243,0.886486486486487,0.356756756756757,0.751351351351351,0.421621621621622,0.756756756756757,0.454054054054054,0.891891891891892,0.805405405405405,0.891891891891892,0.567567567567568,0.691891891891892,0.864864864864865,0.556756756756757,0.421621621621622,0.313513513513514,0.67027027027027,0.508108108108108,0.794594594594595,0.513513513513513,0.762162162162162,0.664864864864865,0.324324324324324,0.886486486486487,0.4,0.67027027027027,0.740540540540541,0.556756756756757,0.827027027027027,0.335135135135135],\"y\":[0.98505666220877,0.91243049201098,0.917512493841064,0.868226930386429,0.942289012458647,0.974153586260294,0.872830294925037,0.851671711128317,0.771908214260576,0.804406278595059,0.956120222425565,0.798514816639685,0.716815654254945,0.852016611529528,0.801266981065672,0.841106496797353,0.690722883085803,0.64112057436475,0.626106848736538,0.73756598859717,0.70436404589287,0.644907439994369,0.729935947068347,0.592320686985289,0.646653058351517,0.52976701625959,0.568508481734356,0.554332371366228,0.514704019145492,0.51115647216161],\"z\":[82.5534562,82.5561036,82.5606972,82.5677865,82.5727196,82.5801781,82.5874306,82.5982963,82.6037588,82.6083703,82.6088395,82.6315796,82.6377098,82.6384231,82.6609697,82.676905,82.689785,82.6956908,82.6972443,82.6985976,82.7052276,82.7057792,82.7076554,82.7604758,82.7659401,82.798848,82.7992514,82.8034859,82.8283028,82.8689894],\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2},\"cmin\":82.5534562,\"cmax\":82.8689894,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5534562,82.5561036,82.5606972,82.5677865,82.5727196,82.5801781,82.5874306,82.5982963,82.6037588,82.6083703,82.6088395,82.6315796,82.6377098,82.6384231,82.6609697,82.676905,82.689785,82.6956908,82.6972443,82.6985976,82.7052276,82.7057792,82.7076554,82.7604758,82.7659401,82.798848,82.7992514,82.8034859,82.8283028,82.8689894],\"size\":5,\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":82.5534562,\"cmax\":82.8689894,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[82.5534562,82.5561036,82.5606972,82.5677865,82.5727196,82.5801781,82.5874306,82.5982963,82.6037588,82.6083703,82.6088395,82.6315796,82.6377098,82.6384231,82.6609697,82.676905,82.689785,82.6956908,82.6972443,82.6985976,82.7052276,82.7057792,82.7076554,82.7604758,82.7659401,82.798848,82.7992514,82.8034859,82.8283028,82.8689894]}},\"type\":\"scatter3d\",\"mode\":\"markers\",\"frame\":null},{\"x\":[0.313513513513514,0.891891891891892],\"y\":[0.51115647216161,0.98505666220877],\"type\":\"scatter3d\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"test_rmse_mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":82.5534562,\"cmax\":82.8689894,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"color\":[82.5534562,82.8689894],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"z\":[82.5534562,82.8689894],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}    iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\r\n 1:  477        77.34670     0.09624726       82.55346     0.5357044\r\n 2:  409        77.59206     0.09019289       82.55610     0.5839695\r\n 3:  511        77.50587     0.07439461       82.56070     0.5872140\r\n 4:  435        77.50257     0.06786992       82.56779     0.6541752\r\n 5:  432        77.93361     0.07199536       82.57272     0.5910682\r\n 6:  451        77.41958     0.07225770       82.58018     0.5463407\r\n 7:  438        77.86621     0.09569565       82.58743     0.4851545\r\n 8:  409        77.62566     0.06778519       82.59830     0.5790606\r\n 9:  395        77.90076     0.09472367       82.60376     0.6282980\r\n10:  354        78.12057     0.07055603       82.60837     0.5912351\r\n    colsample_bytree subsample\r\n 1:        0.6432432 0.9850567\r\n 2:        0.8864865 0.9124305\r\n 3:        0.3567568 0.9175125\r\n 4:        0.7513514 0.8682269\r\n 5:        0.4216216 0.9422890\r\n 6:        0.7567568 0.9741536\r\n 7:        0.4540541 0.8728303\r\n 8:        0.8918919 0.8516717\r\n 9:        0.8054054 0.7719082\r\n10:        0.8918919 0.8044063\r\n\r\nRetune Learning Rate\r\n\r\n\r\n\r\nFinal Fit Statistics\r\n\r\n\r\n\r\n",
    "preview": "posts/11-29-20_Chris/ML_models_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-07T16:53:51-08:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/11-29-20_data_prep_&_description_AY/",
    "title": "Data preparation and exploration",
    "description": "Here we joined our datasets and explored through visualization.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1. Packages\r\n2. Joining the datasets\r\n3.Data Exploration\r\n1. Packages\r\nFor the purpose of data loading and cleaning, we are using following packages in R: {tidyverse}, {here}, {rio}, and {skimr}\r\n2. Joining the datasets\r\nFor the purpose of demonstration, we will be using 1% of the data with sample_frac() to keep computing time low. All our datasets have school ids which we used as key to join the datasets.\r\nAfter loading our three datasets, we joined them together to make one cohesive dataset, to be used for ML modeling. After joining, the dataset contains student-level variables (e.g. gender, ethnicity, enrollement in special education, etc.) as well as district-level variables ( school longitude and latitude, proportion of free and reduced lunch, etc.). All of these variables will be used in our ML models to predict student score in the statewide assessment. Here is the preview of our final dataset, ready to be used for ML modeling.\r\n\r\nTable 1: Data summary\r\nName\r\ndata\r\nNumber of rows\r\n1894\r\nNumber of columns\r\n88\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n5\r\nfactor\r\n28\r\nlogical\r\n1\r\nnumeric\r\n54\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ncounty\r\n28\r\n0.99\r\n11\r\n17\r\n0\r\n34\r\n0\r\nlocale\r\n28\r\n0.99\r\n14\r\n19\r\n0\r\n12\r\n0\r\ntitle1_status\r\n28\r\n0.99\r\n36\r\n47\r\n0\r\n3\r\n0\r\nlea_name\r\n30\r\n0.98\r\n20\r\n43\r\n0\r\n145\r\n0\r\nsch_name\r\n28\r\n0.99\r\n8\r\n60\r\n0\r\n695\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\ngndr\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 967, F: 927\r\nethnic_cd\r\n0\r\n1.00\r\nFALSE\r\n7\r\nW: 1183, H: 441, M: 103, A: 79\r\ntst_bnch\r\n0\r\n1.00\r\nFALSE\r\n6\r\n2B: 347, G4: 319, G6: 316, 3B: 307\r\ntst_dt\r\n0\r\n1.00\r\nFALSE\r\n52\r\n5/2: 160, 5/2: 138, 5/1: 125, 5/2: 114\r\nmigrant_ed_fg\r\n0\r\n1.00\r\nFALSE\r\n2\r\nN: 1848, Y: 46\r\nind_ed_fg\r\n1\r\n1.00\r\nFALSE\r\n2\r\nN: 1872, Y: 21, y: 0\r\nsp_ed_fg\r\n1\r\n1.00\r\nFALSE\r\n2\r\nN: 1652, Y: 241\r\ntag_ed_fg\r\n6\r\n1.00\r\nFALSE\r\n2\r\nN: 1791, Y: 97\r\necon_dsvntg\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1094, N: 794\r\nayp_lep\r\n1534\r\n0.19\r\nFALSE\r\n7\r\nF: 165, Y: 69, E: 64, N: 29\r\nstay_in_dist\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1845, N: 43\r\nstay_in_schl\r\n6\r\n1.00\r\nFALSE\r\n2\r\nY: 1830, N: 58\r\ndist_sped\r\n6\r\n1.00\r\nFALSE\r\n2\r\nN: 1867, Y: 21\r\ntrgt_assist_fg\r\n6\r\n1.00\r\nFALSE\r\n3\r\nN: 1812, Y: 74, y: 2\r\nayp_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\nayp_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\nayp_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1842, N: 52\r\nayp_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1808, N: 86\r\nrc_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\nrc_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\nrc_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1842, N: 52\r\nrc_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1808, N: 86\r\nlang_cd\r\n1842\r\n0.03\r\nFALSE\r\n1\r\nS: 52\r\ntst_atmpt_fg\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1886, P: 8\r\ngrp_rpt_dist_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1888, N: 6\r\ngrp_rpt_schl_partic\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1867, N: 27\r\ngrp_rpt_dist_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1882, N: 12\r\ngrp_rpt_schl_prfrm\r\n0\r\n1.00\r\nFALSE\r\n2\r\nY: 1861, N: 33\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\ncalc_admn_cd\r\n1894\r\n0\r\nNaN\r\n:\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nid\r\n0\r\n1.00\r\n1.273822e+05\r\n73162.85\r\n▇▇▇▇▇\r\nattnd_dist_inst_id\r\n0\r\n1.00\r\n2.127740e+03\r\n212.96\r\n▇▁▁▁▁\r\nattnd_schl_inst_id\r\n0\r\n1.00\r\n1.400890e+03\r\n1393.20\r\n▇▃▁▁▁\r\nenrl_grd\r\n0\r\n1.00\r\n5.490000e+00\r\n1.68\r\n▇▅▅▃▃\r\npartic_dist_inst_id\r\n6\r\n1.00\r\n2.131510e+03\r\n225.53\r\n▇▁▁▁▁\r\npartic_schl_inst_id\r\n6\r\n1.00\r\n1.403160e+03\r\n1394.28\r\n▇▃▁▁▁\r\nscore\r\n0\r\n1.00\r\n2.500430e+03\r\n119.82\r\n▁▁▇▃▁\r\nncessch\r\n26\r\n0.99\r\n4.106893e+11\r\n395453731.73\r\n▆▅▅▇▃\r\nlat\r\n30\r\n0.98\r\n4.475000e+01\r\n1.04\r\n▂▁▂▅▇\r\nlon\r\n30\r\n0.98\r\n-1.225200e+02\r\n1.17\r\n▅▇▁▁▁\r\nncesag\r\n28\r\n0.99\r\n4.106891e+06\r\n3956.50\r\n▆▅▅▇▃\r\nzip\r\n28\r\n0.99\r\n9.731347e+04\r\n232.97\r\n▇▇▆▁▂\r\ntotal_n\r\n28\r\n0.99\r\n5.721300e+02\r\n357.46\r\n▇▁▁▁▁\r\nfr_lnch_n\r\n75\r\n0.96\r\n2.310900e+02\r\n148.02\r\n▇▇▃▁▁\r\nred_lnch_n\r\n75\r\n0.96\r\n4.091000e+01\r\n25.11\r\n▆▇▃▂▁\r\npupil_tch_ratio\r\n28\r\n0.99\r\n2.026000e+01\r\n3.60\r\n▇▇▁▁▁\r\nipr_est\r\n28\r\n0.99\r\n3.220900e+02\r\n155.65\r\n▇▆▂▁▁\r\nleaid\r\n28\r\n0.99\r\n4.106891e+06\r\n3956.50\r\n▆▅▅▇▃\r\nst_schid\r\n28\r\n0.99\r\n1.388760e+03\r\n1387.95\r\n▇▃▁▁▁\r\nlea_cwiftest\r\n30\r\n0.98\r\n9.400000e-01\r\n0.07\r\n▁▂▇▂▆\r\np_american_indian_alaska_native\r\n28\r\n0.99\r\n1.000000e-02\r\n0.04\r\n▇▁▁▁▁\r\np_asian\r\n28\r\n0.99\r\n4.000000e-02\r\n0.07\r\n▇▁▁▁▁\r\np_native_hawaiian_pacific_islander\r\n28\r\n0.99\r\n1.000000e-02\r\n0.01\r\n▇▁▁▁▁\r\np_black_african_american\r\n28\r\n0.99\r\n2.000000e-02\r\n0.04\r\n▇▁▁▁▁\r\np_hispanic_latino\r\n28\r\n0.99\r\n2.500000e-01\r\n0.19\r\n▇▅▂▁▁\r\np_white\r\n28\r\n0.99\r\n6.100000e-01\r\n0.20\r\n▁▃▆▇▇\r\np_multiracial\r\n28\r\n0.99\r\n7.000000e-02\r\n0.03\r\n▇▆▁▁▁\r\nfr_lnch_prop\r\n75\r\n0.96\r\n4.400000e-01\r\n0.21\r\n▅▇▇▆▂\r\nred_lnch_prop\r\n75\r\n0.96\r\n8.000000e-02\r\n0.03\r\n▃▇▃▁▁\r\npercent_level_4\r\n31\r\n0.98\r\n2.211000e+01\r\n13.36\r\n▇▇▃▁▁\r\npercent_level_3\r\n31\r\n0.98\r\n3.123000e+01\r\n9.00\r\n▂▆▇▁▁\r\npercent_level_2\r\n31\r\n0.98\r\n2.268000e+01\r\n7.09\r\n▁▆▇▁▁\r\npercent_level_1\r\n31\r\n0.98\r\n2.399000e+01\r\n13.48\r\n▅▇▃▁▁\r\nsch_percent_level_4\r\n28\r\n0.99\r\n2.243000e+01\r\n12.46\r\n▇▇▃▁▁\r\nsch_percent_level_3\r\n28\r\n0.99\r\n3.118000e+01\r\n6.83\r\n▁▂▇▇▂\r\nsch_percent_level_2\r\n28\r\n0.99\r\n2.259000e+01\r\n5.31\r\n▁▃▇▇▁\r\nsch_percent_level_1\r\n28\r\n0.99\r\n2.380000e+01\r\n12.05\r\n▅▇▃▁▁\r\nunder_25\r\n44\r\n0.98\r\n2.100000e-01\r\n0.06\r\n▃▇▅▁▁\r\nunder_50\r\n44\r\n0.98\r\n2.300000e-01\r\n0.06\r\n▂▆▇▆▂\r\nunder_75\r\n44\r\n0.98\r\n1.600000e-01\r\n0.03\r\n▂▇▁▁▁\r\nunder_100\r\n44\r\n0.98\r\n1.200000e-01\r\n0.02\r\n▁▁▇▃▁\r\nunder_200\r\n44\r\n0.98\r\n2.000000e-01\r\n0.07\r\n▁▆▆▇▁\r\nover_200\r\n44\r\n0.98\r\n7.000000e-02\r\n0.08\r\n▇▂▁▁▁\r\npercent_less_than_9th_grade\r\n40\r\n0.98\r\n4.040000e+00\r\n3.37\r\n▇▅▁▁▁\r\npercent_high_school_graduate_or_higher\r\n40\r\n0.98\r\n8.972000e+01\r\n5.79\r\n▁▁▂▇▅\r\npercent_bachelors_degree_or_higher\r\n40\r\n0.98\r\n3.122000e+01\r\n16.15\r\n▃▇▃▂▁\r\npercent_associates_degree\r\n40\r\n0.98\r\n8.780000e+00\r\n1.99\r\n▂▇▇▁▁\r\npercent_graduate_or_professional_degree\r\n40\r\n0.98\r\n1.171000e+01\r\n7.87\r\n▇▆▃▁▁\r\npercent_bachelors_degree\r\n40\r\n0.98\r\n1.815000e+01\r\n8.52\r\n▂▇▃▃▂\r\npercent_high_school_graduate\r\n40\r\n0.98\r\n2.511000e+01\r\n7.67\r\n▂▃▇▂▁\r\npercent_9th_to_12th_grade_no_diploma\r\n40\r\n0.98\r\n6.900000e+00\r\n3.42\r\n▇▂▁▁▁\r\npercent_some_college\r\n40\r\n0.98\r\n2.676000e+01\r\n4.37\r\n▁▃▇▇▁\r\nmedian_income\r\n28\r\n0.99\r\n2.812101e+04\r\n4487.40\r\n▂▇▂▃▆\r\nmedian_rent\r\n28\r\n0.99\r\n9.835000e+02\r\n153.33\r\n▁▂▇▂▇\r\n\r\n3.Data Exploration\r\nCorrelation: Figure 1 displays significant (p < 0.05) correlation between numeric variables. Red dots show significant pearson’s correlation coefficient between 0 to 1 and white dots show significant coefficient between -1 to 0. Blank spaces are not significant.\r\n\r\n\r\n\r\nWe looked if pupil-teacher ratio and score differ by county.\r\n\r\n\r\n\r\nWe explored adult qualification by economic disadvantage and grade.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T16:53:52-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-29-20_ML_models_AY/",
    "title": "Linear Regression and Penalized Regression Model (Lasso)",
    "description": "Our aim is to predict the score on statewide testing assessment using other variables in the dataset. Let's see how linear and lasso regression perform.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-28",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1.Splitting and resampling1a. Splitting\r\n1b. Resampling\r\n\r\n2.Pre-processing\r\n3. Linear model\r\n1.Splitting and resampling\r\nData splitting and resampling is a method to ensure that predictive performance of ML model is unbiased.\r\n1a. Splitting\r\nWe will split our dataset into training and testing set. The training data will be used to fit our model and tune its parameters, whereas the testing data will be used to evaluate our final model’s performance.\r\nWe will use initial_split() function from the rsample package which creates a split object. The split object d_split, tells how many observations we have in our training and testing set (trainin set = 75% & testing set = 25%).\r\n\r\n\r\nset.seed(3000)\r\n# Create split object specifying (75%) and testing (25%)\r\n\r\ndata_split <- initial_split(data, prop = 3/4) \r\n\r\ndata_split\r\n\r\n<Analysis/Assess/Total>\r\n<1421/473/1894>\r\n\r\nWe will extract the training and testing set from the split object, d_split by using the training() and testing() functions.\r\n\r\n\r\n# Extract training and testing set\r\nset.seed(3000)\r\n\r\ndata_train <- training(data_split)  # Our training dataset has 1421 observations.\r\n\r\ndata_test <- testing(data_split)    # Our test dataset has 473 observations.\r\n\r\n1b. Resampling\r\nAt some point, ML models require parameters tuning (adjustment). In order to prepare for tuning, we resample our data. Resampling reduces bias from over-fitting the data. There are several methods to resample the data and the two most effective and frequently used are 10-fold cross validation and bootstrapping. In our project, we’ll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = number of time we resample.\r\n\r\n\r\n# Resample the data with 10-fold cross validation.\r\nset.seed(3000)\r\ncv <- vfold_cv(data_train)\r\n\r\n2.Pre-processing\r\nPreprocessing is a way of converting data from the raw form to a more usable form for the ML modeling purpose. The pre-processing involves defining roles of variables using role-specifying functions such as all_numeric(), all_predictors() as arguments to pre-processing steps. Pre-processing also involves steps such as imputation (step_medianimpute), rescaling (step_scale), standardizing (step_normalize), PCA (step_pca) and creating dummy variables (step_dummy). A full list of pre-processing can be found here.\r\nWe will use recipe() function to indicate our outcome and predictor variables in our recipe. We will use ~. to indicate that we are using all variables to predict the outcome variable score. All recipe takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific dataset later. Therefore, when we print summary of object, rec, it shows numbers of predictor variables have been specified. It doen’t actually apply the predictors yet. We will use same receipe throughout this post.\r\n\r\n\r\nset.seed(3000)\r\n\r\nrec <- recipe(score ~ ., data_train) %>% \r\n  step_mutate(tst_dt = as.numeric(lubridate::\r\n                                    mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \r\n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\r\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\r\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \r\n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\r\n  step_medianimpute(all_numeric(), -all_outcomes(), \r\n                    -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\r\n  step_dummy(all_nominal(), -has_role(\"id vars\")) %>% # dummy codes categorical variables\r\n  step_nzv(all_predictors(), -starts_with(\"lang_cd\"))\r\n\r\nExtract the pre-processed dataset\r\nTo extract the pre-processed dataset, we can prep() the recipe for our datset then bake() the prepped recipe to extract the pre-processed data.\r\nHowever, in tidymodels we can use workflows() where we don’t need to prep() or bake() the recipe.\r\n\r\n\r\nprep(rec) #%>%\r\n\r\nData Recipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   id vars          8\r\n   outcome          1\r\n predictor         79\r\n\r\nTraining data contained 1421 data points and 1421 incomplete rows. \r\n\r\nOperations:\r\n\r\nVariable mutation for tst_dt [trained]\r\nSparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]\r\nNovel factor level assignment for gndr, ethnic_cd, ... [trained]\r\nUnknown factor level assignment for gndr, ethnic_cd, ... [trained]\r\nMedian Imputation for enrl_grd, tst_dt, lat, lon, ... [trained]\r\nDummy variables from gndr, ethnic_cd, tst_bnch, ... [trained]\r\nSparse, unbalanced variable filter removed gndr_new, ... [trained]\r\n\r\n # bake(data_train)   # We are using workflow so no need to bake.\r\n\r\nThe next step is to specify our ML models, using the parsnip package. To specify the model, there are four primary components: model type, arguments, engine, and mode.\r\n3. Linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\n# Specify the model\r\n\r\nmod_linear <- linear_reg() %>%\r\n  set_engine(\"lm\") %>%  # engine for linear regression\r\n  set_mode(\"regression\")  # regression for continous outcome varaible.\r\n\r\n# Workflow\r\nlm_wf <- workflow() %>% # set the workflow\r\n  add_recipe(rec) %>% # add recipe\r\n  add_model(mod_linear) # add model\r\n  \r\n\r\n# Fit the linear model\r\nmod_linear_fit<- fit_resamples(\r\n  mod_linear,\r\n  preprocessor = rec,\r\n  resamples = cv,\r\n  metrics = metric_set(rmse),\r\n  control = control_resamples(verbose = TRUE,\r\n                                    save_pred = TRUE))\r\n\r\nCollect metrics on linear model\r\n\r\n\r\nset.seed(3000)\r\n\r\nmod_linear_fit %>%\r\n  collect_metrics() %>%\r\n  filter(.metric == \"rmse\") \r\n\r\n# A tibble: 1 x 5\r\n  .metric .estimator  mean     n std_err\r\n  <chr>   <chr>      <dbl> <int>   <dbl>\r\n1 rmse    standard    96.3    10    2.49\r\n\r\n#RMSE = 96.28\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T16:53:52-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_datasets_AY/",
    "title": "Datasets",
    "description": "Here is the preview of datasets used in our final project.",
    "author": [
      {
        "name": "Asha",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\r\nTable of Contents\r\n1. Introduction\r\n2. Statewide testing data (Original dataset)\r\n3. Fall membership report data\r\n4. Bonus Dataset\r\n5. Supplemental small dataset\r\n1. Introduction\r\nFor this project, we collected and joined three structured datasets and one supplemental small dataset only for XG boost model. Datasets are selected on the basis of variables that were correlated with the outcome variable. In our datasets, our outcome variable is children’s score on the statewide assessment. Please see below for more information on each dataset.\r\n2. Statewide testing data (Original dataset)\r\nStudents in every state across the nation are tested annually in reading and math in grades 3-8. Dataset used in the project are simulated from an actual statewide testing administration across the state of Oregon and the overall distribution are highly similar. Our continous outcome variable is score which is also presented in categorical form as classification. For the project, our models are run to predict the continous score variable. Our data contains other variables of interest (predictors) such as gender, ethnicity, economic disadvange, and location. In our simulated dataset, school ids are real which we used as key to join other datasets. Here is the preview of our statewide testing dataset.\r\n\r\nTable 1: Data summary\r\nName\r\nd\r\nNumber of rows\r\n1894\r\nNumber of columns\r\n39\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n28\r\nlogical\r\n1\r\nnumeric\r\n10\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ngndr\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nethnic_cd\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n7\r\n0\r\ntst_bnch\r\n0\r\n1.00\r\n2\r\n2\r\n0\r\n6\r\n0\r\ntst_dt\r\n0\r\n1.00\r\n16\r\n17\r\n0\r\n48\r\n0\r\nmigrant_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nind_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nsp_ed_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ntag_ed_fg\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\necon_dsvntg\r\n4\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_lep\r\n1540\r\n0.19\r\n1\r\n1\r\n0\r\n8\r\n0\r\nstay_in_dist\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nstay_in_schl\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ndist_sped\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ntrgt_assist_fg\r\n3\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nayp_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nrc_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nlang_cd\r\n1851\r\n0.02\r\n1\r\n1\r\n0\r\n1\r\n0\r\ntst_atmpt_fg\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_dist_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_schl_partic\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_dist_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\ngrp_rpt_schl_prfrm\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n2\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\ncalc_admn_cd\r\n1894\r\n0\r\nNaN\r\n:\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nid\r\n0\r\n1.00\r\n1.267936e+05\r\n72111.86\r\n▇▇▇▇▇\r\nattnd_dist_inst_id\r\n0\r\n1.00\r\n2.116080e+03\r\n188.53\r\n▇▁▁▁▁\r\nattnd_schl_inst_id\r\n0\r\n1.00\r\n1.345800e+03\r\n1388.28\r\n▇▃▁▁▁\r\nenrl_grd\r\n0\r\n1.00\r\n5.440000e+00\r\n1.67\r\n▇▅▃▃▃\r\npartic_dist_inst_id\r\n4\r\n1.00\r\n2.116160e+03\r\n188.71\r\n▇▁▁▁▁\r\npartic_schl_inst_id\r\n4\r\n1.00\r\n1.345210e+03\r\n1386.64\r\n▇▃▁▁▁\r\nscore\r\n0\r\n1.00\r\n2.498910e+03\r\n113.99\r\n▁▂▇▅▁\r\nncessch\r\n29\r\n0.98\r\n4.106795e+11\r\n385634091.96\r\n▆▅▅▇▃\r\nlat\r\n31\r\n0.98\r\n4.476000e+01\r\n1.00\r\n▂▁▃▅▇\r\nlon\r\n31\r\n0.98\r\n-1.224600e+02\r\n1.28\r\n▆▇▁▁▁\r\n\r\n3. Fall membership report data\r\nThe Oregon Department for Education (ODE) publicly releases student enrollment reports detailing the number of K-12 students who are enrolled on the first school day in October of each year.This report is known as Fall membership report which contains data on race/ethinicity percentages for schools in Oregon. Here is the preview of our data.\r\n\r\nTable 2: Data summary\r\nName\r\nethnicities\r\nNumber of rows\r\n1459\r\nNumber of columns\r\n9\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nnumeric\r\n8\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nsch_name\r\n0\r\n1\r\n8\r\n60\r\n0\r\n1381\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nattnd_schl_inst_id\r\n0\r\n1\r\n1661.26\r\n1558.62\r\n▇▃▁▁▂\r\np_american_indian_alaska_native\r\n0\r\n1\r\n0.02\r\n0.05\r\n▇▁▁▁▁\r\np_asian\r\n0\r\n1\r\n0.03\r\n0.05\r\n▇▁▁▁▁\r\np_native_hawaiian_pacific_islander\r\n0\r\n1\r\n0.01\r\n0.01\r\n▇▁▁▁▁\r\np_black_african_american\r\n0\r\n1\r\n0.02\r\n0.05\r\n▇▁▁▁▁\r\np_hispanic_latino\r\n0\r\n1\r\n0.21\r\n0.18\r\n▇▃▁▁▁\r\np_white\r\n0\r\n1\r\n0.65\r\n0.22\r\n▁▂▃▇▅\r\np_multiracial\r\n0\r\n1\r\n0.06\r\n0.05\r\n▇▁▁▁▁\r\n\r\n4. Bonus Dataset\r\nWe retrieved another dataset that contains k-12 data collected by zip code, NCES school IDs, State school IDs, and county level. It contains variables that are not present in the statewide testing dataset such as teacher-pupil ratio, percentage of people with high school, no diploma, and higher education. These variables may have effect on our outcome variable (score). Please see below preview other variables present in the dataset.\r\n\r\nTable 3: Data summary\r\nName\r\nbonus_data\r\nNumber of rows\r\n3889\r\nNumber of columns\r\n51\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n7\r\nnumeric\r\n44\r\n________________________\r\n\r\nGroup variables\r\n\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nncessch\r\n0\r\n1.00\r\n18\r\n21\r\n0\r\n1257\r\n0\r\ncounty\r\n0\r\n1.00\r\n11\r\n17\r\n0\r\n36\r\n0\r\nlocale\r\n0\r\n1.00\r\n14\r\n19\r\n0\r\n12\r\n0\r\ntitle1_status\r\n0\r\n1.00\r\n3\r\n47\r\n0\r\n4\r\n0\r\npupil_tch_ratio\r\n0\r\n1.00\r\n1\r\n6\r\n0\r\n789\r\n0\r\nlea_name\r\n54\r\n0.99\r\n20\r\n43\r\n0\r\n195\r\n0\r\nsch_name\r\n12\r\n1.00\r\n8\r\n60\r\n0\r\n1203\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nhist\r\nncesag\r\n0\r\n1.00\r\n4106916.49\r\n4236.76\r\n▇▁▁▁▁\r\nzip\r\n0\r\n1.00\r\n97366.87\r\n258.54\r\n▇▇▇▂▅\r\ntotal_n\r\n1\r\n1.00\r\n406.93\r\n318.97\r\n▇▁▁▁▁\r\nfr_lnch_n\r\n428\r\n0.89\r\n180.93\r\n138.74\r\n▇▁▁▁▁\r\nred_lnch_n\r\n428\r\n0.89\r\n31.81\r\n25.57\r\n▇▁▁▁▁\r\nipr_est\r\n12\r\n1.00\r\n303.52\r\n135.95\r\n▇▅▁▁▁\r\nleaid\r\n12\r\n1.00\r\n4106915.58\r\n4240.21\r\n▇▁▁▁▁\r\nst_schid\r\n12\r\n1.00\r\n1610.13\r\n1592.21\r\n▇▂▁▂▂\r\nlea_cwiftest\r\n54\r\n0.99\r\n0.92\r\n0.07\r\n▁▂▇▂▅\r\np_american_indian_alaska_native\r\n12\r\n1.00\r\n0.01\r\n0.05\r\n▇▁▁▁▁\r\np_asian\r\n12\r\n1.00\r\n0.03\r\n0.06\r\n▇▁▁▁▁\r\np_native_hawaiian_pacific_islander\r\n12\r\n1.00\r\n0.01\r\n0.02\r\n▇▁▁▁▁\r\np_black_african_american\r\n12\r\n1.00\r\n0.02\r\n0.05\r\n▇▁▁▁▁\r\np_hispanic_latino\r\n12\r\n1.00\r\n0.20\r\n0.18\r\n▇▂▁▁▁\r\np_white\r\n12\r\n1.00\r\n0.66\r\n0.21\r\n▁▂▃▇▆\r\np_multiracial\r\n12\r\n1.00\r\n0.06\r\n0.04\r\n▇▁▁▁▁\r\nfr_lnch_prop\r\n428\r\n0.89\r\n0.45\r\n0.21\r\n▅▇▇▆▂\r\nred_lnch_prop\r\n428\r\n0.89\r\n0.08\r\n0.04\r\n▇▇▁▁▁\r\nenrl_grd\r\n236\r\n0.94\r\n5.13\r\n1.66\r\n▇▃▃▂▂\r\npercent_level_4\r\n556\r\n0.86\r\n22.06\r\n14.25\r\n▇▇▃▁▁\r\npercent_level_3\r\n556\r\n0.86\r\n30.18\r\n10.59\r\n▁▇▇▁▁\r\npercent_level_2\r\n556\r\n0.86\r\n22.93\r\n8.54\r\n▂▇▂▁▁\r\npercent_level_1\r\n556\r\n0.86\r\n24.83\r\n14.81\r\n▆▇▃▁▁\r\nsch_percent_level_4\r\n152\r\n0.96\r\n22.89\r\n13.04\r\n▆▇▃▁▁\r\nsch_percent_level_3\r\n152\r\n0.96\r\n30.46\r\n7.65\r\n▁▂▇▃▁\r\nsch_percent_level_2\r\n152\r\n0.96\r\n22.74\r\n6.80\r\n▁▇▂▁▁\r\nsch_percent_level_1\r\n152\r\n0.96\r\n23.91\r\n13.16\r\n▇▇▃▁▁\r\nunder_25\r\n180\r\n0.95\r\n0.22\r\n0.07\r\n▃▇▅▁▁\r\nunder_50\r\n180\r\n0.95\r\n0.24\r\n0.06\r\n▂▇▆▁▁\r\nunder_75\r\n180\r\n0.95\r\n0.17\r\n0.04\r\n▁▅▇▁▁\r\nunder_100\r\n180\r\n0.95\r\n0.12\r\n0.03\r\n▁▂▇▁▁\r\nunder_200\r\n180\r\n0.95\r\n0.19\r\n0.08\r\n▂▇▇▇▁\r\nover_200\r\n180\r\n0.95\r\n0.06\r\n0.07\r\n▇▂▁▁▁\r\npercent_less_than_9th_grade\r\n39\r\n0.99\r\n3.76\r\n3.41\r\n▇▁▁▁▁\r\npercent_high_school_graduate_or_higher\r\n39\r\n0.99\r\n89.72\r\n5.83\r\n▁▁▂▇▇\r\npercent_bachelors_degree_or_higher\r\n39\r\n0.99\r\n28.77\r\n15.64\r\n▂▇▂▂▁\r\npercent_associates_degree\r\n39\r\n0.99\r\n8.88\r\n2.33\r\n▁▃▇▂▁\r\npercent_graduate_or_professional_degree\r\n39\r\n0.99\r\n10.71\r\n7.63\r\n▇▅▂▁▁\r\npercent_bachelors_degree\r\n39\r\n0.99\r\n17.11\r\n8.56\r\n▇▅▁▁▁\r\npercent_high_school_graduate\r\n39\r\n0.99\r\n26.60\r\n8.26\r\n▁▃▇▅▁\r\npercent_9th_to_12th_grade_no_diploma\r\n39\r\n0.99\r\n7.04\r\n3.63\r\n▇▅▁▁▁\r\npercent_some_college\r\n39\r\n0.99\r\n27.29\r\n5.14\r\n▁▁▇▅▁\r\nmedian_income\r\n0\r\n1.00\r\n27158.20\r\n4434.27\r\n▃▇▂▃▃\r\nmedian_rent\r\n0\r\n1.00\r\n942.33\r\n166.09\r\n▂▃▇▂▇\r\n\r\n5. Supplemental small dataset\r\nSupplemental small dataset contains high school dropout rates and out-of-school suspension rates by state districts IDs. This dataset has been used only in XG boost model. For details, please check the post\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T16:53:51-08:00",
    "input_file": {}
  },
  {
    "path": "posts/11-28-20_welcome_AY/",
    "title": "Hello!",
    "description": "Welcome to our Machine Learning blog",
    "author": [
      {
        "name": "AY, AG, and CI",
        "url": {}
      }
    ],
    "date": "2020-11-27",
    "categories": [],
    "contents": "\r\nThis blog was created as part of the final project for the EDLD 654 Applied Machine Learning for Educational Data Science, taught by Prof. Daniel Anderson and Prof. Joseph Nese at the University of Oregon. The blog contains application of Machine Learning models to educational data for predictive analysis. Following models were presented in the final project:\r\nLinear regression (add link to my post, if posting separate.)\r\nRandom Forest (add link Ale’s post if posting separate)\r\nGradient Boosting (add link Chris’s post, if posting separete)\r\nThe contributors of the blog are:\r\nAsha Yadav\r\nAlejandra\r\nChris Ives\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-04T17:55:44-08:00",
    "input_file": {}
  }
]
