---
title: "Random Forest Model"
description: |
    This is how fitting a Random Forest model went for me! 
author:
  - name: Alejandra Garcia Isaza
    url:
    affiliation: University of Oregon
    affiliation_url:
date: 11-28-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(rio)
library(here)
library(tidyverse)
library(tidymodels)

options(scipen=999)

theme_set(theme_minimal(base_size = 10))
```

To complete this project, I chose to run a Random Forest model. I decided to go with the Out-of-bag (OOB) samples to conduct model fitting and model assessment due to its relatively low run time.

Initially I was only working locally on my computer with only 5% of the sample. Overall, my model tunning process was taking between 15 and 25 minutes each round of tunning, but unfortunately my R session froze several times and I could not finish the model tuning process. 

At this point, Daniel suggested I use HPC via the Open On Demand interface. As always, I was too afraid to try something new, but Daniel patiently showed me how to work with the new interface and explained for the 97th time what was the difference between the train and test files we have been working with all term long. I think I finally got it! I'm glad it only took me ten weeks ðŸ˜¬ to understand this. 

I followed the same steps we have been following all term long, reading in the data, sampling a fraction of the data (note that I used 50% of the data because I was working with HPC), and joining with other datasets. Here I joined a dataset we called "bonus" that inludes variables from different datasets. To learn more about this dataset and its variables, please go to <<<< link to page >>>>>.

A huge shout-out to [Chris Ives](https://github.com/cives93) for taking the time to find and process the bonus datasets we are using here. 

```{r eval=FALSE, echo=TRUE}
set.seed(3000)

full_train <- import(here("data", "train.csv"), setclass = "tbl_df") %>%
  select(-classification) %>%
  mutate_if(is.character, factor) %>%
  mutate(ncessch = as.double(ncessch)) %>%
  sample_frac(0.5)

bonus <- import(here("data", "bonus_data.csv")) %>%
  mutate(pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>%
  mutate(ncessch = as.double(ncessch))

## joining data
data <- left_join(full_train, bonus)
```

Here I created the initial split and the train and test objects for model fitting and assessment. Notice that I did not create a cross-validation resample because I will be using only the OOB samples. 

```{r eval=FALSE, echo=TRUE}
set.seed(3000)
data_split <- initial_split(data)

set.seed(3000)
data_train <- training(data_split)
data_test <- testing(data_split)
```

I created a recipe following the recommended preprocessing steps for a Random Forest model. I found [this guide](https://www.tmwr.org/pre-proc-table.html) that Joe shared with our group very useful. 

This recipe is pretty straightforward, the only thing that I think is worth highlighting is that we took the liberty to assume that most of the students' home language was English; that is why we specified this code `lang_cd = case_when(lang_cd == "S" ~ "S", TRUE ~ "E")`. I took extra care not to lose the Spanish speaking students in the `lang_cd` variable to the `step_nzv()` near-zero variance variable removal due to the role of language in academic achievement. 

In this model, we had one outcome `score`, 77 predictors and XX Id variables. 

```{r eval=FALSE, echo=TRUE}
rec <- recipe(score ~ ., data_train) %>%
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt),
              lang_cd = case_when(lang_cd == "S" ~ "S", TRUE ~ "E"),
              pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>% 
  update_role(contains("id"), ncessch, ncesag, sch_name, new_role = "id_vars") %>%
  step_zv(all_predictors(), -starts_with("lang_cd")) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id_vars")) %>%
  step_novel(all_nominal()) %>%
  step_unknown(all_nominal()) %>% 
  step_dummy(all_nominal()) %>%
  step_nzv(all_predictors(), -starts_with("lang_cd"))
```

# Let's dive in on the model fitting process!

First, I wanted to develop a baseline model using the default hypermarameters. I focused only on three hyperparameters: predictors `mtry` = `floor(p/3)`, in this model, around 26 predictors; number of trees`trees` = 500, and minimun node size `min_n` = 5. 

The rmse for the default model was 88.80 *Fitting the default model took less than 3 minutes!!*

```{r eval=FALSE, echo=TRUE}

# default model
rf_def <- rand_forest() %>%
  set_engine("ranger",
             num.threads = 8,
             importance = "permutation",
             verbose = TRUE) %>%
  set_mode("regression")

# workflow for default model
rf_def_wkflw <- workflow() %>% 
  add_model(rf_def) %>% 
  add_recipe(rec)

# fitting the default model
rf_def_fit <- fit(
  rf_def_wkflw,
  data_train)

```

# First round of tuning

Random forest models are know to have good out-of-the-box performance, however, I wanted to tune at least 2 hyperparameters to evaluate how much lower the rmse could be. 

I decided not to spend time tuning for number number of trees as the literature suggests that growing p * 10 trees is pretty safe. The number of trees needs to be large enough to stabilize the error rate, for this model I could have used 770 trees, but I went for 1000 trees just to be extra safe.

To tune for `mtry` and `min_n`, I followed what Boehmke &  Greenwell (2020) suggest in the chapter focused on random forest models in the book [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/random-forest.html)

###### "Start with five evenly spaced values of mtry across the range 2 â€“ p centered at the recommended default" (Boehmke &  Greenwell, 2020)

###### "When adjusting node size start with three values between 1â€“10 and adjust depending on impact to accuracy and run time."

```{r eval=TRUE, echo=FALSE}
mtry_search <- seq(2, 50, 12)
min_n_search <- seq(1, 10, 3)

grd <- expand.grid(mtry_search, min_n_search)

DT::datatable(grd, colnames = c('mtry' = 'Var1', 'min_n' = 'Var2')) 
```


Below is a function that Daniel helped me build. This is an all-in-one function that builds the model, updates the default model workflow with the new hyperparameters, fits the models with the selected hyperparameters, and outputs a table with the tuned hyperparaemters and the corresponding rmse value. It's pretty neat!

```{r eval=FALSE, echo=TRUE}

hyp_rf_search <- function(mtry_val, min_n_val, wf) {
  mod <- rand_forest() %>% 
    set_engine("ranger",
               num.threads = 8,
               importance = "permutation",
               verbose = TRUE) %>% 
    set_mode("regression") %>% 
    set_args(mtry = {{mtry_val}},
             min_n = {{min_n_val}},
             trees = 1000)
  
  wf <- wf %>% 
    update_model(mod)
  
  rmse <- fit(wf, data_train) %>% 
    extract_rmse()
  
  tibble(mtry = mtry_val, min_n = min_n_val, rmse = rmse, workflow = list(wf))
}


mtry_results_1 <- map2_df(grd$Var1, grd$Var2, ~hyp_rf_search(.x, .y, rf_def_wkflw))

```

Need to create a folder with Rds files.

```{r, include=FALSE}

mtry_results_1 <- readRDS("files/mtry_results_1.Rds")

```


These are my results
rmse = 88.5 (50% of data) with mtry = 14, min_n = 10, trees = 1000

```{r echo=FALSE}

table1 <- mtry_results_1 %>%
  select(-workflow) %>%
  arrange(rmse)

DT::datatable(table1)

```


Running this first round of tuning took an hour and forty minutes and the increase relative to the default model was not substantial, only 3 decimal points. Nonetheless, I went for another round of tuning, just to see what could happen.

Looking at the plot, I could see that...


```{r echo=FALSE}
table1 %>%
  ggplot(aes(mtry, rmse)) +
  geom_line(color = "#6E0D83") +
  geom_point(color = "cornflowerblue") +
  facet_wrap(~min_n) +
  labs(title = "Plot 1: First round of tuning",
       x = "mtry values",
       y = "rmse") 
```

So I went for the following values to tune... 

I used the same all-in-one function shown above.

```{r}
mtry_search <- seq(14, 26, 3)
min_n_search <- seq(7, 15, 2)

grd <- expand.grid(mtry_search, min_n_search)

DT::datatable(grd, colnames = c('mtry' = 'Var1', 'min_n' = 'Var2'))
```




```{r}
# mtry_results_2 <- readRDS("scripts/mtry_results_2.Rds")
```



```{r}
# mtry_results_2 %>%
#   arrange(rmse)
```

```{r}
# mtry_results_2 %>%
#   ggplot(aes(mtry, rmse)) +
#   geom_line() +
#   geom_point() +
#   facet_wrap(~min_n)
```

After 2 additional hours, I was convinced that continue to tune was not worth it so I decided to stop and finalize the model with `mtry` = 14 and `min_n` = 15

```{r eval=FALSE, echo=TRUE}
final_mod <- rand_forest() %>%
  set_engine("ranger",
             num.threads = 8,
             importance = "permutation",
             verbose = TRUE) %>%
  set_mode("regression") %>%
  set_args(mtry = 14,
           min_n = 15,
           trees = 1000)

final_wkfl <-  workflow() %>%
  add_model(final_mod) %>%
  add_recipe(rec)
```

Not sure which one of the final fit models to include: the one that uses data_train or the one that uses data_split??