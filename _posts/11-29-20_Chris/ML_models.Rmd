---
title: "XG Boost"
description: |
    Tuning process and final model summary
author:
  - name: Chris Ives
    url:
    affiliation: University of Oregon
    affiliation_url:
date: 11-28-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(here)
library(rio)
library(skimr)
library(rsample)

```


## Data Import
``` {r echo = TRUE}

data <- read.csv("data/train.csv") %>%
  select(-classification) %>%
  mutate_if(is.character, factor) %>%
  mutate(ncessch = as.numeric(ncessch))

bonus <- read.csv("data/bonus_data_v2.csv") %>%
  mutate(ncessch = as.numeric(ncessch)) %>%
  mutate(locale = gsub("^.{0,3}", "", locale)) %>%
  separate(col = locale, into = c("locale", "sublocale"), sep = ": ")

disc <- read_csv("data/disc_drop.csv") %>%
  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))

## join data
data <- data %>% 
  left_join(bonus) %>% 
  left_join(disc)
``` 


Data was merged from three files:

* Original Competition Training Dataset

* "Bonus Dataset" with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels

* Supplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs
  + Dropout Rate Data Source:
  https://www.oregon.gov/ode/reports-and-data/students/Pages/Dropout-Rates.aspx
  + Suspension Rate Data Source:
  https://www.oregon.gov/ode/students-and-family/healthsafety/Pages/School-Discipline,-Bullying,-Restraint-and-Seclusion.aspx

Importantly, the bonus data includes the variables described in the original data description page, as well as the following:

* 2016-2017 District Finance Data:
  + Total revenue (`rev_total`)
  + Total local revenue (`rev_local_total`)
  + Total state revenue (`rev_state_total`)
  + Total federal revenue (`rev_fed_total`)
  + Total expenditures (`exp_total`)
  + Total current expenditures for elementary and secondary education (`exp_current_elsec_total`)
  + Total current expenditures for instruction (`exp_current_instruction_total`)
  + Total current expenditures for support services (`exp_current_supp_serve_total`)
  + Total capital outlay expenditures (`outlay_capital_total`)
  + Total salary amount (`salaries_total`)
  + Total employee benefits in dollars (`benefits_employee_total`)
  + Number of students for which the reporting local education agency is financially responsible (`enrollment_fall_responsible`)

District financial data was obtained using the `educationdata` R package.
  
Lastly, district finance data was included in both its raw form, and transformed by dividing each revenue and expenditure value by the number of students the LEA was financially responsible for (e.g., `exp_total`/`enrollment_fall_responsible`)
  
# Recipe
``` {r echo = TRUE}
```{r, echo=TRUE, include=TRUE, eval = FALSE}
rec <- recipe(score ~ ., train) %>%
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),
              lang_cd = case_when(lang_cd == "S" ~ "S", TRUE ~ "E"),
              pupil_tch_ratio = as.numeric(pupil_tch_ratio),
              pupil_tch_ratio = as.numeric(pupil_tch_ratio),
              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,
                                         pupil_tch_ratio < 25 ~ 2,
                                         pupil_tch_ratio < 30 ~ 3, 
                                         TRUE ~ 4),
              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% 
  step_rm(contains("id"), ncessch, ncesag, lea_name, sch_name) %>%
  step_mutate(hpi = as.numeric(hpi),
              lat = round(lat, 2),
              lon = round(lon, 2),
              median_income = log(median_income),
              frl_prop = fr_lnch_prop + red_lnch_prop,
              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,
                                    TRUE ~ 0),
              over_100 = under_200 + over_200) %>% 
  step_interact(terms = ~ lat:lon) %>% 
  step_rm(fr_lnch_prop, red_lnch_prop) %>% 
  step_string2factor(all_nominal()) %>% 
  step_zv(all_predictors()) %>%
  step_unknown(all_nominal()) %>% 
  step_medianimpute(all_numeric()) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% 
  step_interact(~ lang_cd_S:p_hispanic_latino) %>% 
  step_nzv(all_predictors(), freq_cut = 995/5)

baked_train <- prep(rec) %>% 
  bake(train)

train_x = data.matrix(baked_train[, -73])
train_y = data.matrix(baked_train[, 73])

```

## Recipe Notes
* Pupil/Teacher ratio (`pupil_tch_ratio`) is binned and treated as a factor to remove noise (`pupil_tch_rate`). Both ratio and binned version of the variable remain in the data.
* ID variables are removed
* Latitude (`lat`) and longitude (`lon`) are rounded to two decimal places to reduce noise and limit precision to within 2/3 of a mile.
* Median income is log transformed, as the variations in income level are expected to have greater effect near the poverty threshold and less effect in higher income brackets. 
* Free lunch proportions and reduced lunch proportions are collapsed into a single variable `frl_prop` given their expected similar effects.
  + Free lunch proportions (`fr_lnch_prop`) and reduced lunch proportions (`red_lnch_prop`) are removed from the data set in lieu of their combined proportion. 
* A dummy coded school-level variable (`schl_perf`) is created that denotes whether the percentage of students reaching proficiency on the EOY state test exceeds the percentage of students that fall below proficiency standards.
* Zero variance predictors are removed
* Missing data is median imputed
* Explicit interaction is specified between the student-level special education flag and the per-student district expenditures for special services. Th effect of special education status is expected to vary depending on the funding available and potential quality of the district's special services.
* Explicit interaction is specified for the effect of Spanish language code (`lang_cd`) and percentage of Hispanic/Latino students at the school (`p_hispanic_latino`). The justification is that Spanish-speaking students are expected to receive less effective supports in schools in which they are a greater minority.
* Near-zero variance predictors are removed

# Tuning Hyperparameters

Because the loss reduction (gamma) and maximum tree depth are highly dependent on one another, two approaches were taken to tune the initial model. One approach involved first tuning loss reduction, followed by tree depth and hessian weights. The other involved tuning tree complexity and hessian weights without any severe loss reduction regularization, and then pruning it back by tuning gamma. All tuning was done using the full training dataset.

## Approach #1: Tuning Gamma First
```{r, echo=TRUE, include=TRUE, eval = FALSE}

baked_train <- prep(rec) %>% 
  bake(train)

train_x = data.matrix(baked_train[, -73])
train_y = data.matrix(baked_train[, 73])

grid <- expand.grid(loss_reduction = seq(0, 80, 5))

gamma_mods <- map(grid$loss_reduction, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 10000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50,
    nfold = 10,
    verbose = 1,
    params = list(
      eta = 0.1,
      gamma = .x,
      nthread = 24
    )
  )
})
```

### Tuning Results
``` {r echo = FALSE, fig.width = 8}
gamma_mods <- readRDS("gamma_mods.rds")

gamma_res <- map_df(gamma_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
gamma_params <- map_df(gamma_mods,
                       ~.x$params)

mod_res <- cbind(gamma_res, gamma_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread)

print(head(round(mod_res, 4), 10))

ggplot(mod_res, aes(x = gamma, y = test_rmse_mean)) +
  geom_line()
```

As indicated in the results, a gamma of 10 produced the best fit the to test folds. A gamma of 50 was considered as a potential alternative given its lower RMSE SD; however, 50 was considered too extreme of a regularization setting.

A follow-up gamma tuning process was conducted with a narrowed range of 5 to 15 and is reported below.

`grid <- expand.grid(loss_reduction = seq(5, 15, 1))`

``` {r echo = FALSE, fig.width = 8}

gamma_mods_fine <- readRDS("gamma_mods_fine.rds")

gamma_res <- map_df(gamma_mods_fine,
                    ~.x$evaluation_log[.x$best_iteration, ])
gamma_params <- map_df(gamma_mods_fine,
                       ~.x$params)

mod_res <- cbind(gamma_res, gamma_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread)

print(head(round(mod_res, 4), 10))

ggplot(mod_res, aes(x = gamma, y = test_rmse_mean)) +
  geom_line()
```

After second round of tuning, a gamma value of 12 produced the best fit.

**Best `gamma` value = 12**

**RMSE/SD to beat = 82.6085/0.5934**

## Follow-up Tree-Parameter Tuning
### Create Grid
``` {r eval = TRUE, echo = TRUE}
# Set learning rate, tune tree specific parameters
grid <- grid_max_entropy(min_n(c(4, 12)), # min_child_weight
                         tree_depth(), # max_depth
                         size = 30)
head(grid)
```

### Specify Model

``` {r echo = TRUE, eval = FALSE}
tree_mods <- map2(grid$min_n, grid$tree_depth, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 1,
    params = list( 
      eta = 0.1,
      gamma = 12, 
      min_child_weight = .x,
      max_depth = .y,
      nthread = 24
    ) 
  )  
}) 
```

### Tuning Results
``` {r echo = FALSE, fig.width = 10}
tree_mods_g12 <- readRDS("tree_mods_1206_g12.rds")

tree_res_g12 <- map_df(tree_mods_g12,
                    ~.x$evaluation_log[.x$best_iteration, ])
tree_params_g12 <- map_df(tree_mods_g12,
                       ~.x$params)

mod_res_g12 <- cbind(tree_res_g12, tree_params_g12) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta, -gamma)

plotly::plot_ly(mod_res_g12, x = ~min_child_weight, y = ~max_depth, z = ~test_rmse_mean, marker = list(size = 5),
                color = ~test_rmse_mean)


print(head(round(mod_res, 4), 10))
```

A `min_child_weight` of **42** and `max_depth` of **9** demonstrate the best fit to the test folds and have a low SD relative to other hyperparameter tunings.

Tuning Summary:

* `gamma` = 12

* `min_child_weight` = 42

* `max_depth` = 9

**Final RMSE/SD to beat: 82.5145/0.4865**


## Approach #2: Tuning Tree Parameters First
### Specify Model
``` {r echo = TRUE, eval = FALSE}
tree_mods <- map2(grid$min_n, grid$tree_depth, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 1,
    params = list( 
      eta = 0.1,
      min_child_weight = .x,
      max_depth = .y,
      nthread = 24
    ) 
  )  
}) 
```

### Tuning Results
``` {r echo = FALSE, fig.width = 8}
tree_mods <- readRDS("tree_mods_1206.rds")

tree_res <- map_df(tree_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
tree_params <- map_df(tree_mods,
                       ~.x$params)

mod_res <- cbind(tree_res, tree_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta)

plotly::plot_ly(mod_res, x = ~min_child_weight, y = ~max_depth, z = ~test_rmse_mean, marker = list(size = 5),
                color = ~test_rmse_mean)

print(head(round(mod_res, 4), 10))
```

Without any `gamma` specification, the best `min_child_weight` value was 31 and the best `max_depth` was 6. They will be retained for the `gamma` tuning process.

**RMSE/SD = 82.5124/0.6930**

### Followup Gamma Tuning
``` {r echo = FALSE, fig.width = 8}

gamma_mods_tg <- readRDS("gamma_mods_tg.rds")

gamma_res_tg <- map_df(gamma_mods_tg,
                    ~.x$evaluation_log[.x$best_iteration, ])
gamma_params_tg <- map_df(gamma_mods_tg,
                       ~.x$params)

mod_res <- cbind(gamma_res_tg, gamma_params_tg) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread)

print(head(round(mod_res, 4), 10))

ggplot(mod_res, aes(x = gamma, y = test_rmse_mean)) +
  geom_line()
```

The best RMSE was obtained with a gamma value of 5. Thus a narrower grid search was conducted around this value.

### Fine Tuning Gamma
``` {r echo = FALSE, fig.width = 8}

gamma_mods_tg <- readRDS("gamma_mods_tg_fine.rds")

gamma_res_tg <- map_df(gamma_mods_tg,
                    ~.x$evaluation_log[.x$best_iteration, ])
gamma_params_tg <- map_df(gamma_mods_tg,
                       ~.x$params)

mod_res <- cbind(gamma_res_tg, gamma_params_tg) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread)

print(head(round(mod_res, 4), 10))

ggplot(mod_res, aes(x = gamma, y = test_rmse_mean)) +
  geom_line()
```

After fine tuning gamma, the best RMSE was obtained using a value of 1. However, RMSE values did not appear to converge towards a particular value, as illustrated in the plot. Consequently, some caution is warranted for these hyperparameter values.

**Best RMSE/SD = 82.5199/0.5321**

## Evaluating Approaches

Approach #1: RMSE/SD = 82.5145/0.4865
Approach #2: RMSE/SD = 82.5199/0.5321

Tuning gamma first, followed by the tree hyperparameters yielded better RMSE during the cross validations. Thus, the hyperparameter values generated through the first tuning approach were retained.

**Updated model summary:**

* `gamma` = 12

* `max_depth` = 6

* `min_child_weight` = 10


## Tune Stochastic Parameters

After tuning loss reduction and the tree hyperparameters, the stochastic parameters were tuned to identify the best subsampling of columns and cases. 

``` {r eval = FALSE}

# Dials package requires column sampling (mtry) and subsampling (sample_size) to be expressed as raw numbers rather than intervals
grid <- grid_max_entropy(mtry(as.integer(c(.3*185, .9*185))),
                         sample_size(as.integer(c(.5*nrow(train), nrow(train)))),
                         size = 30)

# Convert raw numbers to proportions
grid <- grid %>% 
  mutate(mtry = mtry/185,
         sample_size = sample_size/nrow(train))

print(grid)
```

### Specify Model
``` {r eval = FALSE, echo = TRUE}
sample_mods <- map2(grid$mtry, grid$sample_size, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 1,
    params = list( 
      eta = 0.1,
      gamma = 12,
      max_depth = 6,
      min_child_weight = 10,
      colsample_bytree = .x,
      subsample = .y,
      nthread = 24
    ) 
  )  
}) 
```


### Tuning Results
``` {r echo = FALSE, eval = TRUE}
sample_mods <- readRDS("sample_mods_gt.rds")

sample_res <- map_df(sample_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
sample_params <- map_df(sample_mods,
                       ~.x$params)

mod_res <- cbind(sample_res, sample_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta, -max_depth, -min_child_weight)

p <- plotly::plot_ly(mod_res, x = ~colsample_bytree, y = ~subsample, z = ~test_rmse_mean, marker = list(size = 5),
                color = ~test_rmse_mean)
print(p)

print(head(round(mod_res, 4), 10))
```

Best values for `colsample_bytree` and `subsample` appear to be around values >.70. A narrower grid search was completed and is summarized below. Notably, gamma was mistakenly specified as 10 (instead of 12) during this tuning process. However, this wasn't expected to dramatically shift the tenable range of best stochastic parameter values during fine tuning.

#### Fine Tuning Results
``` {r echo = FALSE, eval = TRUE}
sample_mods <- readRDS("sample_mods_gt_fine.rds")

sample_res <- map_df(sample_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
sample_params <- map_df(sample_mods,
                       ~.x$params)

mod_res <- cbind(sample_res, sample_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta, -max_depth, -min_child_weight)

plotly::plot_ly(mod_res, x = ~colsample_bytree, y = ~subsample, z = ~test_rmse_mean, marker = list(size = 5),
                color = ~test_rmse_mean)

print(head(round(mod_res, 4), 10))
```

Best value of `colsample_bytree` = .892

Best value of `subsample` = .927


**Updated model summary:**

* `gamma` = 12

* `max_depth` = 6

* `min_child_weight` = 10

* `colsample_bytree` = .892

* `subsample` = .927


## Retune Learning Rate
### Specify Model
``` {r eval = FALSE, echo = TRUE}
r <- seq(0.0001, 0.1, length.out = 20)

lr_mods <- map(lr, function(learn_rate) {
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50,
    nfold = 10,
    verbose = 0,
    params = list(
      eta = learn_rate,
      gamma = 12,
      max_depth = 6,
      min_child_weight = 10,
      colsample_bytree = 0.8918919,
      subsample = 0.9270078,
      nthread = 24
    )
  )
})
```


The learning rate could not be re-tuned under the time constraints, so it was lowered to .04, which was identified to be the best learning rate during tuning of early models.

# Tune Number of Trees
``` {r eval = FALSE, echo = TRUE}
bst <- xgb.cv(
  data = train_x,
  label = train_y,
  nrounds = 10000,
  objective = "reg:squarederror",
  early_stopping_rounds = 50,
  nfold = 10,
  verbose = 1,
  params = list(
    eta = .04,
  gamma = 12,
  max_depth = 6,
  min_child_weight = 10,
  colsample_bytree = 0.8918919,
  subsample = 0.9270078,
  nthread = 24))
```

``` {r eval = TRUE, echo = FALSE}
final_mod <- readRDS("model_submit_1207.rds")

final_mod$evaluation_log[final_mod$best_iteration, ]
```

Best iteration was 1119. Only 1119 trees will be specified in final fit.

# Final Fit Statistics

### Generate Final Fit
``` {r eval = FALSE, echo = TRUE}
final_mod <- xgboost(
  data = train_x,
  label = train_y,
  nrounds = 1119,
  objective = "reg:squarederror",
  verbose = 1,
  params = list(
    eta = .04,
  gamma = 12,
  max_depth = 6,
  min_child_weight = 10,
  colsample_bytree = 0.8918919,
  subsample = 0.9270078,
  nthread = 24))
```

### Fit to test set
``` {r eval = FALSE, echo = TRUE}
baked_test <- prep(rec) %>%
  bake(test)

test_x = data.matrix(baked_test[, -73])

pred <- predict(final_mod, as.matrix(test_x))
actual <- baked_test$score

Metrics::rmse(actual, pred)
```

### Final Fit Results
``` {r eval = TRUE, echo = FALSE}

results <- read_csv("blog_predictions.csv")

Metrics::rmse(results$Actual, results$Predicted)

```


After tuning hyperparameters, reducing the learning rate, and determining the number of trees, the final RMSE on the test split is **78.86593**. This was only slightly higher than the RMSE calculated during the final fit to the training data, which was **77.51970**.

This model was submitted 20 minutes late to the Kaggle competition, but received a RMSE of **81.65183** on the 30% test set for the public leaderboard, and an RMSE of **82.07486** for the private leaderboard.

### Feature Importance Plot of Final Model (Top 35)
``` {r eval = TRUE, echo = FALSE}

model <- readRDS("blog_model.Rds")
importance <- xgboost::xgb.importance(model = model)
importance <- importance[1:35,]
xgboost::xgb.ggplot.importance(importance)

```


