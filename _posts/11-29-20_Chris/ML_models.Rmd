---
title: "XG Boost"
description: |
    We will be fitting linear model, random forest, and gradient boosting.
author:
  - name: Chris Ives
    url:
    affiliation: University of Oregon
    affiliation_url:
date: 11-28-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(here)
library(rio)
library(skimr)
library(rsample)

```


## Data Import
``` {r echo = TRUE}

data <- read.csv("data/train.csv") %>%
  select(-classification) %>%
  mutate_if(is.character, factor) %>%
  mutate(ncessch = as.numeric(ncessch))

bonus <- read.csv("data/bonus_data_v2.csv") %>%
  mutate(ncessch = as.numeric(ncessch)) %>%
  mutate(locale = gsub("^.{0,3}", "", locale)) %>%
  separate(col = locale, into = c("locale", "sublocale"), sep = ": ")

disc <- read_csv("data/disc_drop.csv") %>%
  mutate(attnd_dist_inst_id = as.double(attnd_dist_inst_id))

## join data
data <- data %>% 
  left_join(bonus) %>% 
  left_join(disc)
``` 


Data was merged from three files:

* Original Competition Training Dataset

* "Bonus Dataset" with additional variables collected by zip code, NCES school IDs, state school IDs, and county levels

* Supplemental small dataset of high school dropout rates and out-of-school suspension rates by state district IDs

Importantly, the bonus data includes the variables described in the original data description page, as well as the following:

* District Finance Data:
  + Total revenue (rev_total)
  + Total local revenue (rev_local_total)
  + Total state revenue (rev_state_total)
  + Total federal revenue (rev_fed_total)
  + Total expenditures (exp_total)
  + Total current expenditures for elementary and secondary education (exp_current_elsec_total)
  + Total current expenditures for instruction (exp_current_instruction_total)
  + Total current expenditures for support services (exp_current_supp_serve_total)
  + Total capital outlay expenditures (outlay_capital_total)
  + Total salary amount (salaries_total)
  + Total employee benefits in dollars (benefits_employee_total)
  + Number of students for which the reporting local education agency is financially responsible (enrollment_fall_responsible)

# Recipe
``` {r echo = TRUE}
```{r, echo=TRUE, include=TRUE, eval = FALSE}
rec <- recipe(score ~ ., train) %>%
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),
              lang_cd = case_when(lang_cd == "S" ~ "S", TRUE ~ "E"),
              pupil_tch_ratio = as.numeric(pupil_tch_ratio),
              pupil_tch_ratio = as.numeric(pupil_tch_ratio),
              pupil_tch_rate = case_when(pupil_tch_ratio < 18 ~ 1,
                                         pupil_tch_ratio < 25 ~ 2,
                                         pupil_tch_ratio < 30 ~ 3, 
                                         TRUE ~ 4),
              pupil_tch_rate = as.factor(pupil_tch_rate)) %>% 
  step_rm(contains("id"), ncessch, ncesag, lea_name, sch_name) %>%
  step_mutate(hpi = as.numeric(hpi),
              lat = round(lat, 2),
              lon = round(lon, 2),
              median_income = log(median_income),
              median_rent = log(median_rent),
              frl_prop = fr_lnch_prop + red_lnch_prop,
              schl_perf = case_when(sch_percent_level_1 + sch_percent_level_2 > sch_percent_level_3 + sch_percent_level_4 ~ 1,
                                    TRUE ~ 0),
              over_100 = under_200 + over_200) %>% 
  step_interact(terms = ~ lat:lon) %>% 
  step_rm(fr_lnch_prop, red_lnch_prop) %>% 
  step_string2factor(all_nominal()) %>% 
  step_zv(all_predictors()) %>%
  step_unknown(all_nominal()) %>% 
  step_medianimpute(all_numeric()) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_interact(~ exp_current_supp_serve_total.x:sp_ed_fg_Y) %>% 
  step_interact(~ lang_cd_S:p_hispanic_latino) %>% 
  step_nzv(all_predictors(), freq_cut = 995/5)

baked_train <- prep(rec) %>% 
  bake(train)

train_x = data.matrix(baked_train[, -73])
train_y = data.matrix(baked_train[, 73])

```

## Recipe Notes
* Pupil/Teacher ratio is binned and treated as a factor to remove noise. Ratio and binned variable are left as variables.
* ID variables are removed
# Tuning Gamma
```{r, echo=TRUE, include=TRUE, eval = FALSE}

baked_train <- prep(rec) %>% 
  bake(train)

train_x = data.matrix(baked_train[, -73])
train_y = data.matrix(baked_train[, 73])

grid <- expand.grid(loss_reduction = seq(0, 80, 5))

gamma_mods <- map(grid$loss_reduction, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 10000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50,
    nfold = 10,
    verbose = 1,
    params = list(
      eta = 0.1,
      gamma = .x,
      nthread = 24
    )
  )
})
```

## Gamma Tuning Results
``` {r echo = FALSE, fig.width = 8}
gamma_mods <- readRDS("gamma_mods.rds")

gamma_res <- map_df(gamma_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
gamma_params <- map_df(gamma_mods,
                       ~.x$params)

mod_res <- cbind(gamma_res, gamma_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread)

print(round(mod_res, 4))
```


# Tree Parameter Tuning
## Create Grid
``` {r eval = TRUE, echo = TRUE}
# Set learning rate, tune tree specific parameters
grid <- grid_max_entropy(min_n(c(0, 50)), # min_child_weight
                         tree_depth(), # max_depth
                         size = 30)
head(grid)
```

## Update Model Specification
``` {r echo = TRUE, eval = FALSE}
tree_mods <- map2(grid$min_n, grid$tree_depth, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 1,
    params = list( 
      eta = 0.1,
      gamma = 10,
        max_depth = .x,
      min_child_weight = .y,
      nthread = 24
    ) 
  )  
}) 
```

## Tree Parameter results
``` {r echo = FALSE, fig.width = 8}
tree_mods <- readRDS("tree_mods.rds")

tree_res <- map_df(tree_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
tree_params <- map_df(tree_mods,
                       ~.x$params)

mod_res <- cbind(tree_res, tree_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta, -gamma)

print(round(mod_res, 4))
```


# Tune Stochastic Parameters
``` {r eval = FALSE}

# Dials package requires column sampling (mtry) and subsampling (sample_size) to be expressed as raw numbers rather than intervals
grid <- grid_max_entropy(mtry(as.integer(c(.3*185, .9*185))),
                         sample_size(as.integer(c(.5*nrow(train), nrow(train)))),
                         size = 30)

# Convert raw numbers to proportions
grid <- grid %>% 
  mutate(mtry = mtry/185,
         sample_size = sample_size/nrow(train))

print(grid)
```

## Update Model Specification
``` {r eval = FALSE}
sample_mods <- map2(grid$mtry, grid$sample_size, ~{
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 1,
    params = list( 
      eta = 0.1,
      max_depth = 6,
      min_child_weight = 10,
      colsample_bytree = .x,
      subsample = .y,
      nthread = 24
    ) 
  )  
}) 
```


## Stochastic Parameter Results
#### (Top 10 results shown)
``` {r echo = FALSE, eval = FALSE}
sample_mods <- readRDS("sample_mods.rds")

sample_res <- map_df(sample_mods,
                    ~.x$evaluation_log[.x$best_iteration, ])
sample_params <- map_df(sample_mods,
                       ~.x$params)
names(mod_res)
mod_res <- cbind(sample_res, sample_params) %>% 
  arrange(test_rmse_mean) %>%
  select(-silent, -objective, -nthread, -eta, -gamma, -max_depth, -min_child_weight)

head(mod_res, 10)
```


